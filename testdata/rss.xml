<?xml version="1.0" encoding="UTF-8" standalone="no"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>AWS Architecture Blog</title>
	<atom:link href="https://aws.amazon.com/blogs/architecture/feed/" rel="self" type="application/rss+xml"/>
	<link>https://aws.amazon.com/blogs/architecture/</link>
	<description>Just another Amazon Web Services site</description>
	<lastBuildDate>Wed, 14 Dec 2022 21:23:48 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Building a healthcare data pipeline on AWS with IBM Cloud Pak for Data</title>
		<link>https://aws.amazon.com/blogs/architecture/building-a-healthcare-data-pipeline-on-aws-with-ibm-cloud-pak-for-data/</link>
					
		
		<dc:creator><![CDATA[Eduardo Monich Fronza]]></dc:creator>
		<pubDate>Wed, 14 Dec 2022 14:24:07 +0000</pubDate>
				<category><![CDATA[Amazon Simple Notification Service (SNS)]]></category>
		<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[Amazon VPC]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS CloudTrail]]></category>
		<category><![CDATA[Kinesis Data Firehose]]></category>
		<guid isPermaLink="false">91d45ff6276f95ccaeaf86d0f43f0cabfddd7e03</guid>

					<description>Healthcare data is being generated at an increased rate with the proliferation of connected medical devices and clinical systems. Some examples of these data are time-sensitive patient information, including results of laboratory tests, pathology reports, X-rays, digital imaging, and medical devices to monitor a patient’s vital signs, such as blood pressure, heart rate, and temperature. […]</description>
										<content:encoded>&lt;p&gt;Healthcare data is being generated at an increased rate with the proliferation of connected medical devices and clinical systems. Some examples of these data are time-sensitive patient information, including results of laboratory tests, pathology reports, X-rays, digital imaging, and medical devices to monitor a patient’s vital signs, such as blood pressure, heart rate, and temperature.&lt;/p&gt; 
&lt;p&gt;These different types of data can be difficult to work with, but when combined they can be used to build data pipelines and machine learning (ML) models to address various challenges in the healthcare industry, like the prediction of patient outcome, readmission rate, or disease progression.&lt;/p&gt; 
&lt;p&gt;In this post, we demonstrate how to bring data from different sources, like &lt;a href="https://www.snowflake.com/en/"&gt;Snowflake&lt;/a&gt; and connected health devices, to form a healthcare data lake on Amazon Web Services (AWS). We also explore how to use this data with &lt;a href="https://www.ibm.com/watson"&gt;IBM Watson&lt;/a&gt; to build, train, and deploy ML models. You can learn how to integrate model endpoints with clinical health applications to generate predictions for patient health conditions.&lt;/p&gt; 
&lt;h2&gt;Solution overview&lt;/h2&gt; 
&lt;p&gt;The main parts of the architecture we discuss are (Figure 1):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Using patient data to improve health outcomes&lt;/li&gt; 
 &lt;li&gt;Healthcare data lake formation to store patient health information&lt;/li&gt; 
 &lt;li&gt;Analyzing clinical data to improve medical research&lt;/li&gt; 
 &lt;li&gt;Gaining operational insights from healthcare provider data&lt;/li&gt; 
 &lt;li&gt;Providing data governance to maintain the data privacy&lt;/li&gt; 
 &lt;li&gt;Building, training, and deploying an ML model&lt;/li&gt; 
 &lt;li&gt;Integration with the healthcare system&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_12625" style="width: 1192px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/07/IBM-Healthcare-CP4D-120722.jpg"&gt;&lt;img aria-describedby="caption-attachment-12625" loading="lazy" class="size-full wp-image-12625" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/07/IBM-Healthcare-CP4D-120722.jpg" alt="Data pipeline for the healthcare industry using IBM CP4D on AWS " width="1182" height="781"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12625" class="wp-caption-text"&gt;Figure 1. Data pipeline for the healthcare industry using IBM CP4D on AWS&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=overview"&gt;IBM Cloud Pak for Data (CP4D)&lt;/a&gt; is deployed on &lt;a href="https://aws.amazon.com/rosa/"&gt;Red Hat OpenShift Service on AWS (ROSA)&lt;/a&gt;. It provides the components &lt;a href="https://www.ibm.com/products/datastage"&gt;IBM DataStage&lt;/a&gt;, &lt;a href="https://www.ibm.com/cloud/watson-knowledge-catalog"&gt;IBM Watson Knowledge Catalogue&lt;/a&gt;, &lt;a href="https://www.ibm.com/cloud/watson-studio"&gt;IBM Watson Studio&lt;/a&gt;, &lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=models-watson-machine-learning"&gt;IBM Watson Machine Learning&lt;/a&gt;, plus a wide variety of connections with data sources available in a public cloud or on-premises.&lt;/p&gt; 
&lt;p&gt;Connected health devices, on the edge, use sensors and wireless connectivity to gather patient health data, such as biometrics, and send it to the AWS Cloud through &lt;a href="https://aws.amazon.com/kinesis/data-firehose/"&gt;Amazon Kinesis Data Firehose&lt;/a&gt;. &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; transforms the data that is persisted to &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3)&lt;/a&gt;, making that information available to healthcare providers.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/sns/?id=docs_gateway"&gt;Amazon Simple Notification Service (Amazon SNS)&lt;/a&gt; is used to send notifications whenever there is an issue with the real-time data ingestion from the connected health devices. In case of failures, messages are sent via Amazon SNS topics for rectifying and reprocessing of failure messages.&lt;/p&gt; 
&lt;p&gt;DataStage performs ETL operations and move patient historical information from Snowflake into Amazon S3. This data, combined with the data from the connected health devices, form a healthcare data lake, which is used in IBM CP4D to build and train ML models.&lt;/p&gt; 
&lt;p&gt;The pipeline described in architecture uses Watson Knowledge Catalogue, which provides data governance framework and artifacts to enrich our data assets. It protects sensitive patient information from unauthorized access, like individually identifiable information, medical history, test results, or insurance information.&lt;/p&gt; 
&lt;p&gt;Data protection rules define how to control access to data, mask sensitive values, or filter rows from data assets. The rules are automatically evaluated and enforced each time a user attempts to access a data asset in any governed catalog of the platform.&lt;/p&gt; 
&lt;p&gt;After this, the datasets are published to Watson Studio projects, where they are used to train ML models. You can develop models using &lt;a href="https://jupyter.org/"&gt;Jupyter Notebook&lt;/a&gt;, &lt;a href="https://www.ibm.com/cloud/watson-studio/autoai"&gt;IBM AutoAI&lt;/a&gt; (low-code), or &lt;a href="https://www.ibm.com/products/spss-statistics"&gt;IBM SPSS&lt;/a&gt; modeler (no-code).&lt;/p&gt; 
&lt;p&gt;For the purpose of this use case, we used &lt;a href="https://www.javatpoint.com/logistic-regression-in-machine-learning"&gt;logistic regression algorithm&lt;/a&gt; for classifying and predicting the probability of an event, such as disease risk management to assist doctors in making critical medical decisions. You can also build ML models using algorithms like &lt;a href="https://www.javatpoint.com/classification-algorithm-in-machine-learning"&gt;Classification&lt;/a&gt;, &lt;a href="https://www.javatpoint.com/machine-learning-random-forest-algorithm"&gt;Random Forest&lt;/a&gt;, and &lt;a href="https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning"&gt;K-Nearest Neighbor&lt;/a&gt;. These are widely used to predict disease risk.&lt;/p&gt; 
&lt;p&gt;Once the models are trained, they are exposed as endpoints with Watson Machine Learning and integrated with the healthcare application to generate predictions by analyzing patient symptoms.&lt;/p&gt; 
&lt;p&gt;The healthcare applications are a type of clinical software that offer crucial physiological insights and predict the effects of illnesses and possible treatments. It provides built-in dashboards that display patient information together with the patient’s overall metrics for outcomes and treatments. This can help healthcare practitioners gain insights into patient conditions. It also can help medical institutions prioritize patients with more risk factors and curate clinical and behavioral health plans.&lt;/p&gt; 
&lt;p&gt;Finally, we are using &lt;a href="https://www.ibm.com/qradar"&gt;IBM Security QRadar XDR SIEM&lt;/a&gt; to collect, process, and aggregate &lt;a href="https://docs.aws.amazon.com/vpc/index.html"&gt;Amazon Virtual Private Cloud (Amazon VPC)&lt;/a&gt; &lt;a href="https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html"&gt;flow logs&lt;/a&gt;, &lt;a href="https://aws.amazon.com/cloudtrail/"&gt;AWS CloudTrail&lt;/a&gt; logs, and IBM CP4D logs. QRadar XDR uses this information to manage security by providing real-time monitoring, alerts, and responses to threats.&lt;/p&gt; 
&lt;h2&gt;Healthcare data lake&lt;/h2&gt; 
&lt;p&gt;A healthcare data lake can help health organizations turn data into insights. It is centralized, curated, and securely stores data on Amazon S3. It also enables you to break down data silos and combine different types of analytics to gain insights. We are using the DataStage, Kinesis Data Firehose, and Amazon S3 services to build the healthcare data lake.&lt;/p&gt; 
&lt;h2&gt;Data governance&lt;/h2&gt; 
&lt;p&gt;Watson Knowledge Catalogue provides an ML catalogue for data discovery, cataloging, quality, and governance. We define policies in Watson Knowledge Catalogue to enable data privacy and overall access to and utilization of this data. This includes sensitive data and personal information that needs to be handled through data protection, quality, and automation rules. To learn more about IBM data governance, please refer to &lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=data-running-quality-analysis"&gt;Running a data quality analysis (Watson Knowledge Catalogue)&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Build, train, and deploy the ML model&lt;/h2&gt; 
&lt;p&gt;Watson Studio empowers data scientists, developers, and analysts to build, run, and manage AI models on IBM CP4D.&lt;/p&gt; 
&lt;p&gt;In this solution, we are building models using Watson Studio by:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Promoting the governed data from Watson Knowledge Catalogue to Watson Studio for insights&lt;/li&gt; 
 &lt;li&gt;Using ETL features, such as built-in search, automatic metadata propagation, and simultaneous highlighting, to process and transform large amounts of data&lt;/li&gt; 
 &lt;li&gt;Training the model, including model technique selection and application, hyperparameter setting and adjustment, validation, ensemble model development and testing; algorithm selection; and model optimization&lt;/li&gt; 
 &lt;li&gt;Evaluating the model based on metric evaluation, confusion matrix calculations, KPIs, model performance metrics, model quality measurements for accuracy and precision&lt;/li&gt; 
 &lt;li&gt;Deploying the model on Watson Machine Learning using online deployments, which create an endpoint to generate a score or prediction in real time&lt;/li&gt; 
 &lt;li&gt;Integrating the endpoint with applications like health applications, as demonstrated in Figure 1&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog, we demonstrated how to use patient data to improve health outcomes by creating a healthcare data lake and analyzing clinical data. This can help patients and healthcare practitioners make better, faster decisions and prioritize cases. We also discussed how to build an ML model using IBM Watson and integrate it with healthcare applications for health analysis.&lt;/p&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://partners.amazonaws.com/partners/001E000001IlLnmIAF/IBM"&gt;IBM on AWS Partner Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/deploying-ibm-cloud-pak-for-data-on-red-hat-openshift-service-on-aws/"&gt;Deploying IBM Cloud Pak for Data on Red Hat OpenShift Service on AWS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=solution-exporting-audit-records-qradar"&gt;Exporting Cloud Pak for Data audit records to QRadar&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/marketplace/pp/prodview-xnyxvyyyjigue?sr=0-1&amp;amp;ref_=beagle&amp;amp;applicationId=AWSMPContessa"&gt;IBM Cloud Pak for Data on the AWS Marketplace&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/marketplace/pp/prodview-f2xuznxhvifjm?sr=0-13&amp;amp;ref_=beagle&amp;amp;applicationId=AWSMPContessa"&gt;IBM Cloud Pak for Security on the AWS Marketplace&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>An overview and architecture of building a Customer Data Platform on AWS</title>
		<link>https://aws.amazon.com/blogs/architecture/overview-and-architecture-building-customer-data-platform-on-aws/</link>
					
		
		<dc:creator><![CDATA[Larry Bell]]></dc:creator>
		<pubDate>Mon, 12 Dec 2022 19:06:34 +0000</pubDate>
				<category><![CDATA[Analytics]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Well-Architected]]></category>
		<category><![CDATA[Marketing & Advertising]]></category>
		<category><![CDATA[analytics]]></category>
		<category><![CDATA[architecture best practices]]></category>
		<category><![CDATA[C360]]></category>
		<category><![CDATA[CDP]]></category>
		<category><![CDATA[customer]]></category>
		<category><![CDATA[Customer Solutions]]></category>
		<category><![CDATA[ISV Solutions]]></category>
		<category><![CDATA[well architected]]></category>
		<guid isPermaLink="false">6b250232a7d8b4ad948ba259260967d3cc2b8943</guid>

					<description>The deprecation of digital consumer identifiers, such as third-party cookies and mobile advertising IDs, and the rapid growth of data from expanding consumer touchpoints, has created challenges in identifying, managing, and reaching customers in digital channels. Organizations must rethink their strategies for collecting and storing customer data. Customer Data Platforms (CDPs) collect, aggregate, and organize […]</description>
										<content:encoded>&lt;p&gt;The deprecation of digital consumer identifiers, such as third-party cookies and mobile advertising IDs, and the rapid growth of data from expanding consumer touchpoints, has created challenges in identifying, managing, and reaching customers in digital channels. Organizations must rethink their strategies for collecting and storing customer data. Customer Data Platforms (CDPs) collect, aggregate, and organize customer data sources, and create individual centralized customer profiles to better manage and understand customers.&lt;/p&gt; 
&lt;p&gt;Independent Software Vendors (ISVs) in the advertising and marketing industry vertical can aid many companies in achieving these goals. The ISVs can help organizations with the heavy lifting required to build, secure, govern, and maintain near real-time, high volume CDPs. However, building these types of vendor solutions, that support a large number of customers, data volumes, and use cases, is a complex undertaking with often unforeseen challenges.&lt;/p&gt; 
&lt;p&gt;This post examines the logical architecture of the CDP to provide guidance to help reduce complexity, increase agility, improve operational excellence, and optimize cost. Although the material can benefit advanced marketers evaluating building a CDP, the intent of this post is to provide guidance to those ISVs that are facing these challenges for multiple clients.&lt;/p&gt; 
&lt;h2&gt;Marketing CDP logical architecture&lt;/h2&gt; 
&lt;p&gt;The principal challenge of the CDP architecture is to integrate data from many disparate sources and types. Envision a data lake-centric approach based on a layered architecture where the sources of customer data flow through six logical layers: Ingestion; Processing; Storage; Unified Governance (and Security); Cataloging; and Consumption.&lt;/p&gt; 
&lt;p&gt;A layered, component-oriented architecture promotes separation of concerns, decoupling of tasks, and the flexibility required to build each component consistent with best practices. These components provide the agility necessary to quickly integrate new data sources and support new analytics or product capabilities. The components are depicted in the image below in the conceptual logical model of the marketing CDP and are then described in this post.&lt;/p&gt; 
&lt;div id="attachment_12645" style="width: 1838px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/14/logicalCDPv2.png"&gt;&lt;img aria-describedby="caption-attachment-12645" loading="lazy" class="size-full wp-image-12645" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/14/logicalCDPv2.png" alt="A diagram depicting marketing CDP logical architecture" width="1828" height="1059"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12645" class="wp-caption-text"&gt;Figure 1: Marketing CDP logical architecture&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;CDP components&lt;/h2&gt; 
&lt;h3&gt;Logical ingestion layer&lt;/h3&gt; 
&lt;p&gt;The ingestion layer is responsible for collecting data across various customer touchpoints. It provides the ability to connect with internal and external data sources. It can ingest batch, near real-time, and real-time data into the storage layer. This layer aggregates data from multiple source systems and therefore elevates the marketing CDP as the primary repository of marketing and advertising data across an organization. Sources can include cloud or on-premise data sources, streaming data, file stores, third-party Software as a Service (SaaS) connectors and APIs. Separating this component into three layers reduces complexity while providing agility to the process.&lt;/p&gt; 
&lt;h3&gt;Logical storage layer&lt;/h3&gt; 
&lt;p&gt;A scalable, flexible, resilient, and reliable storage layer is critical to the value proposition of a marketing CDP. It consists of three distinct storage areas:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Raw Zone&lt;/strong&gt; – Contains ingested data in its original, immutable format, which can be used to source additional attributes in the future. It can also be used to restore data in certain disaster recovery scenarios. This layer acts as an immutable record of what has happened/been observed historically so that we can use that immutable data to generate a source of fact.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Clean Zone&lt;/strong&gt; – Contains the first transformation of raw data, including conversions to an efficient data format such as Parquet or Avro, as well as basic data quality validations. This layer also acts as an ad-hoc layer to develop answers to unknown question in reasonable time frames so that they can be migrated to the curated zone.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Curated Zone&lt;/strong&gt; – Contains data, organized by subject area, that is ready for consumption by users and applications For a marketing CDP, this includes identity resolution, data enrichment, customer segmentation, and aggregation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Automated data archival can be configured individually for each layer, and aligned to compliance requirements set by the organization. Access to these layers is controlled at a granular level to ensure a secure and collaborative data exchange and exploration.&lt;/p&gt; 
&lt;h3&gt;Logical cataloging layer&lt;/h3&gt; 
&lt;p&gt;The cataloging layer provides a centralized governance control, including mechanisms for data access control, versioning, and metadata exploration. It provides the ability to track the schema and the partitioning of datasets. This layer makes the datasets discoverable. The Governance capabilities of the Catalog ensure standardization for audit purposes.&lt;/p&gt; 
&lt;h3&gt;Logical processing layer&lt;/h3&gt; 
&lt;p&gt;This layer is responsible for transforming data into a consumable state by applying business rules for data validation, identity resolution, segmentation, normalization, profile aggregation, and machine learning (ML) processing. This layer comprises custom application logic. The compute resources for this layer are designed to scale independently from storage to handle large data volumes; support schema-on-read, support partitioned data and diverse data formats; and orchestrate event-based data processing pipelines.&lt;/p&gt; 
&lt;h3&gt;Logical consumption layer&lt;/h3&gt; 
&lt;p&gt;The consumption layer is responsible for providing scalable tools to gain insights from the vast amount of data in the marketing CDP.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Analytics layer &lt;/strong&gt;– Enables consumption by all user personas through several purpose-built analytics tools that support analysis methods, including ad-hoc SQL queries, batch analytics, business intelligence (BI) dashboards and ML-based insights. Components in this layer should support schema-on-read, data partitioning, and a variety of formats.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data collaboration layer &lt;/strong&gt;– Consists of data clean rooms where organizations can aggregate customer data from different marketing channels or lines of business, and combine it with first-party data to gain insights while enforcing security, anonymization, and compliance controls.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Activation layer&lt;/strong&gt; – This layer integrates customer profiles with the organization. It can also integrate with third-party SaaS providers in the advertising and marketing industry, and is capable of enriching data sets for consumption.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Logical security and governance layer&lt;/h3&gt; 
&lt;p&gt;The Security and Governance layer is responsible for providing mechanisms for access control, encryption, auditing, and data privacy. CDP platforms must securely organize and control the flow of customer event and attribute data. The CDP must manage data, regardless of ingestion method, to unify that data to unique customer profiles, centralizing audience segmentation, and forwarding data to your purpose-built data stores.&lt;/p&gt; 
&lt;p&gt;Privacy regulations, which often vary by region or country, make it necessary to focus on collecting only the vital data for your marketing efforts. The CDP must align to a standards-based security process. There must be procedures in place to audit data collection, follow least privilege data access, and avoid data silos.&lt;/p&gt; 
&lt;p&gt;A marketing CDP must include the following security and governance aspects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Encryption at rest&lt;/strong&gt; – Data must be persisted in encrypted format to protect it from unauthorized access.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Encryption in transit&lt;/strong&gt; – To protect data in transit, &amp;nbsp;encryption protocols such as TLS and certificates to create a secure HTTPS connection to make API requests.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Key management&lt;/strong&gt; – Keys must be managed securely because they grant access to data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secrets management&lt;/strong&gt; – Secrets, such as application passwords and login credentials, must be protected from unintended access.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fine-grained access controls&lt;/strong&gt; – Control data access to only those users that have the right to see the data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data archival &lt;/strong&gt;– Users need to take advantage of storage tiers and data lifecycle policies, which automatically move data to lower cost tiers over time, based on expected access patterns.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Auditing &lt;/strong&gt;– It is critical to monitor and record all activity within the environment with the goal of being able to analyze activity down to individual API call level.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data masking&lt;/strong&gt; – It is important to allow users the ability to automatically detect and optionally mask, substitute, or encrypt/decrypt Personally Identifiable Information (PII). This helps outputs of the CDP to comply with such standards as&lt;a href="https://aws.amazon.com/compliance/hipaa-compliance/"&gt; HIPAA&lt;/a&gt; and &lt;a href="https://aws.amazon.com/compliance/gdpr-center/"&gt;GDPR&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compliance programs&lt;/strong&gt; – Compliance frameworks such as &lt;a href="https://aws.amazon.com/compliance/soc-faqs/"&gt;SOC2&lt;/a&gt;, &lt;a href="https://aws.amazon.com/compliance/gdpr-center/"&gt;GDPR&lt;/a&gt;, &lt;a href="https://aws.amazon.com/compliance/california-consumer-privacy-act/"&gt;CCPA&lt;/a&gt;, and others can be attested by tying together governance-focused, audit-friendly service features with applicable compliance or audit standards.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Conclusion: Using the CDP to better manage customers&lt;/h2&gt; 
&lt;p&gt;In this post, we reviewed a logical CDP data architecture that addresses several complexities at scale, using a decoupled, component-driven architecture. The &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/analytics-lens.html"&gt;Data Analytics Lens&lt;/a&gt;&amp;nbsp;can provide further guidance when designing, deploying, and architecting analytics solution workloads. In addition, ISVs should consider a serverless model for implementation, which helps optimize cost and scalability while reducing the required maintenance on the system.&lt;/p&gt; 
&lt;h2&gt;Further reading&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/solutions/guidance/customer-data-platform-on-aws/"&gt;AWS Guidance on Customer Data Platforms&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Genomics workflows, Part 2: simplify Snakemake launches</title>
		<link>https://aws.amazon.com/blogs/architecture/genomics-workflows-part-2-simplify-snakemake-launches/</link>
					
		
		<dc:creator><![CDATA[Rostislav Markov]]></dc:creator>
		<pubDate>Fri, 09 Dec 2022 14:20:29 +0000</pubDate>
				<category><![CDATA[Amazon CloudWatch]]></category>
		<category><![CDATA[Amazon EC2]]></category>
		<category><![CDATA[Amazon Elastic Container Registry]]></category>
		<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Lambda]]></category>
		<category><![CDATA[AWS Step Functions]]></category>
		<guid isPermaLink="false">09837b773405c27e049dce973424b2c6cbc4d35b</guid>

					<description>Genomics workflows are high-performance computing workloads. In Part 1 of this series, we demonstrated how life-science research teams can focus on scientific discovery without the associated heavy lifting. We used regenie for large genome-wide association studies. Our design pattern built on AWS Step Functions with AWS Batch and Amazon FSx for Lustre. In Part 2, […]</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://aws-samples.github.io/aws-genomics-workflows/"&gt;Genomics workflows&lt;/a&gt; are high-performance computing workloads. In &lt;a href="https://aws.amazon.com/blogs/architecture/automated-launch-of-genomics-workflows/"&gt;Part 1&lt;/a&gt; of this series, we demonstrated how life-science research teams can focus on scientific discovery without the associated heavy lifting. We used &lt;a href="https://rgcgithub.github.io/regenie/"&gt;regenie&lt;/a&gt; for large genome-wide association studies. Our design pattern built on &lt;a href="https://aws.amazon.com/step-functions/"&gt;AWS Step Functions&lt;/a&gt; with &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; and &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In Part 2, we explore genomics workloads with built-in workflow logic. Historically, running bioinformatics data pipelines was a manual and error-prone task. Over the last years, multiple workflow management systems have emerged. An example of these is the &lt;a href="https://snakemake.readthedocs.io/en/stable/index.html"&gt;Snakemake&lt;/a&gt; workflow management system with &lt;a href="https://tibanna.readthedocs.io/en/latest/"&gt;Tibanna&lt;/a&gt; orchestration. We discuss the solution design and how you can fully automate the launch with Amazon Web Services (AWS).&lt;/p&gt; 
&lt;h2&gt;Use case&lt;/h2&gt; 
&lt;p&gt;We focus on the use case of Snakemake, an open-source utility for whole genome sequence mapping in &lt;a href="https://www.jclinepi.com/article/S0895-4356(21)00240-7/fulltext"&gt;directed acyclic graph (DAG)&lt;/a&gt; format. Snakemake uses &lt;a href="https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html"&gt;Snakefiles&lt;/a&gt; to declare workflow steps and commands. A Snakefile extends Python syntax to declare workflow steps such as mapping data sets to DAG structure and identifying variants. Consult the &lt;a href="https://snakemake.readthedocs.io/en/stable/tutorial/basics.html"&gt;Snakemake tutorial&lt;/a&gt; for further information on workflow rules.&lt;/p&gt; 
&lt;p&gt;Snakefiles provide an exception from the general design pattern and an alternative to granular modeling workflow logic in &lt;a href="https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html"&gt;Amazon States Language&lt;/a&gt;. In our real-life use case, we used Tibanna to orchestrate Snakemake. Tibanna is an open-source, AWS-native software that runs bioinformatics data pipelines. It supports Snakefile syntax, plus other workflow languages, including &lt;a href="https://www.commonwl.org/"&gt;Common Workflow Language&lt;/a&gt; and &lt;a href="https://openwdl.org/"&gt;Workflow Description Language (WDL)&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We recommend using &lt;a href="https://aws.amazon.com/genomics-cli/"&gt;Amazon Genomics CLI&lt;/a&gt;, if Tibanna is not needed for your use case, and &lt;a href="https://aws.amazon.com/omics/"&gt;Amazon Omics&lt;/a&gt;, if your workflow definitions are compliant with the supported WDL and &lt;a href="https://www.nextflow.io/"&gt;Nextflow&lt;/a&gt; specifications.&lt;/p&gt; 
&lt;h2&gt;Solution overview&lt;/h2&gt; 
&lt;p&gt;Snakemake is available as Docker image on &lt;a href="https://github.com/snakemake/snakemake"&gt;GitHub&lt;/a&gt;. We push the image to &lt;a href="http://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry&lt;/a&gt;. Tibanna is also available as Docker image on &lt;a href="https://github.com/4dn-dcic/tibanna"&gt;GitHub&lt;/a&gt;—it comes with Snakemake. Consult the &lt;a href="https://tibanna.readthedocs.io/en/latest/installation.html"&gt;Tibanna installation guide&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;We store Snakefiles on &lt;a href="https://docs.aws.amazon.com/s3/?id=docs_gateway"&gt;Amazon Simple Storage Service (Amazon S3)&lt;/a&gt;. We configure &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"&gt;S3 Event Notifications&lt;/a&gt; on &lt;a href="https://s3.amazonaws.com/doc/s3-developer-guide/RESTObjectPUT.html"&gt;PUT&lt;/a&gt; request operations. The event notification triggers an &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; function. The Lambda function launches an &lt;a href="https://docs.aws.amazon.com/ecs/index.html"&gt;AWS Fargate&lt;/a&gt; task, which overrides the task definition command with the appropriate Snakemake start command and arguments.&lt;/p&gt; 
&lt;p&gt;The launched AWS Fargate task pulls the Snakefiles at launch time for each job and prepares the Snakemake initiation commands. Once the Snakefiles are downloaded on the Fargate task, the Snakemake head initiation command is invoked to begin launching jobs using Tibanna. Tibanna invokes a Step Functions state machine which orchestrates the launch of Snakemake on &lt;a href="https://aws.amazon.com/ec2/"&gt;Amazon Elastic Compute Cloud (Amazon EC2)&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/cloudwatch/?id=docs_gateway"&gt;Amazon CloudWatch&lt;/a&gt; provides a consolidated overview of performance metrics, including elapsed time, failed jobs, and error types. You can keep logs of your failed jobs in &lt;a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html"&gt;CloudWatch Logs&lt;/a&gt; (Figure 1). You can set up &lt;a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html"&gt;filters&lt;/a&gt; to match specific error types, plus create &lt;a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html"&gt;subscriptions&lt;/a&gt; to deliver a real-time stream of your log events to &lt;a href="https://docs.aws.amazon.com/kinesis/?id=docs_gateway"&gt;Amazon Kinesis&lt;/a&gt; or Lambda for further retry.&lt;/p&gt; 
&lt;div id="attachment_12618" style="width: 1440px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/06/Figure-1.-Solution-architecture-for-Snakemake-with-Tibanna-on-AWS.png"&gt;&lt;img aria-describedby="caption-attachment-12618" loading="lazy" class="size-full wp-image-12618" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/06/Figure-1.-Solution-architecture-for-Snakemake-with-Tibanna-on-AWS.png" alt="Solution architecture for Snakemake with Tibanna on AWS" width="1430" height="672"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12618" class="wp-caption-text"&gt;Figure 1. Solution architecture for Snakemake with Tibanna on AWS&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Implementation considerations&lt;/h2&gt; 
&lt;p&gt;Here, we describe some of the implementation considerations.&lt;/p&gt; 
&lt;h4&gt;Creating Snakefiles&lt;/h4&gt; 
&lt;p&gt;The launching point for the initiation depends on a Snakefile. Each Snakefile may contain one or more samples to be launched. The sheet resides in an S3 bucket. This adds flexibility and the ability to purge any sensitive or restrictive information after the job has been processed.&lt;/p&gt; 
&lt;h4&gt;Invoking Tibanna&lt;/h4&gt; 
&lt;p&gt;In order to launch Snakemake DAGs using Tibanna, we will need to set up a new Tibanna Unicorn. A Tibanna Unicorn is an Step Functions state machine and a corresponding Lambda function for provisioning EC2 instances.&lt;/p&gt; 
&lt;p&gt;The state machine runs the following sequence:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create EC2 instance&lt;/li&gt; 
 &lt;li&gt;Check EC2 status&lt;/li&gt; 
 &lt;li&gt;Exit&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After the Tibanna Unicorn has been created, we can start a Snakemake DAG using the following sample commands inside of the Fargate task.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ export TIBANNA_DEFAULT_STEP_FUNCTION_NAME=YOUR_UNICORN_PROJECT
$ snakemake --tibanna --tibanna-config spot_instance=true --default-remote-prefix=YOUR_S3_BUCKET/BUCKET_PREFIX --retries 3.&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Snakemake command is used with the &lt;code&gt;--tibanna&lt;/code&gt; flag to send launch requests to the Step Functions state machine in order to provision EC2 instances and run DAG tasks.&lt;/p&gt; 
&lt;p&gt;We recommend deploying the solution with &lt;a href="https://aws.amazon.com/serverless/sam/"&gt;AWS Serverless Application Model&lt;/a&gt; or the &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS Cloud Development Kit&lt;/a&gt;, both of which launch &lt;a href="https://aws.amazon.com/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Logging and troubleshooting&lt;/h4&gt; 
&lt;p&gt;With this solution, each launch will automatically capture and retain start logs in a centralized location in &lt;a href="https://docs.aws.amazon.com/cloudwatch/?id=docs_gateway"&gt;Amazon CloudWatch Logs&lt;/a&gt; for tracing and auditing.&lt;/p&gt; 
&lt;p&gt;If there are issues during the launch of the Tibanna Step Function state machine, such as Amazon EC2 capacity limits, logs will be available in the S3 bucket that was specified during the Tibanna Unicorn creation process. There will be a file available in the format of &lt;code&gt;&amp;lt;EXECUTION_ID&amp;gt;.log&lt;/code&gt; inside of the S3 bucket. This information is easily accessible via the command line interface. Use the following command to display specific log results or error messages.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;tibanna log -j &amp;lt;EXECUTION_ID&amp;gt; -T &lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Retries and EC2 Spot Instances&lt;/h4&gt; 
&lt;p&gt;We advise to use &lt;a href="https://aws.amazon.com/ec2/spot/"&gt;Amazon EC2 Spot Instances&lt;/a&gt;, if possible, for additional cost savings. This option is available in the &lt;code&gt;--tibanna-config&lt;/code&gt; arguments with the setting &lt;code&gt;spot_instance=true&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;This is optional, and you need to create retry logic in the event a Spot Instance gets reclaimed. You can include &lt;code&gt;--retries=3&lt;/code&gt; in your Tibanna launch command. This would ensure all rules are retried three times. You can also specify the number of retries for individual rules when defining the Snakemake DAG definition; for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;rule a:
    output:
        "test.txt"
    retries: 3
    shell:
        "curl https://some.unreliable.server/test.txt &amp;gt; {output}"&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If EC2 Spot Instance capacity is hit, you can automatically switch to using &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html"&gt;EC2 On-Demand Instances&lt;/a&gt; instead. Add the &lt;code&gt;behavior_on_capacity_limit&lt;/code&gt; argument and set &lt;code&gt;retry_without_spot=true&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Adding services&lt;/h4&gt; 
&lt;p&gt;The presented solution can be adapted to use other compute services supported by Snakemake. These include &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon Elastic Kubernetes Service&lt;/a&gt; and &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; with &lt;a href="https://slurm.schedmd.com/documentation.html"&gt;Slurm Workload Manager&lt;/a&gt; plus Amazon FSx for Lustre volumes attached to the head node and cluster nodes.&lt;/p&gt; 
&lt;p&gt;To initiate jobs on ParallelCluster, install the &lt;a href="https://aws.amazon.com/systems-manager/"&gt;AWS Systems Manager&lt;/a&gt; agent on the head node. This is the launching point into the cluster and used for submitting jobs to the initiation queue. Systems Manager is a secure way to remotely invoke commands on an EC2 instance without the need for SSH access. You can restrict access to your EC2 instance through &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html"&gt;IAM policies&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post, we demonstrated how life-science research teams can simplify the launch of Snakemake using AWS. We used Snakefiles and Tibanna to orchestrate workflow steps. Snakefiles provide an exception from the general design pattern and an alternative to Amazon States Language. File uploads to Amazon S3 served as our launching point for workflow initiations.&lt;/p&gt; 
&lt;p&gt;Stay tuned for Part 3 of this series, in which we create a job manager that administrates multiple workflows.&lt;/p&gt; 
&lt;h2&gt;Related information&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/health/genomics/"&gt;Genomics on AWS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/omics/"&gt;Amazon Omics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/genomics-cli/"&gt;Amazon Genomics CLI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Let’s Architect! Optimizing the cost of your architecture</title>
		<link>https://aws.amazon.com/blogs/architecture/lets-architect-optimizing-the-cost-of-your-architecture/</link>
					
		
		<dc:creator><![CDATA[Luca Mezzalira]]></dc:creator>
		<pubDate>Wed, 07 Dec 2022 16:53:00 +0000</pubDate>
				<category><![CDATA[Amazon Aurora]]></category>
		<category><![CDATA[Amazon Elastic Block Store (Amazon EBS)]]></category>
		<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Cost Explorer]]></category>
		<category><![CDATA[AWS Lambda]]></category>
		<category><![CDATA[Graviton]]></category>
		<category><![CDATA[cost optimization]]></category>
		<category><![CDATA[Let's Architect]]></category>
		<category><![CDATA[Networking]]></category>
		<category><![CDATA[serverless]]></category>
		<guid isPermaLink="false">59f7219907b7c9deccd5896dc2adb2a5410af716</guid>

					<description>Written in collaboration with Ben Moses, AWS Senior Solutions Architect, and Michael Holtby, AWS Senior Manager Solutions Architecture Designing an architecture is not a simple task. There are many dimensions and characteristics of a solution to consider, such as the availability, performance, or resilience. In this Let’s Architect!, we explore cost optimization and ideas on […]</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;Written in collaboration with Ben Moses, AWS Senior Solutions Architect, and Michael Holtby, AWS Senior Manager Solutions Architecture&lt;/em&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;Designing an architecture is not a simple task. There are many dimensions and characteristics of a solution to consider, such as the availability, performance, or resilience.&lt;/p&gt; 
&lt;p&gt;In this &lt;em&gt;Let’s Architect!&lt;/em&gt;, we explore cost optimization and ideas on how to rethink your AWS workloads, providing suggestions that span from compute to data transfer.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://aws.amazon.com/blogs/compute/migrating-aws-lambda-functions-to-arm-based-aws-graviton2-processors/"&gt;Migrating AWS Lambda functions to Arm-based AWS Graviton2 processors&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;AWS Graviton processors are custom silicon from Amazon’s Annapurna Labs. Based on the Arm processor architecture, they are optimized for performance and cost, which allows customers to get up to 34% better price performance.&lt;/p&gt; 
&lt;p&gt;This &lt;a href="https://aws.amazon.com/blogs/compute/"&gt;AWS Compute Blog&lt;/a&gt; post discusses some of the differences between the x86 and Arm architectures, as well as methods for developing Lambda functions on Graviton2, including performance benchmarking.&lt;/p&gt; 
&lt;p&gt;Many serverless workloads can benefit from Graviton2, especially when they are not using a library that requires an x86 architecture to run.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://aws.amazon.com/blogs/compute/migrating-aws-lambda-functions-to-arm-based-aws-graviton2-processors/"&gt;Take me to this Compute post!&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_12593" style="width: 800px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/02/Choosing-Graviton2-for-AWS-Lambda-function-in-the-AWS-console.png"&gt;&lt;img aria-describedby="caption-attachment-12593" loading="lazy" class="size-full wp-image-12593" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/02/Choosing-Graviton2-for-AWS-Lambda-function-in-the-AWS-console.png" alt="Choosing Graviton2 for AWS Lambda function in the AWS console" width="790" height="576"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12593" class="wp-caption-text"&gt;Choosing Graviton2 for AWS Lambda function in the AWS console&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;&lt;a href="https://aws.amazon.com/blogs/database/key-considerations-in-moving-to-graviton2-for-amazon-rds-and-amazon-aurora-databases/"&gt;Key considerations in moving to Graviton2 for Amazon RDS and Amazon Aurora databases&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/rds/?id=docs_gateway"&gt;Amazon Relational Database Service (Amazon RDS)&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/rds/?id=docs_gateway"&gt;Amazon Aurora&lt;/a&gt; support a multitude of instance types to scale database workloads based on needs. Both services now support Arm-based AWS Graviton2 instances, which provide up to 52% price/performance improvement for Amazon RDS open-source databases, depending on database engine, version, and workload. They also provide up to 35% price/performance improvement for Amazon Aurora, depending on database size.&lt;/p&gt; 
&lt;p&gt;This &lt;a href="https://aws.amazon.com/blogs/database/"&gt;AWS Database Blog&lt;/a&gt; post showcases strategies for updating RDS DB instances to make use of Graviton2 with minimal changes.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://aws.amazon.com/blogs/database/key-considerations-in-moving-to-graviton2-for-amazon-rds-and-amazon-aurora-databases/"&gt;Take me to this Database post!&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_12594" style="width: 1020px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/02/Choose-your-instance-class-that-leverages-Graviton2-such-as-db.r6g.large-the-g-stands-for-Graviton2.png"&gt;&lt;img aria-describedby="caption-attachment-12594" loading="lazy" class="size-full wp-image-12594" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/02/Choose-your-instance-class-that-leverages-Graviton2-such-as-db.r6g.large-the-g-stands-for-Graviton2.png" alt="Choose your instance class that leverages Graviton2, such as db.r6g.large (the “g” stands for Graviton2)" width="1010" height="1024"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12594" class="wp-caption-text"&gt;Choose your instance class that leverages Graviton2, such as db.r6g.large (the “g” stands for Graviton2)&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures/"&gt;Overview of Data Transfer Costs for Common Architectures&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Data transfer charges are often overlooked while architecting an AWS solution. Considering data transfer charges while making architectural decisions can save costs. This &lt;a href="https://aws.amazon.com/blogs/architecture/"&gt;AWS Architecture Blog&lt;/a&gt; post describes the different flows of traffic within a typical cloud architecture, showing where costs do and do not apply. For areas where cost applies, it shows best-practice strategies to minimize these expenses while retaining a healthy security posture.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures/"&gt;Take me to this Architecture post!&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_12595" style="width: 975px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/02/Accessing-AWS-services-in-different-Regions.png"&gt;&lt;img aria-describedby="caption-attachment-12595" loading="lazy" class="size-full wp-image-12595" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/02/Accessing-AWS-services-in-different-Regions.png" alt="Accessing AWS services in different Regions" width="965" height="518"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12595" class="wp-caption-text"&gt;Accessing AWS services in different Regions&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/journey-to-cloud-native-architecture-series-6-improve-cost-visibility-and-re-architect-for-cost-optimization/"&gt;Improve cost visibility and re-architect for cost optimization&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;This Architecture Blog post is a collection of best practices for cost management in AWS, including the relevant tools; plus, it is part of a series on cost optimization using an e-commerce example.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/aws-cost-management/aws-cost-explorer/"&gt;AWS Cost Explorer&lt;/a&gt; is used to first identify opportunities for optimizations, including data transfer, storage in &lt;a href="https://docs.aws.amazon.com/s3/?id=docs_gateway"&gt;Amazon Simple Storage Service&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/ebs/?id=docs_gateway"&gt;Amazon Elastic Block Store&lt;/a&gt;, idle resources, and the use of Graviton2 (Amazon’s Arm-based custom silicon). The post discusses establishing a FinOps culture and making use of Service Control Policies (SCPs) to control ongoing costs and guide deployment decisions, such as instance-type selection.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/journey-to-cloud-native-architecture-series-6-improve-cost-visibility-and-re-architect-for-cost-optimization/"&gt;Take me to this Architecture post!&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_12596" style="width: 1032px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/02/Applying-SCPs-on-different-environments-for-cost-control.png"&gt;&lt;img aria-describedby="caption-attachment-12596" loading="lazy" class="size-full wp-image-12596" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/02/Applying-SCPs-on-different-environments-for-cost-control.png" alt="Applying SCPs on different environments for cost control" width="1022" height="556"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12596" class="wp-caption-text"&gt;Applying SCPs on different environments for cost control&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;See you next time!&lt;/h2&gt; 
&lt;p&gt;Thanks for joining us to discuss optimizing costs while architecting! This is the last &lt;em&gt;Let’s Architect!&lt;/em&gt; post of 2022. We will see you again in 2023, when we explore even more architecture topics together.&lt;/p&gt; 
&lt;p&gt;Wishing you a happy holiday season and joyous new year!&lt;/p&gt; 
&lt;h2&gt;Can’t get enough of&amp;nbsp;&lt;em&gt;Let’s Architect!&lt;/em&gt;?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/tag/lets-architect/"&gt;Visit the &lt;em&gt;Let’s Architect!&lt;/em&gt; page of the AWS Architecture Blog for access to the whole series.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Looking for more architecture content?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/architecture/"&gt;AWS Architecture Center&lt;/a&gt;&amp;nbsp;provides reference architecture diagrams, vetted architecture solutions, Well-Architected best practices, patterns, icons, and more!&lt;/p&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Email delta cost usage report in a multi-account organization using AWS Lambda</title>
		<link>https://aws.amazon.com/blogs/architecture/email-delta-cost-usage-report-in-a-multi-account-organization-using-aws-lambda/</link>
					
		
		<dc:creator><![CDATA[Ashutosh Dubey]]></dc:creator>
		<pubDate>Mon, 05 Dec 2022 18:59:07 +0000</pubDate>
				<category><![CDATA[Amazon CloudWatch]]></category>
		<category><![CDATA[Amazon Simple Email Service (SES)]]></category>
		<category><![CDATA[AWS Cloud Financial Management]]></category>
		<category><![CDATA[AWS Cost Explorer]]></category>
		<category><![CDATA[AWS Lambda]]></category>
		<category><![CDATA[Customer Solutions]]></category>
		<category><![CDATA[Messaging]]></category>
		<guid isPermaLink="false">cfff486b5ae1a58df560e55aff0d3ac1d3dae232</guid>

					<description>AWS Organizations gives customers the ability to consolidate their billing across accounts. This reduces billing complexity and centralizes cost reporting to a single account. These reports and cost information are available only to users with billing access to the primary AWS account. In many cases, there are members of senior leadership or finance decision makers […]</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://aws.amazon.com/organizations/"&gt;AWS Organizations&lt;/a&gt; gives customers the ability to consolidate their billing across accounts. This reduces billing complexity and centralizes cost reporting to a single account. These reports and cost information are available only to users with billing access to the primary AWS account.&lt;/p&gt; 
&lt;p&gt;In many cases, there are members of senior leadership or finance decision makers who don’t have access to AWS accounts, and therefore depend on individuals or additional custom processes to share billing information. This task becomes specifically complicated when there is a complex account organization structure in place.&lt;/p&gt; 
&lt;p&gt;In such cases, you can email cost reports periodically and automatically to these groups or individuals using &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt;. In this blog post, you’ll learn how to send automated emails for AWS billing usage and consumption drifts from previous days.&lt;/p&gt; 
&lt;h2&gt;Solution architecture&lt;/h2&gt; 
&lt;div id="attachment_12498" style="width: 987px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/18/email-delta-ra.png"&gt;&lt;img aria-describedby="caption-attachment-12498" loading="lazy" class="size-full wp-image-12498" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta-ra.png" alt="Account structure and architecture diagram" width="977" height="689"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12498" class="wp-caption-text"&gt;Figure 1. Account structure and architecture diagram&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;AWS provides the &lt;a href="https://docs.aws.amazon.com/cost-management/latest/userguide/ce-api.html"&gt;Cost Explorer API&lt;/a&gt; to enable you to programmatically query data for cost and usage of AWS services. This solution uses a Lambda function to query aggregated data from the API, format that data and send it to a defined list of recipients.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; (Amazon CloudWatch Events) is configured to cue the Lambda function at a specific time.&lt;/li&gt; 
 &lt;li&gt;The function uses the &lt;a href="https://aws.amazon.com/aws-cost-management/aws-cost-explorer/"&gt;AWS Cost Explorer&lt;/a&gt; API to fetch the cost details for each account.&lt;/li&gt; 
 &lt;li&gt;The Lambda function calculates the change in cost over time and formats the information to be sent in an email.&lt;/li&gt; 
 &lt;li&gt;The formatted information is passed to &lt;a href="https://aws.amazon.com/ses/"&gt;Amazon Simple Email Service&lt;/a&gt; (Amazon SES).&lt;/li&gt; 
 &lt;li&gt;The report is emailed to the recipients configured in the environment variables of the function.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;For this walkthrough, you should have the following prerequisites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An &lt;a href="https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&amp;amp;client_id=signup"&gt;AWS account&lt;/a&gt; with &lt;a href="https://docs.aws.amazon.com/cost-management/latest/userguide/ce-enable.html"&gt;Cost Explorer enabled&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;AWS &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html"&gt;user or role&lt;/a&gt; with permissions to deploy the &lt;a href="https://aws.amazon.com/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt; template.&lt;/li&gt; 
 &lt;li&gt;Valid email IDs to receive email notifications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Walkthrough&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download the AWS CloudFormation template from this link: &lt;a href="https://aws-big-data-blog.s3.amazonaws.com/cost-explorer-blog/DailyUsage_Summary.yaml"&gt;AWS CloudFormation template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Once downloaded, open the template in your favorite text editor&lt;/li&gt; 
 &lt;li&gt;Update account-specific variables in the template. You need to update the &lt;a href="https://en.wikipedia.org/wiki/Tuple"&gt;tuple&lt;/a&gt;, dictionary, display list, and display list monthly sections of the script for all the accounts which you want to appear in the daily report email. Refer to Figure 2 for an example of some dummy account IDs and email IDs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div id="attachment_12500" style="width: 878px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta1.png"&gt;&lt;img aria-describedby="caption-attachment-12500" loading="lazy" class="size-full wp-image-12500" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta1.png" alt="A screenshot showing account IDs in AWS Lambda" width="868" height="702"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12500" class="wp-caption-text"&gt;Figure 2. Account IDs in AWS Lambda&lt;/p&gt;
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optionally, locate “&lt;code&gt;def send_report_email&lt;/code&gt;” in the template. The subject variable controls the subject line of the email. This can be modified to something meaningful to the recipients.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After these changes are made according to your requirements, you can deploy the CloudFormation template:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Log in to the &lt;a href="https://console.aws.amazon.com/cloudformation/"&gt;Cloud Formation console&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Create Stack&lt;/strong&gt;. From the dropdown, choose &lt;strong&gt;With new resources (standard)&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;On the next screen under &lt;strong&gt;Specify Template&lt;/strong&gt;, choose &lt;strong&gt;Upload a template file&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Choose file&lt;/strong&gt;. Choose the local template you modified earlier, then choose&lt;strong&gt; Next&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Fill out the parameter fields with valid email address. For &lt;strong&gt;&lt;code&gt;SchduleExpression&lt;/code&gt;&lt;/strong&gt;, use a valid &lt;a href="https://cron.com/"&gt;Cron expression&lt;/a&gt; for when you would like the report sent. Choose &lt;strong&gt;Next&lt;/strong&gt;.&lt;br&gt; Here is an example for a cron schedule: &amp;nbsp;&lt;code&gt;18 11 * * ? *&lt;/code&gt;&lt;br&gt; (This example cron expression sets the schedule to send every day at 11:18 UTC time.)&lt;br&gt; This creates the Lambda function and needed &lt;a href="https://aws.amazon.com/iam/"&gt;AWS Identity and Access Management&lt;/a&gt; (AWS IAM) roles.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You will now need to make a few modifications to the created resources.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Log in to the &lt;a href="http://console.aws.amazon.com/iam/"&gt;IAM console&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Roles&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Locate the role created by the CloudFormation template called “&lt;code&gt;daily-services-usage-lambdarole&lt;/code&gt;”&lt;/li&gt; 
 &lt;li&gt;Under the Permissions tab, choose Add Permissions. From the dropdown., choose &lt;strong&gt;Attach Policy&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;In the search bar, search for “Billing”.&lt;/li&gt; 
 &lt;li&gt;Select the check box next to the AWS Managed Billing Policy and then choose &lt;strong&gt;Attach Policy&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Log in to the &lt;a href="http://console.aws.amazon.com/lambda"&gt;AWS Lambda console&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose the &lt;code&gt;&lt;strong&gt;DailyServicesUsage&lt;/strong&gt;&lt;/code&gt; function.&lt;/li&gt; 
 &lt;li&gt;Choose the &lt;strong&gt;Configuration&lt;/strong&gt; tab.&lt;/li&gt; 
 &lt;li&gt;In the options that appear, choose &lt;strong&gt;General Configuration&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose the &lt;strong&gt;Edit&lt;/strong&gt; button.&lt;/li&gt; 
 &lt;li&gt;Change the timeout option to 10 seconds, because the default of three seconds may not be enough time to run the function to retrieve the cost details from multiple accounts.&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Save&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Still under the &lt;strong&gt;General Configuration&lt;/strong&gt; tab, choose the &lt;strong&gt;Permissions&lt;/strong&gt; option and validate the execution role.&lt;br&gt; The edited IAM execution role should display the &lt;strong&gt;Resource&lt;/strong&gt; details for which the access has been gained. Figure 3 shows that the allow actions to &lt;code&gt;aws-portal&lt;/code&gt; for &lt;code&gt;Billing&lt;/code&gt;, &lt;code&gt;Usage&lt;/code&gt;, &lt;code&gt;PaymentMethods&lt;/code&gt;, and ViewBilling are enabled. If the Resource summary does not show these permissions, the IAM role is likely not correct. Go back to the IAM console and confirm that you updated the correct role with billing access.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_12506" style="width: 909px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta2.png"&gt;&lt;img aria-describedby="caption-attachment-12506" loading="lazy" class="size-full wp-image-12506" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta2.png" alt="A screenshot of the AWS Lambda console showing Lambda role permissions" width="899" height="460"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12506" class="wp-caption-text"&gt;Figure 3. Lambda role permissions&lt;/p&gt;
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optionally, in the left navigation pane, choose &lt;strong&gt;Environment variables&lt;/strong&gt;. Here you will see the email recipients you configured in the Cloud Formation template. If changes are needed to the list in the future, you can add or remove recipients by editing the environment variables. You can skip this step if you’re satisfied with the parameters you specified earlier.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Next, you will create a few Amazon SES identities for the email addresses that were provided as environment variables for the sender and recipients:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Log in to the &lt;a href="http://console.aws.amazon.com/ses"&gt;SES console&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Configuration&lt;/strong&gt;, choose &lt;strong&gt;Verified Identities&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Create Identity&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose the identity type &lt;strong&gt;Email Address&lt;/strong&gt;, fill out the &lt;strong&gt;Email address&lt;/strong&gt; field with the sender email, and choose &lt;strong&gt;Create Identify&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Repeat this step for all receiver emails.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The email IDs included will receive an email for the confirmation. Once confirmed, the status shows as verified in the &lt;strong&gt;Verified Identities&lt;/strong&gt; tab of the SES console. The verified email IDs will start receiving the email with the cost reports.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Amazon EventBridge (CloudWatch) event configuration&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To configure events:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li style="list-style-type: none"&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Go to the &lt;a href="http://console.aws.amazon.com/events"&gt;Amazon EventBridge console&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Choose &lt;strong&gt;Create rule&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;Fill out the rule details with meaningful descriptions.&lt;/li&gt; 
   &lt;li&gt;Under &lt;strong&gt;Rule Type&lt;/strong&gt;, choose &lt;strong&gt;Schedule&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;Schedule the cron pattern from when you would like the report to run.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Figure 4 shows that the highlighted rule is configured to run the Lambda function every 24 hours.&lt;/p&gt; 
&lt;div id="attachment_12509" style="width: 909px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta3.png"&gt;&lt;img aria-describedby="caption-attachment-12509" loading="lazy" class="size-full wp-image-12509" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta3.png" alt="A screenshot of the Amazon EventBridge console showing an EventBridge rule" width="899" height="460"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12509" class="wp-caption-text"&gt;Figure 4. EventBridge rule&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;An example AWS Daily Cost Report email&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;From&lt;/strong&gt;:&amp;nbsp;xxx@example.com (the email ID mentioned as “sender”)&lt;br&gt; &lt;strong&gt;Sent&lt;/strong&gt;:&amp;nbsp;Tuesday, April 12, 2022 1:43 PM&lt;br&gt; &lt;strong&gt;To&lt;/strong&gt;:&amp;nbsp;yyy@example.com (the email ID mentioned as “receiver”)&lt;br&gt; &lt;strong&gt;Subject&lt;/strong&gt;:&amp;nbsp;AWS Daily Cost Report for Selected Accounts (the subject of email as set in the Lambda function)&lt;/p&gt; 
&lt;p&gt;Figure 5 shows the first part of the cost report. It provides the cost summary and delta of the cost variance percentage compare to the previous day. You can also see the trend based on the last seven days from the same table. This helps in understanding a pattern around cost and usage.&lt;/p&gt; 
&lt;p&gt;This summary is broken down per account, and then totaled, in order to help you understand the accounts contributing to the cost changes. The daily change percentages are also color coded to highlight significant variations.&lt;/p&gt; 
&lt;div id="attachment_12514" style="width: 987px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta4.png"&gt;&lt;img aria-describedby="caption-attachment-12514" loading="lazy" class="size-full wp-image-12514" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta4.png" alt="AWS Daily Cost Report email body part 1" width="977" height="268"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12514" class="wp-caption-text"&gt;Figure 5. AWS Daily Cost Report email body part 1&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The second part of the report in the email provides the service-related cost breakup for each account configured in the Account dictionary section of the function. This is a further drilldown report; you will get these for all configured accounts.&lt;/p&gt; 
&lt;div id="attachment_12515" style="width: 987px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta5.png"&gt;&lt;img aria-describedby="caption-attachment-12515" loading="lazy" class="size-full wp-image-12515" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/19/email-delta5.png" alt="AWS Daily Cost Report email body part 2" width="977" height="231"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12515" class="wp-caption-text"&gt;Figure 6. AWS Daily Cost Report email body part 2&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Cleanup&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Delete the Amazon CloudFormation stack.&lt;/li&gt; 
 &lt;li&gt;Delete the identities on Amazon SES.&lt;/li&gt; 
 &lt;li&gt;Delete the Amazon EventBridge (CloudWatch) event rule.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;The blog demonstrates how you can automatically and seamlessly share your AWS accounts’ billing and change information with your leadership and finance teams daily (or on any schedule you choose). While the solution was designed for accounts that are part of an organization in the service AWS organizations, it could also be deployed in a standalone account without making any changes. This allows information sharing without the need to provide account access to the recipients, and avoids any dependency on other manual processes. As a next step, you can also store these reports in &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3), generate a historical trend summary for consumption, and continue making informed decisions.&lt;/p&gt; 
&lt;h2&gt;Additional reading&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/aws-cloud-financial-management/aws-cloud-financial-management-2022-q1-recap/"&gt;AWS Cloud Financial Management 2022 Q1 Recap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/aws-cloud-financial-management/aws-cost-anomaly-detection-alert-notifications-in-slack-through-aws-chatbot/"&gt;Get AWS Cost Anomaly Detection alert notifications in Slack through AWS Chatbot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Amazon CloudWatch Insights for Amazon EKS on EC2 using AWS Distro for OpenTelemetry Helm charts</title>
		<link>https://aws.amazon.com/blogs/architecture/amazon-cloudwatch-insights-for-amazon-eks-on-ec2-using-aws-distro-for-opentelemetry-helm-charts/</link>
					
		
		<dc:creator><![CDATA[Vimala Pydi]]></dc:creator>
		<pubDate>Fri, 02 Dec 2022 16:00:09 +0000</pubDate>
				<category><![CDATA[Amazon CloudWatch]]></category>
		<category><![CDATA[Amazon EC2]]></category>
		<category><![CDATA[Amazon Elastic Kubernetes Service]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Distro for OpenTelemetry]]></category>
		<guid isPermaLink="false">4a4962051fc0d0ddef4e1308ec2e89d14fc17fd8</guid>

					<description>This blog provides a simplified three-step solution to collect metrics and logs from an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon Elastic Compute Cloud (Amazon EC2) using the AWS Distro for OpenTelemetry (ADOT) Helm charts repository and send them to Amazon CloudWatch Logs and Amazon CloudWatch Container Insights. The ADOT Helm charts repository […]</description>
										<content:encoded>&lt;p&gt;This blog provides a simplified three-step solution to collect metrics and logs from an &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon Elastic Kubernetes Service&lt;/a&gt; (Amazon EKS) cluster on &lt;a href="https://aws.amazon.com/ec2/"&gt;Amazon Elastic Compute Cloud&lt;/a&gt; (Amazon EC2) using the &lt;a href="https://github.com/aws-observability/aws-otel-helm-charts"&gt;AWS Distro for OpenTelemetry (ADOT) Helm charts repository&lt;/a&gt; and send them to &lt;a href="https://aws.amazon.com/cloudwatch/"&gt;Amazon CloudWatch&lt;/a&gt; Logs and Amazon CloudWatch Container Insights. The ADOT Helm charts repository contains Helm charts to provide easy mechanisms to set up the ADOT Collector and other collection agents like &lt;a href="https://fluentbit.io/"&gt;fluentbit&lt;/a&gt; to collect telemetry data such as metrics, logs and traces to send to AWS monitoring services.&lt;/p&gt; 
&lt;p&gt;Amazon EKS is a managed Kubernetes service that makes it easy for organizations to run Kubernetes on AWS Cloud and on premises. Organizations use Amazon EKS to automatically manage the availability and scalability of the Kubernetes control plane nodes responsible for scheduling containers, managing application availability, storing cluster data, and performing other key tasks. &lt;a href="https://aws-otel.github.io/"&gt;ADOT&lt;/a&gt; is a secure, production-ready, AWS-supported distribution of the OpenTelemetry project. Applications can set up ADOT Collector and other collector agents only once to send correlated metrics and traces to multiple AWS and Partner monitoring solutions. Fluent Bit is an open-source log processor and forwarder that you can use to collect data such as metrics and logs from different sources. &lt;a href="https://helm.sh/"&gt;Helm&lt;/a&gt; deploys packaged applications to Kubernetes and structures them into Helm charts.&lt;/p&gt; 
&lt;h2&gt;Solution overview&lt;/h2&gt; 
&lt;p&gt;A high-level architecture diagram depicted in Figure 1 shows a simple solution for collecting metrics and logs to send to Amazon CloudWatch Container Insights by installing an ADOT Helm chart on your existing or new Amazon EKS cluster.&lt;/p&gt; 
&lt;p&gt;Here are the steps to set up an ADOT and fluentbit collector:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Set up your environment and install the necessary tools to connect to an existing or newly created Amazon EKS cluster.&lt;/li&gt; 
 &lt;li&gt;Configure the necessary roles for AWS Identity and Access Management (IAM) roles for service accounts and install Helm charts for ADOT, enabling fluentbit.&lt;/li&gt; 
 &lt;li&gt;Monitor logs, metrics, and traces from Amazon CloudWatch Logs and Container Insights.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_12567" style="width: 738px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig1-helm-chart-diagram.png"&gt;&lt;img aria-describedby="caption-attachment-12567" loading="lazy" class="size-full wp-image-12567" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig1-helm-chart-diagram.png" alt="Architecture diagram for Helm chart installation of ADOT and fluentbit to an existing Amazon EKS cluster" width="728" height="506"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12567" class="wp-caption-text"&gt;Figure 1. Architecture diagram for Helm chart installation of ADOT and fluentbit to an existing Amazon EKS cluster&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Existing AWS account with access to AWS Management Console&lt;/li&gt; 
 &lt;li&gt;Intermediate-level knowledge and understanding of Amazon EKS&lt;/li&gt; 
 &lt;li&gt;An existing or new Amazon EKS cluster&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install the tools&lt;/h2&gt; 
&lt;p&gt;In this blog, &lt;a href="https://aws.amazon.com/cloud9/"&gt;AWS Cloud9&lt;/a&gt; is used as an environment to connect to the Amazon EKS cluster and install Helm charts. If you choose to use AWS Cloud9, follow the step-by-step instructions provided in &lt;a href="https://docs.aws.amazon.com/cloud9/latest/user-guide/create-environment-main.html"&gt;Creating an EC2 Environment&lt;/a&gt;. Refer to &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html"&gt;Getting started with Amazon EKS&lt;/a&gt; for additional instructions to install eksctl, create EKS clusters, and set up required IAM permissions for connecting to an EKS cluster.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Log in to your Amazon EKS cluster and inspect the cluster. Select an EKS cluster in AWS Management Console. On the &lt;strong&gt;Resources&lt;/strong&gt; tab, check the &lt;strong&gt;DaemonSets&lt;/strong&gt;, as in Figure 2a. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12570" style="width: 1450px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig2a-eks-cluster-daemonset.png"&gt;&lt;img aria-describedby="caption-attachment-12570" loading="lazy" class="size-full wp-image-12570" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig2a-eks-cluster-daemonset.png" alt="EKS cluster DaemonSets" width="1440" height="600"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12570" class="wp-caption-text"&gt;Figure 2a. EKS cluster DaemonSets&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Open Amazon CloudWatch and inspect the Log groups and Amazon CloudWatch Container Insights. Note that the Log groups and Amazon CloudWatch Container Insights in Figure 2b do not show any EKS cluster-specific logs. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12572" style="width: 1441px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig2b-container-insights.png"&gt;&lt;img aria-describedby="caption-attachment-12572" loading="lazy" class="size-full wp-image-12572" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig2b-container-insights.png" alt="Container Insights before ADOT and fluentbit collector installation" width="1431" height="435"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12572" class="wp-caption-text"&gt;Figure 2b. Container Insights before ADOT and fluentbit collector installation&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Install Helm and configure IAM roles&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run the following command to install Helm, verify the version, and configure Bash completion for the Helm command: &lt;pre&gt;&lt;code class="lang-bash"&gt;curl -ssl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
helm version --short

helm completion bash &amp;gt;&amp;gt; ~/.bash_completion
. /etc/profile.d/bash_completion.sh
. ~/.bash_completion
source &amp;lt;(helm completion bash)&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Set up IAM roles for service accounts.&lt;br&gt; Replace XXX in the following commands with your EKS Cluster name.&lt;p&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="lang-bash"&gt;eksctl create iamserviceaccount \
--name fluent-bit \
--role-name EKS-ADOT-CWCI-Helm-Chart-Role-CW \
--namespace amazon-cloudwatch \
--cluster XXX \
--attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \
--role-only \
--approve
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="lang-bash"&gt;eksctl create iamserviceaccount \
--name adot-collector-sa \
--role-name EKS-ADOT-CWCI-Helm-Chart-Role-METRICS \
--namespace amazon-metrics \
--cluster XXX \
--attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \
--role-only \
--approve
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Deploy the ADOT Helm chart.&lt;br&gt; Replace XXX in the following code with your EKS Cluster name.&lt;p&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="lang-bash"&gt;CWCI_ADOT_HELM_ROLE_ARN_CW=$(aws iam get-role --role-name EKS-ADOT-CWCI-Helm-Chart-Role-CW | jq .Role.Arn -r)
CWCI_ADOT_HELM_ROLE_ARN_METRICS=$(aws iam get-role --role-name EKS-ADOT-CWCI-Helm-Chart-Role-METRICS | jq .Role.Arn -r)
helm repo add adot-helm-repo https://aws-observability.github.io/aws-otel-helm-charts
helm install adot-release adot-helm-repo/adot-exporter-for-eks-on-ec2  \
--set clusterName=XXX --set awsRegion=us-east-1 --set fluentbit.enabled=true \
--set adotCollector.daemonSet.service.metrics.receivers={awscontainerinsightreceiver} \
--set adotCollector.daemonSet.service.metrics.exporters={awsemf} \
--set adotCollector.daemonSet.cwexporters.logStreamName=EKSNode \
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Run the following commands to validate the successful deployment. 
  &lt;ul&gt; 
   &lt;li&gt;Verify that two new namespaces have been created.&lt;br&gt; &lt;code&gt;kubectl get ns&lt;/code&gt;&lt;br&gt; The result should be:&lt;p&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="lang-bash"&gt;$ kubectl get ns
NAME                STATUS           AGE
amazon-cloudwatch   Active           2d20h
amazon-metrics      Active           2d20h&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;Verify that a fluentbit pod was enabled as part of the ADOT Helm Chart under the &lt;em&gt;amazon-cloudwatch&lt;/em&gt; namespace.&lt;br&gt; &lt;code&gt;kubectl get all -n amazon-cloudwatch&lt;/code&gt;&lt;br&gt; The result should be:&lt;p&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="lang-bash"&gt;kubectl get all -n amazon-cloudwatch
NAME                   READY   STATUS    RESTARTS   AGE
pod/fluent-bit-9lrnt   1/1     Running   0          2d20h
pod/fluent-bit-h9lvt   1/1     Running   0          2d20h
pod/fluent-bit-nbqjm   1/1     Running   0          2d20h

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;Verify the &lt;em&gt;adot-collector-pod&lt;/em&gt; under the &lt;em&gt;amazon-metrics&lt;/em&gt; namespace.&lt;br&gt; &lt;code&gt;kubectl get all -n amazon-metrics&lt;/code&gt;&lt;br&gt; The result should be:&lt;p&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="lang-bash"&gt;$ kubectl get all -n amazon-metrics
NAME                                 READY   STATUS    RESTARTS   AGE
pod/adot-collector-daemonset-6qcsd   1/1     Running   0          2d20h
pod/adot-collector-daemonset-f92fr   1/1     Running   0          2d20h
pod/adot-collector-daemonset-gmhbx   1/1     Running   0          2d20h

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/adot-collector-daemonset   3         3         3       3            3           &amp;lt;none&amp;gt;          2d20h&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Validate the installation through the Amazon EKS cluster.&lt;br&gt; Go to the Amazon EKS cluster and select the &lt;strong&gt;Resources&lt;/strong&gt; tab. Under &lt;strong&gt;Workloads&lt;/strong&gt;, select &lt;strong&gt;DaemonSets,&lt;/strong&gt; and find the fluent-bit and adot-collector-daemonsets as demonstrated in Figure 3.&lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12573" style="width: 1438px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig3-daemonsets.png"&gt;&lt;img aria-describedby="caption-attachment-12573" loading="lazy" class="size-full wp-image-12573" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig3-daemonsets.png" alt="DaemonSet under Amazon EKS cluster resources" width="1428" height="600"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12573" class="wp-caption-text"&gt;Figure 3. DaemonSet under Amazon EKS cluster resources&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Monitor logs, metrics, and traces&lt;/h2&gt; 
&lt;p&gt;Monitor the CloudWatch Logs and CloudWatch Insights.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the &lt;strong&gt;Logs&lt;/strong&gt; section, choose &lt;strong&gt;Log groups&lt;/strong&gt; to view Amazon EKS cluster log groups with a prefix of &lt;em&gt;/aws/containerinsights&lt;/em&gt;, as in Figure 4a. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12574" style="width: 1439px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig4a-eks-cluster.png"&gt;&lt;img aria-describedby="caption-attachment-12574" loading="lazy" class="size-full wp-image-12574" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig4a-eks-cluster.png" alt="EKS cluster log groups" width="1429" height="362"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12574" class="wp-caption-text"&gt;Figure 4a. EKS cluster log groups&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;In the &lt;strong&gt;Insights&lt;/strong&gt; section, choose &lt;strong&gt;Container Insights&lt;/strong&gt; to view all the resources within your Amazon EKS cluster, as in Figure 4b. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12575" style="width: 1440px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig4b-eks-cluster.png"&gt;&lt;img aria-describedby="caption-attachment-12575" loading="lazy" class="size-full wp-image-12575" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig4b-eks-cluster.png" alt="EKS cluster's Container Insights resources" width="1430" height="651"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12575" class="wp-caption-text"&gt;Figure 4b. EKS cluster’s Container Insights resources&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;On the &lt;strong&gt;Container Insights&lt;/strong&gt; page, select &lt;strong&gt;Container map&lt;/strong&gt; from the dropdown to check the container map for Amazon EKS clusters, as demonstrated in Figure 4c. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12576" style="width: 1441px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig4c-eks-cluster.png"&gt;&lt;img aria-describedby="caption-attachment-12576" loading="lazy" class="size-full wp-image-12576" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig4c-eks-cluster.png" alt="EKS cluster's Container Insights container map" width="1431" height="578"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12576" class="wp-caption-text"&gt;Figure 4c. EKS cluster’s Container Insights container map&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;On the &lt;strong&gt;Container Insights&lt;/strong&gt; page, select &lt;strong&gt;Performance monitoring&lt;/strong&gt; from the dropdown to view various performance metrics for Amazon EKS cluster, as demonstrated in Figure 4d. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12577" style="width: 2247px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig4d-eks-cluster.png"&gt;&lt;img aria-describedby="caption-attachment-12577" loading="lazy" class="size-full wp-image-12577" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/12/01/fig4d-eks-cluster.png" alt="EKS cluster's Container Insights performance monitoring" width="2237" height="987"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12577" class="wp-caption-text"&gt;Figure 4d. EKS cluster’s Container Insights performance monitoring&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cleanup&lt;/h2&gt; 
&lt;p&gt;If you are no longer using the resources discussed in this blog, remove the excess AWS resources to avoid incurring charges. After you finish setting up ADOT and fluentbit collectors to send logs and metrics to Amazon CloudWatch Logs and Container Insights, clean up resources by &lt;a href="https://helm.sh/docs/helm/helm_uninstall/"&gt;uninstalling the ADOT Helm chart&lt;/a&gt;, &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_manage_delete.html"&gt;deleting IAM Roles&lt;/a&gt; created for the services, &lt;a href="https://docs.aws.amazon.com/solutions/latest/cloudwatch-monitoring-on-aws/uninstall-the-solution.html#delete-cloudwatch-log-groups"&gt;deleting CloudWatch Logs&lt;/a&gt;, and &lt;a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights-update-delete.html"&gt;deleting Container Insights&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog we walked through a simple three-step solution to set up Amazon EKS cluster logs and Container Insights using Helm charts. The Helm chart installs ADOT and fluentbit as a DaemonSet in the existing EKS cluster to collect and port logs, metrics, and traces to Amazon CloudWatch Logs and Container Insights. The Amazon CloudWatch Container Insights provide insights into resources, monitor performance, and container map of all the resources within the Amazon EKS cluster.&lt;/p&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Optimize your modern data architecture for sustainability: Part 2 – unified data governance, data movement, and purpose-built analytics</title>
		<link>https://aws.amazon.com/blogs/architecture/optimize-your-modern-data-architecture-for-sustainability-part-2-unified-data-governance-data-movement-and-purpose-built-analytics/</link>
					
		
		<dc:creator><![CDATA[Sam Mokhtari]]></dc:creator>
		<pubDate>Fri, 25 Nov 2022 13:24:46 +0000</pubDate>
				<category><![CDATA[Amazon Redshift]]></category>
		<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Glue]]></category>
		<category><![CDATA[AWS Well-Architected]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">119f126e63b7832a0602920a9b34a62671b4c462</guid>

					<description>In the first part of this blog series, Optimize your modern data architecture for sustainability: Part 1 – data ingestion and data lake, we focused on the 1) data ingestion, and 2) data lake pillars of the modern data architecture. In this blog post, we will provide guidance and best practices to optimize the components […]</description>
										<content:encoded>&lt;p&gt;In the first part of this blog series, &lt;a href="https://aws.amazon.com/blogs/architecture/optimize-your-modern-data-architecture-for-sustainability-part-1-data-ingestion-and-data-lake/"&gt;Optimize your modern data architecture for sustainability: Part 1 – data ingestion and data lake&lt;/a&gt;, we focused on the 1) data ingestion, and 2) data lake pillars of the &lt;a href="https://aws.amazon.com/big-data/datalakes-and-analytics/modern-data-architecture/"&gt;modern data architecture&lt;/a&gt;. In this blog post, we will provide guidance and best practices to optimize the components within the 3) unified data governance, 4) data movement, and 5) purpose-built analytics pillars.&lt;br&gt; Figure 1 shows the different pillars of the modern data architecture. It includes data ingestion, data lake, unified data governance, data movement, and purpose-built analytics pillars.&lt;/p&gt; 
&lt;div id="attachment_12272" style="width: 1147px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12272" loading="lazy" class="size-full wp-image-12272" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/02/Fig1-MDARefArch.png" alt="Modern Data Analytics Reference Architecture on AWS" width="1137" height="688"&gt;
 &lt;p id="caption-attachment-12272" class="wp-caption-text"&gt;Figure 1. Modern Data Analytics Reference Architecture on AWS&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;3. Unified data governance&lt;/h2&gt; 
&lt;p&gt;A centralized Data Catalog is responsible for storing business and technical metadata about datasets in the storage layer. Administrators apply permissions in this layer and track events for security audits.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Data discovery&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;To increase data sharing and reduce data movement and duplication, enable data discovery and well-defined access controls for different user personas. This reduces redundant data processing activities. Separate teams within an organization can rely on this central catalog. It provides first-party data (such as sales data) or third-party data (such as stock prices, climate change datasets). You’ll only need access data once, rather than having to pull from source repeatedly.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/glue"&gt;AWS Glue&lt;/a&gt;&amp;nbsp;Data Catalog can simplify the process for adding and searching metadata. Use AWS Glue crawlers to update the existing schemas and discover new datasets. Carefully plan schedules to reduce unnecessary crawling.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Data sharing&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Establish well-defined access control mechanisms for different data consumers using services such as &lt;a href="https://www.amazonaws.cn/en/lake-formation/"&gt;AWS Lake Formation&lt;/a&gt;. This will enable datasets to be shared between organizational units with fine-grained access control, which reduces redundant copying and movement. Use &lt;a href="https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html"&gt;Amazon Redshift data sharing&lt;/a&gt; to avoid copying the data across data warehouses.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Well-defined datasets&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Create well-defined datasets and associated metadata to avoid unnecessary data wrangling and manipulation. This will reduce resource usage that might result from additional data manipulation.&lt;/p&gt; 
&lt;h2&gt;4. Data movement&lt;/h2&gt; 
&lt;p&gt;AWS Glue provides serverless,&amp;nbsp;&lt;a href="https://aws.amazon.com/glue/pricing/"&gt;pay-per-use&lt;/a&gt; data movement capability, without having to stand up and manage servers or clusters. Set up ETL pipelines that can process tens of terabytes of data.&lt;/p&gt; 
&lt;p&gt;To minimize idle resources without sacrificing performance, use &lt;a href="https://docs.aws.amazon.com/glue/latest/dg/auto-scaling.html"&gt;auto scaling for AWS Glue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can create and share AWS Glue workflows for similar use cases by using &lt;a href="https://docs.aws.amazon.com/glue/latest/dg/blueprints-overview.html"&gt;AWS Glue blueprints&lt;/a&gt;, rather than creating an AWS Glue workflow for &lt;em&gt;each&lt;/em&gt; use case. &lt;a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html"&gt;AWS Glue&amp;nbsp;job bookmark&lt;/a&gt; can track previously processed data.&lt;/p&gt; 
&lt;p&gt;Consider using &lt;a href="https://aws.amazon.com/blogs/big-data/introducing-aws-glue-flex-jobs-cost-savings-on-etl-workloads/"&gt;Glue Flex Jobs&lt;/a&gt; for non-urgent or non-time sensitive data integration workloads such as pre-production jobs, testing, and one-time data loads. With Flex, AWS Glue jobs run on spare compute capacity instead of dedicated hardware.&lt;/p&gt; 
&lt;p&gt;Joins between several&amp;nbsp;dataframes&amp;nbsp;is a common operation in Spark jobs. To reduce shuffling of data between nodes, use&amp;nbsp;&lt;a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html"&gt;broadcast joins&lt;/a&gt;&amp;nbsp;when&amp;nbsp;one of the merged&amp;nbsp;dataframes&amp;nbsp;is small enough to be duplicated on all the executing nodes.&lt;/p&gt; 
&lt;p&gt;The latest &lt;a href="https://docs.aws.amazon.com/glue/latest/dg/release-notes.html"&gt;AWS Glue version&lt;/a&gt; provides more new and efficient features for your workload.&lt;/p&gt; 
&lt;h2&gt;5. Purpose-built analytics&lt;/h2&gt; 
&lt;h3&gt;&lt;em&gt;Data Processing modes&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Real-time data processing options need continuous computing resources and require more energy consumption. For the most favorable sustainability impact, evaluate trade-offs and choose the optimal batch data processing option.&lt;/p&gt; 
&lt;p&gt;Identify the batch and interactive workload requirements and design &lt;a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html"&gt;transient clusters&lt;/a&gt; in &lt;a href="https://aws.amazon.com/emr/"&gt;Amazon EMR&lt;/a&gt;. Using Spot Instances and configuring &lt;a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html"&gt;instance fleets&lt;/a&gt; can maximize utilization.&lt;/p&gt; 
&lt;p&gt;To improve energy efficiency, &lt;a href="https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/emr-serverless.html"&gt;Amazon EMR Serverless&lt;/a&gt; can help you avoid over- or under-provisioning resources for your data processing jobs. Amazon EMR Serverless automatically determines the resources that the application needs, gathers these resources to process your jobs, and releases the resources when the jobs finish.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#rs-ra3-node-types"&gt;Amazon Redshift RA3 nodes&lt;/a&gt; can improve compute efficiency. With RA3 nodes, you can scale compute up and down without having to scale storage. You can choose &lt;a href="https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-whatis.html"&gt;Amazon Redshift Serverless&lt;/a&gt; to intelligently scale data warehouse capacity. This will deliver faster performance for the most demanding and unpredictable workloads.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Energy efficient transformation and data model design&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Data processing and data modeling best practices can reduce your organization’s environmental impact.&lt;/p&gt; 
&lt;p&gt;To avoid unnecessary data movement between nodes in an Amazon Redshift cluster, follow &lt;a href="https://docs.aws.amazon.com/redshift/latest/dg/c_designing-tables-best-practices.html"&gt;best practices for table design&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also use &lt;a href="https://docs.aws.amazon.com/redshift/latest/dg/t_Creating_tables.html"&gt;automatic table optimization (ATO)&lt;/a&gt; for Amazon Redshift to self-tune tables based on usage patterns.&lt;/p&gt; 
&lt;p&gt;Use the EXPLAIN feature in&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/athena/latest/ug/athena-explain-statement.html"&gt;Amazon Athena&lt;/a&gt;&amp;nbsp;or&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/redshift/latest/dg/c_data_redistribution.html"&gt;Amazon Redshift&lt;/a&gt;&amp;nbsp;to tune and optimize the queries.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://docs.aws.amazon.com/redshift/latest/dg/advisor.html"&gt;Amazon Redshift Advisor&lt;/a&gt; provides specific, tailored recommendations to optimize the data warehouse based on performance statistics and operations data.&lt;/p&gt; 
&lt;p&gt;Consider migrating Amazon EMR or &lt;a href="https://aws.amazon.com/opensearch-service/"&gt;Amazon OpenSearch Service&lt;/a&gt; to a more power-efficient processor such as&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/aws/graviton-fast-start-a-new-program-to-help-move-your-workloads-to-aws-graviton/"&gt;AWS Graviton&lt;/a&gt;. &lt;a href="https://aws.amazon.com/ec2/graviton/"&gt;AWS Graviton 3&lt;/a&gt; delivers 2.5–3 times better performance over other CPUs. Graviton 3-based instances use up to 60% less energy for the same performance than comparable EC2 instances.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Minimize idle resources&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Use auto scaling features in&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-automatic-scaling.html"&gt;EMR Clusters&lt;/a&gt; or employ&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/aws/amazon-kinesis-data-streams-on-demand-stream-data-at-scale-without-managing-capacity/"&gt;Amazon Kinesis Data Streams On-Demand&lt;/a&gt;&amp;nbsp;to minimize idle resources without sacrificing performance.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html#underutilized-amazon-redshift-clusters"&gt;AWS Trusted Advisor&lt;/a&gt; can help you identify underutilized &lt;a href="https://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html#rs-mgmt-pause-resume-cluster"&gt;Amazon Redshift Clusters&lt;/a&gt;. Pause Amazon Redshift clusters when not in use and resume when needed.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Energy efficient consumption patterns&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Consider querying the data in place with Amazon Athena or &lt;a href="https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html"&gt;Amazon Redshift Spectrum&lt;/a&gt; for one-off analysis, rather than copying the data to Amazon Redshift.&lt;/p&gt; 
&lt;p&gt;Enable a&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds/"&gt;caching layer for frequent queries&lt;/a&gt; as needed. This is in addition to the&amp;nbsp;result caching&amp;nbsp;that comes built-in with services such as Amazon Redshift. Also, use &lt;a href="https://aws.amazon.com/blogs/big-data/reduce-cost-and-improve-query-performance-with-amazon-athena-query-result-reuse/"&gt;Amazon Athena Query Result Reuse&lt;/a&gt; for every query where the source data doesn’t change frequently.&lt;/p&gt; 
&lt;p&gt;Use&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html"&gt;materialized views&lt;/a&gt;&amp;nbsp;capabilities available in Amazon Redshift or &lt;a href="https://aws.amazon.com/rds/aurora/"&gt;Amazon Aurora&lt;/a&gt; Postgres to avoid unnecessary computation.&lt;/p&gt; 
&lt;p&gt;Use federated queries across data stores powered by&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html"&gt;Amazon Athena federated query&lt;/a&gt;&amp;nbsp;or&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/redshift/latest/dg/federated-overview.html"&gt;Amazon Redshift federated query&lt;/a&gt;&amp;nbsp;to reduce data movement. For querying across separate Amazon Redshift clusters, consider using &lt;a href="https://aws.amazon.com/redshift/features/data-sharing/"&gt;Amazon Redshift data sharing&lt;/a&gt; feature that decreases data movement between these clusters.&lt;/p&gt; 
&lt;h2&gt;Track and assess improvement for environmental sustainability&lt;/h2&gt; 
&lt;p&gt;The optimal way to evaluate success in optimizing your workloads for sustainability is to use &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/evaluate-specific-improvements.html"&gt;proxy measures and unit of work KPI&lt;/a&gt;. This can be GB per transaction for storage, or vCPU minutes per transaction for compute.&lt;/p&gt; 
&lt;p&gt;In Table 1, we list certain metrics you could collect on analytics services as proxies to measure improvement. These fall under each pillar of the modern data architecture covered in this post.&lt;/p&gt; 
&lt;table style="text-align: top-left;border: 1px;padding: 5px"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th style="background-color: #6699cc;color: black;text-align: left;vertical-align: top;padding: 5px"&gt;Pillar&lt;/th&gt; 
   &lt;th style="background-color: #6699cc;color: black;text-align: left;vertical-align: top;padding: 5px"&gt;Metrics&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="background-color: #6699cc;color: black;text-align: left;vertical-align: top;padding: 5px"&gt;&lt;strong&gt;Unified data governance&lt;br&gt; &lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="background-color: #bdd5ed;text-align: left;vertical-align: top;padding: 5px"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-cloudtrail.html"&gt;CloudTrail events&lt;/a&gt; – for monitoring crawler runs and job runs&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="background-color: #6699cc;color: black;text-align: left;vertical-align: top;padding: 5px"&gt;&lt;strong&gt;Data movement&lt;br&gt; &lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="background-color: #deeaf6;text-align: left;vertical-align: top;padding: 5px"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-debug-capacity.html"&gt;DPU-hour&lt;/a&gt; for AWS Glue jobs&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="background-color: #6699cc;color: black;text-align: left;vertical-align: top;padding: 5px"&gt;&lt;strong&gt;Purpose-built Analytics &lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="background-color: #bdd5ed;text-align: left;vertical-align: top;padding: 5px"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/redshift/latest/mgmt/performance-metrics-perf.html"&gt;Redshift cluster performance data&lt;/a&gt; – CPUUtilization, percentage disk space used, read throughput, write throughput, query duration, query throughput&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/redshift/latest/mgmt/performance-metrics-query-history.html"&gt;Redshift query history&lt;/a&gt; (optimize queries) – query runtime, CPUUtilization, storage capacity used&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-metrics.html"&gt;Amazon Redshift Spectrum queries&lt;/a&gt; – System views: SVL_S3QUERY, SVL_S3QUERY_SUMMARY&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html"&gt;CloudWatch metrics for Amazon EMR&lt;/a&gt; – IsIdle, HDFSUtilization, S3BytesRead, S3BytesWritten&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/opensearch-service/latest/developerguide/managedomains-cloudwatchmetrics.html"&gt;CloudWatch metrics for Amazon OpenSearch&lt;/a&gt; (Cluster metrics) – CPUUtilization, FreeStorageSpace, ClusterUsedSpace, JVMMemoryPressure, DiskThroughputThrottle&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/athena/latest/ug/query-metrics-viewing.html"&gt;CloudWatch metrics for Amazon Athena&lt;/a&gt; – ProcessedBytes, QueryQueueTime, TotalExecutionTime&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-jobs"&gt;CloudWatch metrics for Amazon SageMaker&lt;/a&gt; – CPUUtilization, GPUUtilization, GPUMemoryUtilization, MemoryUtilization, and DiskUtilization&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/kinesisanalytics/latest/java/metrics-dimensions.html#applicationmetrics"&gt;Kinesis Data Analytics application metrics&lt;/a&gt; – CPUUtilization, containerCPUUtilization, containerDiskUtilization, idleTimeMsPerSecond&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Table 1. Metrics for the Modern data architecture pillars&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post, we provided best practices to optimize processes under the unified data governance, data movement, and purpose-built analytics pillars of modern architecture.&lt;/p&gt; 
&lt;p&gt;If you want to learn more, check out the&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html"&gt;Sustainability Pillar of the AWS Well-Architected Framework&lt;/a&gt; and other blog posts on&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/architecture/tag/sustainability/"&gt;architecting for sustainability&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are looking for more architecture content, refer to the &lt;a href="https://aws.amazon.com/architecture/"&gt;AWS Architecture Center&lt;/a&gt;&amp;nbsp;for reference architecture diagrams, vetted architecture solutions,&amp;nbsp;&lt;a href="https://aws.amazon.com/architecture/well-architected/"&gt;Well-Architected&lt;/a&gt;&amp;nbsp;best practices, patterns, icons, and more.&lt;/p&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>How to select a Region for your workload based on sustainability goals</title>
		<link>https://aws.amazon.com/blogs/architecture/how-to-select-a-region-for-your-workload-based-on-sustainability-goals/</link>
					
		
		<dc:creator><![CDATA[Sam Mokhtari]]></dc:creator>
		<pubDate>Wed, 23 Nov 2022 16:22:49 +0000</pubDate>
				<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Well-Architected]]></category>
		<category><![CDATA[Regions]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">08f2757b451018ff072170ba86967e26a4e7dfe8</guid>

					<description>The Amazon Web Services (AWS) Cloud is a constantly expanding network of Regions and points of presence (PoP), with a global network infrastructure linking them together. The choice of Regions for your workload significantly affects your workload KPIs, including performance, cost, and carbon footprint. The Well-Architected Framework’s sustainability pillar offers design principles and best practices […]</description>
										<content:encoded>&lt;p&gt;The &lt;a href="https://aws.amazon.com/"&gt;Amazon Web Services (AWS)&lt;/a&gt; Cloud is a constantly expanding network of Regions and points of presence (PoP), with a global network infrastructure linking them together. The choice of Regions for your workload significantly affects your workload KPIs, including performance, cost, and carbon footprint.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html"&gt;Well-Architected Framework’s sustainability pillar&lt;/a&gt; offers design principles and best practices that you can use to meet sustainability goals for your AWS workloads. It recommends choosing Regions for your workload based on both your business requirements and sustainability goals. In this blog, we explain how to select an appropriate AWS Region for your workload. This process includes two key steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Assess and shortlist potential Regions for your workload based on your business requirements.&lt;/li&gt; 
 &lt;li&gt;Choose Regions near Amazon renewable energy projects and Region(s) where the grid has a lower published carbon intensity.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To demonstrate this two-step process, let’s assume we have a web application that must be deployed in the AWS Cloud to support end users in the UK and Sweden. Also, let’s assume there is no local regulation that binds the data residency to a specific location. Let’s select a Region for this workload based on guidance in the sustainability pillar of AWS Well-Architected Framework.&lt;/p&gt; 
&lt;h2&gt;Shortlist potential Regions for your workload&lt;/h2&gt; 
&lt;p&gt;Let’s follow the best practice on &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/region-selection.html"&gt;Region selection&lt;/a&gt; in the sustainability pillar of AWS Well-Architected Framework. The first step is to assess and shortlist potential Regions for your workload based on your business requirements.&lt;/p&gt; 
&lt;p&gt;In &lt;a href="https://aws.amazon.com/blogs/architecture/what-to-consider-when-selecting-a-region-for-your-workloads/"&gt;What to Consider when Selecting a Region for your Workloads&lt;/a&gt;, there are four key business factors to consider when evaluating and shortlisting each AWS Region for a workload:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Latency&lt;/li&gt; 
 &lt;li&gt;Cost&lt;/li&gt; 
 &lt;li&gt;Services and features&lt;/li&gt; 
 &lt;li&gt;Compliance&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To shortlist your potential Regions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Confirm that these Regions are compliant, based on your local regulations.&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/"&gt;AWS Regional Services Lists&lt;/a&gt; to check if the Regions have the services and features you need to run your workload.&lt;/li&gt; 
 &lt;li&gt;Calculate the cost of the workload on each Region using the &lt;a href="https://calculator.aws/"&gt;AWS Pricing Calculator&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Test the network latency between your end user locations and each AWS Region.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;At this point, you should have a list of AWS Regions. For this sample workload, let’s assume only &lt;strong&gt;Europe (London)&lt;/strong&gt; and &lt;strong&gt;Europe (Stockholm)&lt;/strong&gt; Regions are shortlisted. They can address the requirements for latency, cost, and features for our use case.&lt;/p&gt; 
&lt;h2&gt;Choose Regions for your workload&lt;/h2&gt; 
&lt;p&gt;After shortlisting the potential Regions, the next step is to choose Regions for your workload. Choose Regions near Amazon renewable energy projects or Regions where the grid has a lower published carbon intensity. To understand this step, you need to first understand the &lt;a href="https://ghgprotocol.org/"&gt;Greenhouse Gas (GHG) Protocol&lt;/a&gt; to track emissions.&lt;/p&gt; 
&lt;p&gt;Based on the GHG Protocol, there are two methods to track emissions from electricity production: &lt;em&gt;market-based&lt;/em&gt; and &lt;em&gt;location-based&lt;/em&gt;. Companies may choose one of these methods based on their relevant sustainability guidelines to track and compare their year-to-year emissions.&amp;nbsp;Amazon uses the &lt;a href="https://sustainability.aboutamazon.com/carbon-methodology.pdf"&gt;market-based model to report our emissions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;AWS Region(s) selection based on market-based method&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;With the market-based method, emissions are calculated based on the electricity that businesses have chosen to purchase. For example, the business could decide to contract and purchase electricity produced by renewable energy sources like solar and wind.&lt;/p&gt; 
&lt;p&gt;Amazon’s goal is to power our operations with 100% renewable energy by 2025 – five years ahead of our original 2030 target. We contract for renewable power from utility-scale wind and solar projects that add clean energy to the grid. These new renewable projects support hundreds of jobs and hundreds of millions of dollars investment in local communities. Find more details about &lt;a href="https://sustainability.aboutamazon.com/around-the-globe?energyType=true&amp;amp;workerCount=true&amp;amp;engagementProgram=true&amp;amp;productCategory=true"&gt;our work around the globe&lt;/a&gt;. We support these grids through the purchase of environmental attributes, like Renewable Energy Certificates (RECs) and Guarantees of Origin (GoO), in line with our &lt;a href="https://sustainability.aboutamazon.com/renewable-energy-methodology.pdf"&gt;renewable energy methodology&lt;/a&gt;. As a result, we have a number of Regions listed that are powered by more than 95% renewable energy on the &lt;a href="https://sustainability.aboutamazon.com/environment/the-cloud?energyType=true#renewable-energy-map"&gt;Amazon sustainability website&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Choose one of these Regions to help you power your workload with more renewable energy and reduce your carbon footprint. For the sample workload we’re using as our example, both the &lt;strong&gt;Europe (London)&lt;/strong&gt; and E&lt;strong&gt;urope (Stockholm)&lt;/strong&gt; Regions are in this list. They are powered by over 95% renewable energy based on the market-based emission method.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;AWS Regions selection based on location-based method&amp;nbsp;&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;The location-based method considers the average emissions intensity of the energy grids where consumption takes place. As a result, wherever the organization conducts business, it assesses emissions from the local electricity system. You can use the emissions intensity of the energy grids through a trusted data source to assess Regions for your workload.&lt;/p&gt; 
&lt;p&gt;Let’s look how we can use Electricity Maps data to select a Region for our sample workload:&lt;/p&gt; 
&lt;p style="padding-left: 40px"&gt;1. Go to &lt;a href="https://app.electricitymaps.com/map"&gt;Electricity Maps&lt;/a&gt; (see Figure 1)&lt;/p&gt; 
&lt;p style="padding-left: 40px"&gt;2. Search for South Central Sweden zone to get carbon intensity of electricity consumed for &lt;strong&gt;Europe (Stockholm)&lt;/strong&gt; Region (display aggregated data on yearly basis)&lt;/p&gt; 
&lt;div id="attachment_12472" style="width: 1249px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12472" loading="lazy" class="size-full wp-image-12472" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/16/Fig1a-sweden.png" alt="Carbon intensity of electricity for South Central Sweden" width="1239" height="641"&gt;
 &lt;p id="caption-attachment-12472" class="wp-caption-text"&gt;Figure 1. Carbon intensity of electricity for South Central Sweden&lt;/p&gt;
&lt;/div&gt; 
&lt;p style="padding-left: 40px"&gt;3. Search for Great Britain to get carbon intensity of electricity consumed for &lt;strong&gt;Europe (London)&lt;/strong&gt; Region (display aggregated data on yearly basis)&lt;/p&gt; 
&lt;div id="attachment_12473" style="width: 1311px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12473" loading="lazy" class="size-full wp-image-12473" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/16/Fig2a-carb-intensity.png" alt="Carbon intensity of electricity for Great Britain" width="1301" height="671"&gt;
 &lt;p id="caption-attachment-12473" class="wp-caption-text"&gt;Figure 2. Carbon intensity of electricity for Great Britain&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As you can determine from Figure 2, the &lt;strong&gt;Europe (Stockholm)&lt;/strong&gt; Region has a lower carbon intensity of electricity consumed compared to the &lt;strong&gt;Europe (London)&lt;/strong&gt; Region.&lt;/p&gt; 
&lt;p&gt;For our sample workload, we have selected the &lt;strong&gt;Europe (Stockholm)&lt;/strong&gt; Region due to latency, cost, features, and compliance. It also provides 95% renewable energy using the market-based method, and low grid carbon intensity with the location-based method.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog, we explained the process for selecting an appropriate AWS Region for your workload based on both business requirements and sustainability goals.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To learn more, check out the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html"&gt;Sustainability Pillar of the AWS Well-Architected Framework&lt;/a&gt; and other blog posts on &lt;a href="https://aws.amazon.com/blogs/architecture/tag/sustainability/"&gt;architecting for sustainability&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For more architecture content, refer to &lt;a href="https://aws.amazon.com/architecture/"&gt;AWS Architecture Center&lt;/a&gt; for reference architecture diagrams, vetted architecture solutions, &lt;a href="https://aws.amazon.com/architecture/well-architected/"&gt;Well-Architected&lt;/a&gt; best practices, patterns, icons, and more.&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Let’s Architect! Architecting with Amazon DynamoDB</title>
		<link>https://aws.amazon.com/blogs/architecture/lets-architect-architecting-with-amazon-dynamodb/</link>
					
		
		<dc:creator><![CDATA[Luca Mezzalira]]></dc:creator>
		<pubDate>Tue, 22 Nov 2022 14:26:48 +0000</pubDate>
				<category><![CDATA[Amazon DynamoDB]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[databases]]></category>
		<category><![CDATA[DynamoDB]]></category>
		<category><![CDATA[Let's Architect]]></category>
		<category><![CDATA[multi-region]]></category>
		<category><![CDATA[NoSQL]]></category>
		<category><![CDATA[Resilience]]></category>
		<guid isPermaLink="false">360ceea0073124c71648a25a4dca855680d15d4f</guid>

					<description>NoSQL databases are an essential part of the technology industry in today’s world. Why are we talking about NoSQL databases? NoSQL databases often allow developers to be in control of the structure of the data, and they are a good fit for big data scenarios and offer fast performance. In this issue of Let’s Architect!, […]</description>
										<content:encoded>&lt;p&gt;NoSQL databases are an essential part of the technology industry in today’s world. Why are we talking about NoSQL databases? NoSQL databases often allow developers to be in control of the structure of the data, and they are a good fit for big data scenarios and offer fast performance.&lt;/p&gt; 
&lt;p&gt;In this issue of &lt;em&gt;Let’s Architect!&lt;/em&gt;, we explore &lt;a href="https://docs.aws.amazon.com/dynamodb/?id=docs_gateway"&gt;Amazon DynamoDB&lt;/a&gt; capabilities and potential solutions to apply in your architectures. A key strength of DynamoDB is the capability of operating at scale globally; for instance, multiple products built by Amazon are powered by DynamoDB. During &lt;a href="https://aws.amazon.com/blogs/aws/amazon-prime-day-2022-aws-for-the-win/"&gt;Prime Day 2022&lt;/a&gt;, the service also maintained high availability while delivering single-digit millisecond responses, peaking at 105.2 million requests-per-second. Let’s start!&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://www.youtube.com/watch?v=yNOVamgIXGQ"&gt;Data modeling with DynamoDB&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Working with a new database technology means understanding exactly how it works and the best design practices for taking full advantage of its features.&lt;/p&gt; 
&lt;p&gt;In this video, the key principles for modeling DynamoDB tables are discussed, plus practical patterns to use while defining your data models are explored and how data modeling for NoSQL databases (like DynamoDB) is different from modeling for traditional relational databases.&lt;/p&gt; 
&lt;p&gt;With this video, you can learn about the main components of DynamoDB, some design considerations that led to its creation, and all the best practices for efficiently using primary keys, secondary keys, and indexes. Peruse the original paper to learn more about DyanamoDB in &lt;a href="https://assets.amazon.science/ac/1d/eb50c4064c538c8ac440ce6a1d91/dynamo-amazons-highly-available-key-value-store.pdf"&gt;&lt;em&gt;Dynamo: Amazon’s Highly Available Key-value Store&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_12185" style="width: 861px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/18/Amazon-DynamoDB-uses-partitioning-to-provide-horizontal-scalability.png"&gt;&lt;img aria-describedby="caption-attachment-12185" loading="lazy" class="wp-image-12185 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/18/Amazon-DynamoDB-uses-partitioning-to-provide-horizontal-scalability.png" alt="Amazon DynamoDB uses partitioning to provide horizontal scalability" width="851" height="461"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12185" class="wp-caption-text"&gt;Amazon DynamoDB uses partitioning to provide horizontal scalability&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;&lt;a href="https://aws.amazon.com/blogs/database/single-table-vs-multi-table-design-in-amazon-dynamodb/"&gt;Single-table vs. multi-table in Amazon DynamoDB&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;When considering single-table versus multi-table in DynamoDB, it is all about your application’s needs. It is possible to avoid naïve lifting-and-shifting your relational data model into DynamoDB tables. In this post, you will discover different use cases on when to use single-table compared with multi-table designs, plus understand certain data-modeling principles for DynamoDB.&lt;/p&gt; 
&lt;div id="attachment_12186" style="width: 1090px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12186" loading="lazy" class="size-full wp-image-12186" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/18/Use-a-single-table-design-to-provide-materialized-joins-in-Amazon-DynamoDB.png" alt="Use a single-table design to provide materialized joins in Amazon DynamoDB" width="1080" height="940"&gt;
 &lt;p id="caption-attachment-12186" class="wp-caption-text"&gt;Use a single-table design to provide materialized joins in Amazon DynamoDB&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;&lt;a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-cost-optimization.html"&gt;Optimizing costs on DynamoDB tables&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Infrastructure cost is an important dimension for every customer. Despite your role inside an organization, you should monitor opportunities for optimizing costs, when possible.&lt;br&gt; For this reason, we have created a guide on DynamoDB tables cost-optimization that provides several suggestions for reducing your bill at the end of the month.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://aws.amazon.com/blogs/database/part-1-build-resilient-applications-with-amazon-dynamodb-global-tables/"&gt;Build resilient applications with Amazon DynamoDB global tables: Part 1&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;When you operate global systems that are spread across multiple AWS regions, dealing with data replication and writes across regions can be a challenge. DynamoDB global tables help by providing the performance of DynamoDB across multiple regions with data synchronization and multi-active database where each replica can be used for both writing and reading data.&lt;/p&gt; 
&lt;p&gt;Another use case for global tables are resilient applications with the lowest possible recovery time objective (RTO) and recovery point objective (RPO). In this blog series, we show you how to approach such a scenario.&lt;/p&gt; 
&lt;div id="attachment_12491" style="width: 1650px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/17/Amazon-DynamoDB-active-active-architecture.png"&gt;&lt;img aria-describedby="caption-attachment-12491" loading="lazy" class="size-full wp-image-12491" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/17/Amazon-DynamoDB-active-active-architecture.png" alt="Amazon DynamoDB active-active architecture" width="1640" height="1242"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12491" class="wp-caption-text"&gt;Amazon DynamoDB active-active architecture&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;See you next time!&lt;/h2&gt; 
&lt;p&gt;Thanks for joining our discussion on DynamoDB. See you in a few weeks, when we explore cost optimization!&lt;/p&gt; 
&lt;h2&gt;Other posts in this series&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-architecting-in-health-tech/"&gt;Let’s Architect! Architecting in health tech&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-custom-chips-and-accelerators/"&gt;Let’s Architect! Architecting with custom chips and accelerators&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-modern-data-architectures/"&gt;Let’s Architect! Modern data architectures&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-architecting-for-the-edge/"&gt;Let’s Architect! Architecting for the edge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-1-architecture-and-sustainability/"&gt;Let’s Architect! Architecting for Sustainability&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/architecting-for-machine-learning/"&gt;Let’s Architect! Architecting for Machine Learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-architecting-for-security/"&gt;Let’s Architect! Architecting for Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-tools-for-cloud-architects/"&gt;Let’s Architect! Tools for Cloud Architects&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-architecting-for-blockchain/"&gt;Let’s Architect! Architecting for Blockchain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-architecting-microservices-with-containers/"&gt;Let’s Architect! Architecting microservices with containers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-using-open-source-technologies-on-aws/"&gt;Let’s Architect! Using open-source technologies on AWS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-serverless-architecture-on-aws/"&gt;Let’s Architect! Serverless architecture on AWS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-creating-resilient-architecture/"&gt;Let’s Architect! Creating resilient architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-architecting-for-governance-and-management/"&gt;Let’s Architect! Architecting for governance and management&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-architecting-for-front-end/"&gt;Let’s Architect! Architecting for front end&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-understanding-the-build-versus-buy-dilemma/"&gt;Let’s Architect! Understanding the build versus buy dilemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-architecting-for-devops/"&gt;Let’s Architect! Architecting for DevOps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-designing-well-architected-systems/"&gt;Let’s Architect! Designing Well-Architected systems&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/lets-architect-architecting-for-big-data-workloads/"&gt;Let’s Architect! Architecting for big data workloads&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Looking for more architecture content?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/architecture/"&gt;AWS Architecture Center&lt;/a&gt;&amp;nbsp;provides reference architecture diagrams, vetted architecture solutions, Well-Architected best practices, patterns, icons, and more!&lt;/p&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Building event-driven architectures with IoT sensor data</title>
		<link>https://aws.amazon.com/blogs/architecture/building-event-driven-architectures-with-iot-sensor-data/</link>
					
		
		<dc:creator><![CDATA[Raghavarao Sodabathina]]></dc:creator>
		<pubDate>Mon, 21 Nov 2022 15:14:44 +0000</pubDate>
				<category><![CDATA[Amazon Kinesis]]></category>
		<category><![CDATA[Amazon SageMaker]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS IoT Core]]></category>
		<category><![CDATA[AWS IoT Greengrass]]></category>
		<category><![CDATA[AWS Lambda]]></category>
		<guid isPermaLink="false">5345ad730977fcbd5ccb5673d506c2f23f4a687f</guid>

					<description>The Internet of Things (IoT) brings sensors, cloud computing, analytics, and people together to improve productivity and efficiency. It empowers customers with the intelligence they need to build new services and business models, improve products and services over time, understand their customers’ needs to provide better services, and improve customer experiences. Business operations become more […]</description>
										<content:encoded>&lt;p&gt;The Internet of Things (IoT) brings sensors, cloud computing, analytics, and people together to improve productivity and efficiency. It empowers customers with the intelligence they need to build new services and business models, improve products and services over time, understand their customers’ needs to provide better services, and improve customer experiences. Business operations become more efficient by making intelligent decisions more quickly and over time develop a data-driven discipline leading to revenue growth and greater operational efficiency.&lt;/p&gt; 
&lt;p&gt;In this post, we showcase how to build an event-driven architecture by using &lt;a href="https://aws.amazon.com/iot/"&gt;AWS IoT services&lt;/a&gt; and &lt;a href="https://aws.amazon.com/big-data/datalakes-and-analytics/?nc=sn&amp;amp;loc=0"&gt;AWS purpose-built data services&lt;/a&gt;. We also discuss key considerations and best practices while building event-driven application architectures with IoT sensor data.&lt;/p&gt; 
&lt;h2&gt;Deriving insights from IoT sensor data&lt;/h2&gt; 
&lt;p&gt;Organizations create value by making decisions from their IoT sensor data in near real time. Some common use cases and solutions that fit under event-driven architecture using IoT sensor data include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Medical device data collection for personalized patient health monitoring, adverse event prediction, and avoidance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://aws.amazon.com/blogs/iot/industrial-iot-from-condition-based-monitoring-to-predictive-quality-to-digitize-your-factory-with-aws-iot-services/"&gt;Industrial IoT use cases&lt;/a&gt; to monitor equipment quality and determine actions like adjusting machine settings, using different sources of raw materials, or performing additional worker training to improve the quality of the factory output.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://aws.amazon.com/solutions/implementations/aws-connected-vehicle-solution/"&gt;Connected vehicle use cases&lt;/a&gt;, such as voice interaction, navigation, location-based services, remote vehicle diagnostics, predictive maintenance, media streaming, and vehicle safety, that are based on in-vehicle computing and near real-time predictive analytics in the cloud.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://aws.amazon.com/solutions/travel/sustainability-and-waste-reduction/?solutions-all.sort-by=item.additionalFields.sortDate&amp;amp;solutions-all.sort-order=desc&amp;amp;marketplace-ppa-and-quickstart.sort-by=item.additionalFields.sortDate&amp;amp;marketplace-ppa-and-quickstart.sort-order=desc&amp;amp;whitepapers-main.sort-by=item.additionalFields.sortDate&amp;amp;whitepapers-main.sort-order=desc"&gt;Sustainability and waste reduction solutions&lt;/a&gt;, which provide access to dashboards, monitoring systems, data collection, and summarization tools that use machine learning (ML) algorithms to meet sustainability goals. Meeting sustainability goals is paramount for customers in the travel and hospitality industries.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Event-driven reference architecture with IoT sensor data&lt;/h2&gt; 
&lt;p&gt;Figure 1 illustrates how to architect an event-driven architecture with IoT sensor data for near real-time predictive analytics and recommendations.&lt;/p&gt; 
&lt;div id="attachment_12481" style="width: 1922px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/16/Figure-1.-Building-event-driven-architecture-with-IoT-sensor-data.png"&gt;&lt;img aria-describedby="caption-attachment-12481" loading="lazy" class="size-full wp-image-12481" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/16/Figure-1.-Building-event-driven-architecture-with-IoT-sensor-data.png" alt="Building event-driven architecture with IoT sensor data" width="1912" height="975"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12481" class="wp-caption-text"&gt;Figure 1. Building event-driven architecture with IoT sensor data&lt;/p&gt;
&lt;/div&gt; 
&lt;h4&gt;Architecture flow:&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Data originates in IoT devices such as medical devices, car sensors, industrial IoT sensors.This telemetry data is collected using &lt;a href="https://docs.aws.amazon.com/greengrass/v1/developerguide/what-is-gg.html"&gt;AWS IoT Greengrass&lt;/a&gt;, an open-source IoT edge runtime and cloud service that helps your devices collect and analyze data closer to where the data is generated.When an event arrives, AWS IoT Greengrass reacts autonomously to local events, filters and aggregates device data, then communicates securely with the cloud and other local devices in your network to send the data.&lt;/li&gt; 
 &lt;li&gt;Event data is ingested into the cloud using edge-to-cloud interface services such as &lt;a href="http://aws.amazon.com/iot-core/?nc=sn&amp;amp;loc=2&amp;amp;dn=3"&gt;AWS IoT Core&lt;/a&gt;, a managed cloud platform that connects, manages, and scales devices easily and securely.AWS IoT Core interacts with cloud applications and other devices. You can also use &lt;a href="https://docs.aws.amazon.com/iot-sitewise/latest/userguide/what-is-sitewise.html"&gt;AWS IoT SiteWise&lt;/a&gt;, a managed service that helps you collect, model, analyze, and visualize data from industrial equipment at scale.&lt;/li&gt; 
 &lt;li&gt;AWS IoT Core can directly stream ingested data into &lt;a href="https://aws.amazon.com/kinesis/data-streams/"&gt;Amazon Kinesis Data Streams&lt;/a&gt;. The ingested data gets transformed and analyzed in near real time using &lt;a href="http://aws.amazon.com/kinesis/data-analytics/?nc=sn&amp;amp;loc=1"&gt;Amazon Kinesis Data Analytics&lt;/a&gt; with Apache Flink and Apache Beam frameworks.Stream data can further be enriched using lookup data hosted in a data warehouse such as Amazon Redshift. Amazon Kinesis Data Analytics can persist SQL results to Amazon Redshift after the customer’s integration and stream aggregation (for example, one minute or five minutes).The results in Amazon Redshift can be used for further downstream business intelligence (BI) reporting services, such as &lt;a href="https://docs.aws.amazon.com/quicksight/latest/user/welcome.html"&gt;Amazon QuickSight&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Amazon Kinesis Data Analytics can also write to an AWS Lambda function, which can invoke &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html"&gt;Amazon SageMaker models&lt;/a&gt;. Amazon SageMaker is a the most complete, end-to-end service for machine learning.&lt;/li&gt; 
 &lt;li&gt;Once the ML model is trained and deployed in SageMaker, inferences are invoked in a micro batch using &lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/welcome.html"&gt;AWS Lambda&lt;/a&gt;. Inferenced data is sent to &lt;a href="http://aws.amazon.com/opensearch-service/the-elk-stack/what-is-opensearch/"&gt;Amazon OpenSearch Service&lt;/a&gt; to create personalized monitoring dashboards using &lt;a href="https://docs.aws.amazon.com/opensearch-service/latest/developerguide/dashboards.html"&gt;Amazon OpenSearch Service dashboards&lt;/a&gt;.The transformed IoT sensor data can be stored in &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt;. Customers can use &lt;a href="https://aws.amazon.com/appsync/"&gt;AWS AppSync&lt;/a&gt; to provide near real-time data queries to API services for downstream applications. These enterprise applications can be mobile apps or business applications to track and monitor the IoT sensor data in near real-time.Amazon Kinesis Data Analytics can write to an &lt;a href="https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"&gt;Amazon Kinesis Data Firehose&lt;/a&gt; stream, which is a fully managed service for delivering near real-time streaming data to destinations like Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoints or endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic. &lt;p&gt;In this example, data from Amazon Kinesis Data Analytics is written to Amazon Kinesis Data Firehose, which micro-batch streams data into an Amazon S3 data lake. The Amazon S3 data lake stores telemetry data for future batch analytics.&lt;/p&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Key considerations and best practices&lt;/h2&gt; 
&lt;p&gt;Keep the following best practices in mind:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Define the business value from IoT sensor data through interactive discovery sessions with various stakeholders within your organization.&lt;/li&gt; 
 &lt;li&gt;Identify the type of IoT sensor data you want to collect and analyze for predictive analytics.&lt;/li&gt; 
 &lt;li&gt;Choose the right tools for the job, depending upon your business use case and your data consumers. Please refer to step 5 earlier in this post, where different purpose-built data services were used based on user personas.&lt;/li&gt; 
 &lt;li&gt;Consider the event-driven architecture as three key components: event producers, event routers, and event consumers. A producer publishes an event to the router, which filters and pushes the events to consumers. Producer and consumer services are decoupled, which allows them to be scaled, updated, and deployed independently.&lt;/li&gt; 
 &lt;li&gt;In this architecture, IoT sensors are event producers. Amazon IoT Greengrass, Amazon IoT Core, Amazon Kinesis Data Streams, and Amazon Kinesis Data Analytics work together as the router from which multiple consumers can consume IoT sensor-generated data. These consumers include Amazon S3 data lakes for telemetry data analysis, Amazon OpenSearch Service for personalized dashboards, and Amazon DynamoDB or AWS AppSync for the downstream enterprise application’s consumption.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we demonstrated how to build an event-driven architecture with IoT sensor data using AWS IoT services and AWS purpose-built data services. You can now build your own event-driven applications using this post with your IoT sensor data and integrate with your business applications as needed.&lt;/p&gt; 
&lt;h2&gt;Further reading&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/iot/"&gt;AWS IoT&lt;/a&gt; services&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/big-data/datalakes-and-analytics/?nc=sn&amp;amp;loc=0"&gt;Analytics on AWS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/sagemaker/"&gt;Amazon SageMaker&lt;/a&gt; and other &lt;a href="https://aws.amazon.com/machine-learning/ai-services/"&gt;artificial intelligence services&lt;/a&gt; for data prediction&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/iot/7-patterns-for-iot-data-ingestion-and-visualization-how-to-decide-what-works-best-for-your-use-case/"&gt;Seven patterns for IoT data ingestion and visualization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/iot/solutions/?iot-solution-repository-cards.sort-by=item.additionalFields.headline&amp;amp;iot-solution-repository-cards.sort-order=asc&amp;amp;awsf.iot-solution-repository-filter-industry=*all&amp;amp;awsf.iot-solution-repository-filter-products=*all&amp;amp;awsf.iot-solution-repository-filter-usecase=*all&amp;amp;awsf.iot-solution-repository-filter-solution-type=*all"&gt;AWS IoT solutions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/?awsf.blog-master-category=category%23internet-of-things&amp;amp;awsf.blog-master-learning-levels=*all&amp;amp;awsf.blog-master-industry=*all&amp;amp;awsf.blog-master-analytics-products=*all&amp;amp;awsf.blog-master-artificial-intelligence=*all&amp;amp;awsf.blog-master-aws-cloud-financial-management=*all&amp;amp;awsf.blog-master-blockchain=*all&amp;amp;awsf.blog-master-business-applications=*all&amp;amp;awsf.blog-master-compute=*all&amp;amp;awsf.blog-master-customer-enablement=*all&amp;amp;awsf.blog-master-customer-engagement=*all&amp;amp;awsf.blog-master-database=*all&amp;amp;awsf.blog-master-developer-tools=*all&amp;amp;awsf.blog-master-devops=*all&amp;amp;awsf.blog-master-end-user-computing=*all&amp;amp;awsf.blog-master-mobile=*all&amp;amp;awsf.blog-master-iot=*all&amp;amp;awsf.blog-master-management-governance=*all&amp;amp;awsf.blog-master-media-services=*all&amp;amp;awsf.blog-master-migration-transfer=*all&amp;amp;awsf.blog-master-migration-solutions=*all&amp;amp;awsf.blog-master-networking-content-delivery=*all&amp;amp;awsf.blog-master-programming-language=*all&amp;amp;awsf.blog-master-sector=*all&amp;amp;awsf.blog-master-security=*all&amp;amp;awsf.blog-master-storage=*all"&gt;AWS IoT blog posts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Author Spotlight: Luca Mezzalira, Principal Serverless Specialist Solutions Architect</title>
		<link>https://aws.amazon.com/blogs/architecture/author-spotlight-luca-mezzalira-principal-serverless-specialist-solutions-architect/</link>
					
		
		<dc:creator><![CDATA[Elise Chahine]]></dc:creator>
		<pubDate>Fri, 18 Nov 2022 14:37:33 +0000</pubDate>
				<category><![CDATA[Architecture]]></category>
		<category><![CDATA[Thought Leadership]]></category>
		<guid isPermaLink="false">1759c4db7e32e274bdf38d076fa41334c4d312ac</guid>

					<description>The Author Spotlight series pulls back the curtain on some of AWS’s most prolific authors. Read on to find out more about our very own Luca Mezzalira’s journey, in his own words! My name is Luca, and I’m a Principal Serverless Specialist Solutions Architect—probably the longest job title I’ve ever had in my 20-year career […]</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;The Author Spotlight series pulls back the curtain on some of AWS’s most prolific authors. Read on to find out more about our very own Luca Mezzalira’s journey, in his own words!&lt;/em&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;My name is Luca, and I’m a Principal Serverless Specialist Solutions Architect—probably the longest job title I’ve ever had in my 20-year career in the tech industry. One thing you have to know about me upfront: I love challenges. I tread an unconventional path, on which I found several hurdles, but, after a few years, I grew to love them.&lt;/p&gt; 
&lt;p&gt;Since I joined Amazon Web Services (AWS) in January 2021, I discovered (and continue to discover) all the challenges I’ve always dreamed of. I can also find solutions for customers, industries, and communities—what better place is there for a challenge-hunter like me!&lt;/p&gt; 
&lt;p&gt;I am self-taught. I learned my foundational skills from the developer communities I joined out of a thirst for knowledge. Fast-forward 20 years later, I still try to pay my “debt” to them by sharing what I learn and do on a regular basis.&lt;/p&gt; 
&lt;div id="attachment_12446" style="width: 1440px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Luca-Mezzalira-during-the-opening-talk-at-JS-Poland-2022.png"&gt;&lt;img aria-describedby="caption-attachment-12446" loading="lazy" class="wp-image-12446 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Luca-Mezzalira-during-the-opening-talk-at-JS-Poland-2022.png" alt="Luca Mezzalira during the opening talk at JS Poland 2022" width="1430" height="954"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12446" class="wp-caption-text"&gt;Luca Mezzalira during the opening talk at JS Poland 2022&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;AWS gave me the opportunity to first help our Media &amp;amp; Entertainment industry customers in the UK and Ireland and, now, to follow my passion working as a Serverless Specialist.&lt;/p&gt; 
&lt;p&gt;“Passionate” is another word that characterizes me, both personally and professionally: I’m Italian and there is a lot of passion under our skin. I don’t consider what I do a job but, rather, something I just love to do.&lt;/p&gt; 
&lt;p&gt;During these past couple of years with AWS, I have been able to use all 360° of my knowledge. With customers experimenting with new ideas and solutions, with colleagues urging customers outside their comfort zone and onto new horizons or into new adventures with AWS, I am blurring the edges of different worlds. With each passing day, I provide new perspectives for solving existing challenges! With internal and external communities, I support and organize events for spreading our ever-growing knowledge and creating new, meaningful connections.&lt;/p&gt; 
&lt;p&gt;Another great passion of mine is software architecture. Design patterns, distributed systems, team topology, domain-driven design, and any topic related to software architecture is what I deeply love. Do you know why? Because there isn’t right or wrong in architecture—it’s just trade-offs! The challenge is to find the least-worse decision for making a project successful.&lt;/p&gt; 
&lt;p&gt;Moreover, architectures are like living organisms. They evolve, requiring care and attention. Many might think that architecting is only a technical concern, but it is deeply connected with the organizational structure, as well the communication and engineering practices. When we acknowledge these aspects and work across these dimensions, the role of an architect is one of the best you can have—or at least it is for me!&lt;/p&gt; 
&lt;h2&gt;What’s on my mind&lt;/h2&gt; 
&lt;p&gt;There are two main topics I am focusing on at the moment: (1) distributed architecture on the frontend (i.e., micro-frontends); and (2) educating our builders on thinking in patterns, choosing the right solution to implement at the right moment.&lt;/p&gt; 
&lt;p&gt;In both cases, I create a lot of content trying to bridge the gap between the technical implementation and the architecture characteristics a company wants to optimize for.&lt;/p&gt; 
&lt;h2&gt;My favorite blog posts&lt;/h2&gt; 
&lt;h4&gt;&lt;a href="https://aws.amazon.com/blogs/compute/developing-evolutionary-architecture-with-aws-lambda/"&gt;Developing evolutionary architecture with AWS Lambda&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;The first contribution I wanted to provide in AWS was without any doubt architectural. Hexagonal architecture (or ports and adapters) is not a new topic by any stretch, however, I wasn’t able to find solid resources with a simplified explanation of this approach. Once in place, hexagonal architectures can help the portability of your business logic across different AWS services or even on a hybrid-cloud. Using this architecture on Lambda functions has generated a lot of interest inside the serverless community.&lt;/p&gt; 
&lt;p&gt;If you want to know more, I leave you to the &lt;a href="https://www.youtube.com/watch?v=kRFg6fkVChQ&amp;amp;t=1s"&gt;re:Invent talk I delivered in 2021&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="AWS re:Invent 2021 - Evolutionary AWS Lambda functions with hexagonal architecture [REPEAT]" width="500" height="281" src="https://www.youtube-nocookie.com/embed/kRFg6fkVChQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;h4&gt;&lt;a href="https://aws.amazon.com/blogs/architecture/tag/lets-architect/"&gt;Let’s Architect!&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;The second resource I am extremely proud of is a collaboration with AWS’s Zamira Jaupaj, Laura Hyatt, and Vittorio Denti… the &lt;em&gt;Let’s Architect!&lt;/em&gt; team.&lt;/p&gt; 
&lt;p&gt;I met them in my first year in AWS, and they share a similar passion for helping people and community engagement. Moreover, we all want to learn something new.&lt;br&gt; Together, we created &lt;em&gt;Let’s Architect!&lt;/em&gt;, a blog series that publishes a fortnightly post on a specific topic since January 2022. For example, serverless, containers, or data architectures are explored, gathering four different AWS content pieces that provide an architect’s perspective on why that content is relevant (or still relevant).&lt;/p&gt; 
&lt;p&gt;This initiative has had a strong influence, and we now have customers and even many of our colleagues awaiting our upcoming posts. If you want to discover more, check out the &lt;a href="https://aws.amazon.com/blogs/architecture/tag/lets-architect/"&gt;AWS Architecture Blog&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_12447" style="width: 1438px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12447" loading="lazy" class="size-full wp-image-12447" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Lets-Architect.png" alt="Let's Architect" width="1428" height="918"&gt;
 &lt;p id="caption-attachment-12447" class="wp-caption-text"&gt;&lt;em&gt;Let’s Architect!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt; 
&lt;h4&gt;&lt;a href="https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/server-side-rendering-micro-frontends-ra.pdf?did=wp_card&amp;amp;trk=wp_card"&gt;Server-Side Rendering Micro-Frontends in AWS&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;The last resource is part of my dream to lead the frontend community in their discovery of AWS services.&lt;/p&gt; 
&lt;p&gt;&lt;img loading="lazy" class="alignnone size-full wp-image-12448" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Server-side-rednering-micro-frontends-in-AWS.png" alt="" width="1431" height="801"&gt;&lt;/p&gt; 
&lt;p&gt;The frontend community is exposed to a lot of new frameworks and libraries, however, I believe they should look to the cloud as well, as they can unlock a variety of new possibilities.&lt;/p&gt; 
&lt;p&gt;Considering my expertise on micro-frontends and serverless, I started with a reference architecture to build distributed frontend using serverless. I recently started a new series on the &lt;a href="https://aws.amazon.com/blogs/compute/"&gt;AWS Compute Blog&lt;/a&gt; explaining the reasoning behind this reference architecture and how to approach server-side rendering micro-frontends using serverless. Read my &lt;a href="https://aws.amazon.com/blogs/compute/server-side-rendering-micro-frontends-the-architecture/"&gt;first post&lt;/a&gt; on server-side rendering micro-frontends.&lt;/p&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Genomics workflows, Part 1: automated launches</title>
		<link>https://aws.amazon.com/blogs/architecture/automated-launch-of-genomics-workflows/</link>
					
		
		<dc:creator><![CDATA[Rostislav Markov]]></dc:creator>
		<pubDate>Wed, 16 Nov 2022 15:13:50 +0000</pubDate>
				<category><![CDATA[Amazon Elastic Container Registry]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS Lambda]]></category>
		<category><![CDATA[AWS Step Functions]]></category>
		<guid isPermaLink="false">cf12f435c845d7de35394c7aecaf93f6bf6eed99</guid>

					<description>Genomics workflows are high-performance computing workloads. Traditionally, they run on-premises with a collection of scripts. Scientists run and manage these workflows manually, which slows down the product development lifecycle. Scientists spend time to administer workflows and handle errors on a day-to-day basis. They also lack sufficient compute capacity on-premises. In Part 1 of this series, […]</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://aws-samples.github.io/aws-genomics-workflows/"&gt;Genomics workflows&lt;/a&gt; are high-performance computing workloads. Traditionally, they run on-premises with a collection of scripts. Scientists run and manage these workflows manually, which slows down the product development lifecycle. Scientists spend time to administer workflows and handle errors on a day-to-day basis. They also lack sufficient compute capacity on-premises.&lt;/p&gt; 
&lt;p&gt;In Part 1 of this series, we demonstrate how life sciences companies can use Amazon Web Services (AWS) to remove the traditional heavy lifting associated with genomic studies. We use &lt;a href="https://docs.aws.amazon.com/step-functions/?id=docs_gateway"&gt;AWS Step Functions&lt;/a&gt; to orchestrate workflow steps, including error handling. With &lt;a href="https://docs.aws.amazon.com/batch/?id=docs_gateway"&gt;AWS Batch&lt;/a&gt;, we horizontally scale-out the analytic tasks for optimal performance. This allows genome scientists to focus on scientific discovery while AWS runs their workflows.&lt;/p&gt; 
&lt;h2&gt;Use case&lt;/h2&gt; 
&lt;p&gt;Workflow systems used for genomic analysis include &lt;a href="https://cromwell.readthedocs.io/en/stable/"&gt;Cromwell&lt;/a&gt;, &lt;a href="https://www.nextflow.io/"&gt;Nextflow&lt;/a&gt;, and &lt;a href="https://rgcgithub.github.io/regenie/"&gt;regenie&lt;/a&gt;. These high-performance computing systems share the following requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fast access to datasets at petabyte scale&lt;/li&gt; 
 &lt;li&gt;Parallel task distribution, with horizontal compute scale-out&lt;/li&gt; 
 &lt;li&gt;Data processing in batches following a specific sequence of data analysis steps, which vary by use case&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We explore the use case of regenie. regenie is a common, open-source utility for whole-genome regression modelling of large &lt;a href="https://www.genome.gov/about-genomics/fact-sheets/Genome-Wide-Association-Studies-Fact-Sheet"&gt;genome-wide association studies&lt;/a&gt; (GWAS). GWAS compare &lt;a href="https://www.genome.gov/genetics-glossary/Deoxyribonucleic-Acid"&gt;DNA&lt;/a&gt; datasets of individuals with a specific trait or disease. The intent is to associate the identified trait/disease with DNA variants. Among other positive results, this helps identify at-risk patients, plus testing and prevention opportunities.&lt;/p&gt; 
&lt;p&gt;regenie is a C++ program that runs in &lt;a href="https://rgcgithub.github.io/regenie/options/"&gt;two steps&lt;/a&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The first step searches for variants associated with a specific trait in a dataset of individuals with the trait, in order to create a whole-genome regression model that captures the variance.&lt;/li&gt; 
 &lt;li&gt;The second step validates for association with the identified variants against a larger dataset, typically in the scale of petabytes, and launches a sequence of tasks run on data batches.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Solution overview&lt;/h2&gt; 
&lt;p&gt;The entire regenie workflow and associated tasks of attaching and deleting file-share access to sample data, as well as spinning up compute instances for parallel computing, can be orchestrated with Step Functions. We use &lt;a href="https://docs.aws.amazon.com/fsx/?id=docs_gateway"&gt;Amazon FSx for Lustre&lt;/a&gt; as a high-performance, transient file system providing file access to the datasets stored in an &lt;a href="https://docs.aws.amazon.com/s3/?id=docs_gateway"&gt;Amazon Simple Storage Service (Amazon S3)&lt;/a&gt; bucket. AWS Batch allows us to programmatically spin up multiple &lt;a href="https://docs.aws.amazon.com/ec2/?id=docs_gateway"&gt;Amazon Elastic Compute Cloud (Amazon EC2)&lt;/a&gt; instances on which regenie can distribute parallel computing tasks. We do this with an &lt;a href="https://docs.aws.amazon.com/lambda/?id=docs_gateway"&gt;AWS Lambda&lt;/a&gt; function that calculates the number of required batch jobs based on the requested size of samples per batch.&lt;/p&gt; 
&lt;p&gt;regenie is available as Docker image on &lt;a href="https://github.com/rgcgithub/regenie"&gt;GitHub&lt;/a&gt;. We push the image to &lt;a href="http://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry&lt;/a&gt; from which AWS Batch can pull it with the creation of new jobs at launch time. The Step Functions state machine is initiated by a Lambda function, with interactive user input. In the past, scientists have also directly interacted with the &lt;a href="https://docs.aws.amazon.com/step-functions/latest/apireference/Welcome.html"&gt;Step Functions API&lt;/a&gt; via the &lt;a href="https://docs.aws.amazon.com/awsconsolehelpdocs/latest/gsg/learn-whats-new.html"&gt;AWS Management Console&lt;/a&gt; or by running &lt;a href="https://docs.aws.amazon.com/cli/latest/reference/stepfunctions/start-execution.html"&gt;start-execution&lt;/a&gt; in the &lt;a href="https://aws.amazon.com/cli/"&gt;AWS Command Line Interface&lt;/a&gt; and passing a &lt;a href="https://aws.amazon.com/documentdb/what-is-json/"&gt;JSON&lt;/a&gt; file with the input parameters.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/cloudwatch/?id=docs_gateway"&gt;Amazon CloudWatch&lt;/a&gt; provides a consolidated overview of performance metrics, including elapsed time, failed jobs, and error types. You can keep logs of your failed jobs in &lt;a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html"&gt;Amazon CloudWatch Logs&lt;/a&gt; (Figure 1). You can set up &lt;a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html"&gt;filters&lt;/a&gt; to match specific error types, plus create &lt;a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html"&gt;subscriptions&lt;/a&gt; to deliver a real-time stream of your log events to &lt;a href="https://docs.aws.amazon.com/kinesis/?id=docs_gateway"&gt;Amazon Kinesis&lt;/a&gt; or AWS Lambda for further retry.&lt;/p&gt; 
&lt;div id="attachment_12435" style="width: 1094px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-1.-Solution-overview-for-automating-regenie-workflows-on-AWS.png"&gt;&lt;img aria-describedby="caption-attachment-12435" loading="lazy" class="wp-image-12435 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-1.-Solution-overview-for-automating-regenie-workflows-on-AWS.png" alt="Solution overview for automating regenie workflows on AWS" width="1084" height="712"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12435" class="wp-caption-text"&gt;Figure 1. Solution overview for automating regenie workflows on AWS&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Alternatively, the Step Functions workflow triggers another Lambda function, which puts failed job logs to &lt;a href="https://docs.aws.amazon.com/dynamodb/?id=docs_gateway"&gt;Amazon DynamoDB&lt;/a&gt;. In the past, we have used this to ease data access and manipulation via the AWS management console. Scientists updated table items and &lt;a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"&gt;DynamoDB Streams&lt;/a&gt; initiated the retry.&lt;/p&gt; 
&lt;h2&gt;Workflow automation&lt;/h2&gt; 
&lt;p&gt;With each invocation, Step Functions initiates a new instance of the state machine. AWS documentation provides an overview of the API &lt;a href="https://docs.aws.amazon.com/step-functions/latest/dg/limits-overview.html#service-limits-api-state-throttling"&gt;quotas&lt;/a&gt;. Step Functions allows the modeling of the entire workflow, including custom application &lt;a href="https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html"&gt;error handling&lt;/a&gt;. &lt;a href="https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-map-state.html"&gt;Map&lt;/a&gt; state improves performance by parallelizing workflow branches.&lt;/p&gt; 
&lt;p&gt;The state machine initiates the build of the file system and, once it’s ready, creates a data repository association with the sample data stored on Amazon S3. It waits until the &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html"&gt;data repository association&lt;/a&gt; is complete and proceeds with the calculation of batch jobs, based on a user-defined number of samples to be processed per batch job (Figure 2). This is essential to determine the amount of compute instances required for data processing.&lt;/p&gt; 
&lt;div id="attachment_12436" style="width: 1442px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-2.-AWS-Step-Functions-workflow-for-regenie-initialize-file-access.png"&gt;&lt;img aria-describedby="caption-attachment-12436" loading="lazy" class="wp-image-12436 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-2.-AWS-Step-Functions-workflow-for-regenie-initialize-file-access.png" alt="AWS Step Functions workflow for regenie: initialize file access" width="1432" height="1150"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12436" class="wp-caption-text"&gt;Figure 2. AWS Step Functions workflow for regenie: initialize file access&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Next, the state machine builds the commands to launch the regenie steps, as requested by the user, and submit the jobs for AWS Batch (Figure 3). The workflow checks if a specific version of regenie was requested by the user, otherwise, it defaults to the version of regenie on the container.&lt;/p&gt; 
&lt;p&gt;Then, we build the commands to initiate the two regenie steps. Step 2 may need to run in multiple iterations on different datasets (more often than Step 1). This is also determined with user input at initiation of the workflow. With Step Functions, we create runner logic to build the set of commands dynamically. This pattern is applicable to other scientific workloads, as well.&lt;/p&gt; 
&lt;div id="attachment_12437" style="width: 585px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-3.-AWS-Step-Functions-workflow-for-regenie-prepare-and-submit-jobs.png"&gt;&lt;img aria-describedby="caption-attachment-12437" loading="lazy" class="wp-image-12437 " src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-3.-AWS-Step-Functions-workflow-for-regenie-prepare-and-submit-jobs.png" alt="AWS Step Functions workflow for regenie: prepare and submit jobs" width="575" height="613" data-wp-editing="1"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12437" class="wp-caption-text"&gt;Figure 3. AWS Step Functions workflow for regenie: prepare and submit jobs&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Once jobs are submitted, the workflow proceeds (by default) with the initiation of Step 1 of regenie; if requested by the user, the workflow will proceed directly to step 2 (Figure 4).&lt;/p&gt; 
&lt;p&gt;Any errors during batch launch leading to the failure of a job are passed, in this case, to a Lambda function. We configure the Lambda function to write the failed job logs to Amazon DynamoDB or as S3 objects.&lt;/p&gt; 
&lt;div id="attachment_12438" style="width: 554px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-4.-AWS-Step-Functions-workflow-for-regenie-launch-jobs.png"&gt;&lt;img aria-describedby="caption-attachment-12438" loading="lazy" class="wp-image-12438 " src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-4.-AWS-Step-Functions-workflow-for-regenie-launch-jobs.png" alt="AWS Step Functions workflow for regenie: launch jobs" width="544" height="689"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12438" class="wp-caption-text"&gt;Figure 4. AWS Step Functions workflow for regenie: launch jobs&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Finally, the Step Functions workflow checks for pending errors and confirms that all jobs have finished their initiation. Then, it deletes the file system and data repository association and ends the workflow instance (Figure 5).&lt;/p&gt; 
&lt;div id="attachment_12439" style="width: 586px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-5.-AWS-Step-Functions-workflow-for-regenie-complete-error-handling-and-delete-file-system.png"&gt;&lt;img aria-describedby="caption-attachment-12439" loading="lazy" class="wp-image-12439 " src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/14/Figure-5.-AWS-Step-Functions-workflow-for-regenie-complete-error-handling-and-delete-file-system.png" alt="AWS Step Functions workflow for regenie: complete error handling and delete file system" width="576" height="474"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12439" class="wp-caption-text"&gt;Figure 5. AWS Step Functions workflow for regenie: complete error handling and delete file system&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As demonstrated, we can automate the entire process, from data access to verifying job completion and cleaning-up transient resources. This removes manual error handling and retry, plus reduces the overall cost of running regenie workflows. We also showed in Figure 3 that you can build commands dynamically for different scientific workloads.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post, we addressed a common pain point in the daily work of life sciences research teams. Traditionally, they had to run genomics workflows manually on limited compute capacity. Moving those workflows to AWS eliminates the heavy lifting of running scripts manually and expedites computational cycles. This allows research teams to stay focused on scientific discovery.&lt;/p&gt; 
&lt;p&gt;We recommend a thorough performance testing when setting up your genomics workflows. This includes determining the most suitable EC2 instance size. Some workflows, such as regenie, are single-threaded and benefit from horizontal scale-out of the number of instances but not from vertical scale-out of instance sizes.&lt;/p&gt; 
&lt;h2&gt;Related information&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws-samples.github.io/aws-genomics-workflows/"&gt;Genomics workflows on AWS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/health/genomics/"&gt;Genomics on AWS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/genomics-cli/"&gt;Amazon Genomics CLI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Architecting near real-time personalized recommendations with Amazon Personalize</title>
		<link>https://aws.amazon.com/blogs/architecture/architecting-near-real-time-personalized-recommendations-with-amazon-personalize/</link>
					
		
		<dc:creator><![CDATA[Raghavarao Sodabathina]]></dc:creator>
		<pubDate>Mon, 14 Nov 2022 18:53:48 +0000</pubDate>
				<category><![CDATA[Amazon Personalize]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Customer Solutions]]></category>
		<category><![CDATA[Marketing & Advertising]]></category>
		<guid isPermaLink="false">fdd16d9ba9cb15c0f4946102114c49df7bdecc17</guid>

					<description>Delivering personalized customer experiences enables organizations to improve business outcomes such as acquiring and retaining customers, increasing engagement, driving efficiencies, and improving discoverability. Developing an in-house personalization solution can take a lot of time, which increases the time it takes for your business to launch new features and user experiences. In this post, we show […]</description>
										<content:encoded>&lt;p&gt;Delivering personalized customer experiences enables organizations to improve business outcomes such as acquiring and retaining customers, increasing engagement, driving efficiencies, and improving discoverability. Developing an in-house personalization solution can take a lot of time, which increases the time it takes for your business to launch new features and user experiences.&lt;/p&gt; 
&lt;p&gt;In this post, we show you how to architect near real-time personalized recommendations using &lt;a href="https://aws.amazon.com/personalize/"&gt;Amazon Personalize&lt;/a&gt; and &lt;a href="https://aws.amazon.com/big-data/datalakes-and-analytics/?nc=sn&amp;amp;loc=0"&gt;AWS purpose-built data services&lt;/a&gt;. &amp;nbsp;We also discuss key considerations and best practices while building near real-time personalized recommendations.&lt;/p&gt; 
&lt;h2&gt;Building personalized recommendations with Amazon Personalize&lt;/h2&gt; 
&lt;p&gt;Amazon Personalize makes it easy for developers to build applications capable of delivering a wide array of personalization experiences, including specific product recommendations, personalized product re-ranking, and customized direct marketing.&lt;/p&gt; 
&lt;p&gt;Amazon Personalize provisions the necessary infrastructure and manages the entire machine learning (ML) pipeline, including processing the data, identifying features, using the most appropriate algorithms, and training, optimizing, and hosting the models. You receive results through an Application Programming Interface (API) and pay only for what you use, with no minimum fees or upfront commitments.&lt;/p&gt; 
&lt;p&gt;Figure 1 illustrates the comparison of Amazon Personalize with the ML lifecycle.&lt;/p&gt; 
&lt;div id="attachment_12390" style="width: 993px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/09/personalize-1.png"&gt;&lt;img aria-describedby="caption-attachment-12390" loading="lazy" class="wp-image-12390 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/09/personalize-1.png" alt="Machine learning lifecycle vs. Amazon Personalize" width="983" height="343"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12390" class="wp-caption-text"&gt;Figure 1. Machine learning lifecycle vs. Amazon Personalize&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;First, provide the user and items data to Amazon Personalize. In general, there are three steps for building near real-time recommendations with Amazon Personalize:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Data preparation&lt;/strong&gt;: Preparing data is one of the prerequisites for building accurate ML models and analytics, and it is the most time-consuming part of an ML project. There are three types of data you use for modeling on Amazon Personalize: 
  &lt;ul&gt; 
   &lt;li&gt;An &lt;strong&gt;Interactions&lt;/strong&gt; data set captures the activity of your users, also known as events. Examples include items your users click on, purchase, or watch. The events you choose to send are dependent on your business domain. This data set has the strongest signal for personalization, and is the only mandatory data set.&lt;/li&gt; 
   &lt;li&gt;An &lt;strong&gt;Items&lt;/strong&gt; data set includes details about your items, such as price point, category information, and other essential information from your catalog. This data set is optional, but very useful for scenarios such as recommending new items.&lt;/li&gt; 
   &lt;li&gt;A &lt;strong&gt;Users&lt;/strong&gt; data set includes details about the users, such as their location, age, and other details.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train the model with Amazon Personalize&lt;/strong&gt;: Amazon Personalize provides recipes, based on common use cases for training models. A recipe is an Amazon Personalize algorithm prepared for a given use case. Refer to Amazon Personalize recipes for more details. The four types of recipes are: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;&lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/user-personalization-recipes.html"&gt;USER_PERSONALIZATION&lt;/a&gt;&lt;/code&gt;: Recommends items for a user from a catalog. This is often included on a landing page.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;&lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/related-items-recipes.html"&gt;RELATED_ITEM&lt;/a&gt;&lt;/code&gt;: Suggests items similar to a selected item on a detail page.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;&lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/personalized-ranking-recipes.html"&gt;PERSONALZIED_RANKING&lt;/a&gt;&lt;/code&gt;: Re-ranks a list of items for a user within a category or in within search results.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;&lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/user-segmentation-recipes.html"&gt;USER_SEGMENTATION&lt;/a&gt;&lt;/code&gt;: Generates segments of users based on item input data. You can use this to create a targeted marketing campaign for particular products by brand.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Get near real-time recommendations: Once your model is trained, a private personalization model is hosted for you. You can then provide recommendations for your users through a private API.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Figure 2 illustrates a high-level overview of Amazon Personalize:&lt;/p&gt; 
&lt;div id="attachment_12391" style="width: 993px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/09/personalize-2.png"&gt;&lt;img aria-describedby="caption-attachment-12391" loading="lazy" class="size-full wp-image-12391" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/09/personalize-2.png" alt="Figure 2. Building recommendations with Amazon Personalize " width="983" height="393"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12391" class="wp-caption-text"&gt;Figure 2. Building recommendations with Amazon Personalize&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Near real-time personalized recommendations reference architecture&lt;/h2&gt; 
&lt;p&gt;Figure 3 illustrates how to architect near real-time personalized recommendations using Amazon Personalize and AWS purpose-built data services.&lt;/p&gt; 
&lt;div id="attachment_12393" style="width: 993px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/09/personalize-3.png"&gt;&lt;img aria-describedby="caption-attachment-12393" loading="lazy" class="wp-image-12393 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/09/personalize-3.png" alt="Reference architecture for near real-time recommendations" width="983" height="641"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12393" class="wp-caption-text"&gt;Figure 3. Near real-time recommendations reference architecture&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Architecture flow&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Data preparation&lt;/strong&gt;: Start by &lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/create-domain-dataset-group.html"&gt;creating a dataset group&lt;/a&gt;, schemas, and &lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/custom-datasets-and-schemas.html"&gt;datasets&lt;/a&gt; representing your items, interactions, and user data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train the model&lt;/strong&gt;: After importing your data, select the recipe matching your use case, and then &lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/training-deploying-solutions.html"&gt;create a solution&lt;/a&gt; to train a model by &lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/creating-a-solution-version.html"&gt;creating a solution version.&lt;/a&gt;&lt;br&gt; Once your solution version is ready, you can create a campaign for your solution version. You can create a campaign for every solution version that you want to use for near real-time recommendations.&lt;br&gt; In this example architecture, we’re just showing a single solution version and campaign. If you were building out multiple personalization use cases with different recipes, you could create multiple solution versions and campaigns from the same datasets.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Get near real-time recommendations&lt;/strong&gt;: Once you have a campaign, you can integrate calls to the campaign in your application. This is where calls to the &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/API_RS_GetRecommendations.html"&gt;GetRecommendations&lt;/a&gt;&lt;/code&gt; or &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/API_RS_GetPersonalizedRanking.html"&gt;GetPersonalizedRanking&lt;/a&gt;&lt;/code&gt; APIs are made to request near real-time recommendations from Amazon Personalize. 
  &lt;ul&gt; 
   &lt;li&gt;The approach you take to integrate recommendations into your application varies based on your architecture but it typically involves encapsulating recommendations in a microservice or &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; function that is called by your website or mobile application through a RESTful or GraphQL API interface.&lt;/li&gt; 
   &lt;li&gt;Near real-time recommendations support the ability to adapt to each user’s evolving interests. This is done by creating an &lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/API_EventTracker.html"&gt;event tracker&lt;/a&gt; in Amazon Personalize.&lt;/li&gt; 
   &lt;li&gt;An event tracker provides an endpoint that allows you to stream interactions that occur in your application back to Amazon Personalize in near real-time. You do this by using the &lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/API_UBS_PutEvents.html"&gt;PutEvents API&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Again, the architectural details on how you integrate PutEvents into your application varies, but it typically involves collecting events using a JavaScript library in your website or a native library in your mobile apps, and making API calls to stream them to your backend. AWS provides the &lt;a href="https://aws.amazon.com/amplify/"&gt;AWS Amplify framework&lt;/a&gt; that can be integrated into your web and mobile apps to handle this for you.&lt;/li&gt; 
   &lt;li&gt;In this example architecture, you can build an event collection pipeline using &amp;nbsp;&lt;a href="https://aws.amazon.com/api-gateway/"&gt;Amazon API Gateway&lt;/a&gt;, Amazon Kinesis Data Streams, and Lambda to receive and forward interactions to Amazon Personalize.&lt;/li&gt; 
   &lt;li&gt;The Event Tracker performs two primary functions. First, it persists all streamed interactions so they will be incorporated into future retraining of your model. This also how Amazon Personalize cold starts new users. When a new user visits your site, Amazon Personalize will recommend popular items. After you stream in an event or two, Amazon Personalize immediately starts adjusting recommendations.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Key considerations and best practices&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;For all use cases, your interactions data must have a minimum 1000 interaction records from users interacting with items in your catalog. These interactions can be from bulk imports, streamed events, or both, and a minimum 25 unique user IDs with at least two interactions for each.&lt;/li&gt; 
 &lt;li&gt;Metadata fields (user or item) can be used for training, filters, or both.&lt;/li&gt; 
 &lt;li&gt;Amazon Personalize supports the encryption of your imported data. You can specify a role allowing Amazon Personalize to use an &lt;a href="https://aws.amazon.com/kms/"&gt;AWS Key Management Service&lt;/a&gt; (AWS KMS) key to decrypt your data, or use the &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) AES-256 server-side default encryption.&lt;/li&gt; 
 &lt;li&gt;You can re-train Amazon Personalize deployments based on how much interaction data you generate on a daily basis. A good rule is to re-train your models once every week or two as needed.&lt;/li&gt; 
 &lt;li&gt;You can apply business rules for personalized recommendations using filters. Refer to &lt;a href="https://docs.aws.amazon.com/personalize/latest/dg/filter.html"&gt;Filtering recommendations and user segments&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we showed you how to build near real-time personalized recommendations using Amazon Personalize and AWS purpose-built data services. With the information in this post, you can now build your own personalized recommendations for your applications.&lt;/p&gt; 
&lt;p&gt;Read more and get started on building personalized recommendations on AWS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/personalize/"&gt;Amazon Personalize Developer Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/amazon-personalize-samples"&gt;Amazon Personalize samples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://youtube.com/playlist?list=PLhr1KZpdzukd9GSGRy329wahNO_8TkRo_"&gt;Amazon Personalize deep dive video series&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/retail-demo-store"&gt;Retail demo store&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/?awsf.blog-master-category=*all&amp;amp;awsf.blog-master-learning-levels=*all&amp;amp;awsf.blog-master-industry=*all&amp;amp;awsf.blog-master-analytics-products=*all&amp;amp;awsf.blog-master-artificial-intelligence=*all&amp;amp;awsf.blog-master-aws-cloud-financial-management=*all&amp;amp;awsf.blog-master-blockchain=*all&amp;amp;awsf.blog-master-business-applications=*all&amp;amp;awsf.blog-master-compute=*all&amp;amp;awsf.blog-master-customer-enablement=*all&amp;amp;awsf.blog-master-customer-engagement=*all&amp;amp;awsf.blog-master-database=*all&amp;amp;awsf.blog-master-developer-tools=*all&amp;amp;awsf.blog-master-devops=*all&amp;amp;awsf.blog-master-end-user-computing=*all&amp;amp;awsf.blog-master-mobile=*all&amp;amp;awsf.blog-master-iot=*all&amp;amp;awsf.blog-master-management-governance=*all&amp;amp;awsf.blog-master-media-services=*all&amp;amp;awsf.blog-master-migration-transfer=*all&amp;amp;awsf.blog-master-migration-solutions=*all&amp;amp;awsf.blog-master-networking-content-delivery=*all&amp;amp;awsf.blog-master-programming-language=*all&amp;amp;awsf.blog-master-sector=*all&amp;amp;awsf.blog-master-security=*all&amp;amp;awsf.blog-master-storage=*all&amp;amp;filtered-posts.q=Amazon%2Bpersonalize&amp;amp;filtered-posts.q_operator=AND"&gt;Amazon Personalize blog posts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Why Signeasy chose AWS Serverless to build their SaaS dashboard</title>
		<link>https://aws.amazon.com/blogs/architecture/why-signeasy-chose-aws-serverless-to-build-their-saas-dashboard/</link>
					
		
		<dc:creator><![CDATA[Venkatramana Ameth Achar]]></dc:creator>
		<pubDate>Fri, 11 Nov 2022 18:45:29 +0000</pubDate>
				<category><![CDATA[Architecture]]></category>
		<category><![CDATA[Customer Solutions]]></category>
		<category><![CDATA[Serverless]]></category>
		<guid isPermaLink="false">4a15da8fca780141a2a026fe82db034ecf1c5ede</guid>

					<description>Signeasy is a leading eSignature company that offers an easy-to-use, cross-platform and cloud-based eSignature and document transaction management software as a service (SaaS) solution for businesses. Over 43,000 companies worldwide use Signeasy to digitize and streamline business workflows. In this blog, you will learn why and how Signeasy used AWS Serverless to create a SaaS […]</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://signeasy.com/"&gt;Signeasy&lt;/a&gt; is a leading eSignature company that offers an easy-to-use, cross-platform and cloud-based eSignature and document transaction management software as a service (SaaS) solution for businesses. Over 43,000 companies worldwide use Signeasy to digitize and streamline business workflows. In this blog, you will learn why and how Signeasy used &lt;a href="https://aws.amazon.com/serverless/"&gt;AWS Serverless&lt;/a&gt; to create a SaaS dashboard for their tenants.&lt;/p&gt; 
&lt;p&gt;Signeasy’s SaaS tenants asked for an easier way to get insights into tenant usage data on Signeasy’s eSignature platform. To address that, Signeasy built a self-service usage metrics dashboard for their SaaS tenant using AWS Serverless.&lt;/p&gt; 
&lt;h2&gt;Usage reports&lt;/h2&gt; 
&lt;p&gt;What was it like before the self-service dashboard experience? In the past, tenants requested Signeasy to share their usage metrics through support channels or emails. The Signeasy support team compiled the reports and then emailed the report back to the tenant to service the request. This was a repetitive manual task. It involved querying a database, fetching and collating the results into an Excel table to be emailed to the tenant. The turnaround time on these manual reports was eight hours.&lt;/p&gt; 
&lt;p&gt;The following table illustrates the report format (with example data) that the tenants received through email.&lt;/p&gt; 
&lt;div id="attachment_12366" style="width: 1045px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/usage-reports-1-4.png"&gt;&lt;img aria-describedby="caption-attachment-12366" loading="lazy" class="size-full wp-image-12366" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/usage-reports-1-4.png" alt="Archives usage reports" width="1035" height="232"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12366" class="wp-caption-text"&gt;Figure 1. Archived usage reports&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;The design&lt;/h2&gt; 
&lt;p&gt;Signeasy deliberated numerous aspects and arrived at the following design considerations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enhance tenant experience&lt;/strong&gt; — Provide the reports to tenants on-demand, using a self-service mechanism.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable aggregation queries &lt;/strong&gt;— The reports ran aggregation queries on usage data within a time range on a relational database management system (RDBMS). Signeasy considered moving to a data store that has the scalability to store and run aggregation queries on millions of records.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agility&lt;/strong&gt; — Signeasy wanted to build the module in a time-bound manner and deliver it to tenants as quickly as possible.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reduce infrastructure management&lt;/strong&gt; — The load on the reports infrastructure that stores and processes data increases linearly in relation to the count of usage reports requested. This meant an increase in the undifferentiated heavy lifting of infrastructure management tasks such as capacity management and patching.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With the design considerations and constraints called out, Signeasy began to look for the suitable solution. Signeasy decided to build their usage reports on a serverless architecture. They chose AWS Serverless, because it offers scalable compute and database, application integration capabilities, automatic scaling, and a pay-for-use billing model. This reduces infrastructure management tasks such as capacity provisioning and patching. Refer to the following diagram to see how Signeasy augmented their existing SaaS with self-service usage reports.&lt;/p&gt; 
&lt;h2&gt;Architecture of self-service usage reports&lt;/h2&gt; 
&lt;div id="attachment_12369" style="width: 1027px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/usage-reports-2-2.png"&gt;&lt;img aria-describedby="caption-attachment-12369" loading="lazy" class="wp-image-12369 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/usage-reports-2-2.png" alt="Architecture diagram depicting the data flow of the self-service usage reports" width="1017" height="502"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12369" class="wp-caption-text"&gt;Figure 2. Architecture diagram depicting the data flow of the self-service usage reports&lt;/p&gt;
&lt;/div&gt; 
&lt;ol&gt; 
 &lt;li&gt;Signeasy’s tenant users log in to the Signeasy portal to authenticate their tenant identity.&lt;/li&gt; 
 &lt;li&gt;The Signeasy portal uses a combination of tenant ID and user ID in &lt;a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-jwt-authorizer.html"&gt;JSON Web Tokens&lt;/a&gt; (JWT) to distinguish one tenant user from another when storing and processing documents.&lt;/li&gt; 
 &lt;li&gt;The documents are stored in &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3).&lt;/li&gt; 
 &lt;li&gt;The users’ actions are stored in the transactional database on &lt;a href="https://aws.amazon.com/rds/"&gt;Amazon Relational Database Service&lt;/a&gt; (Amazon RDS).&lt;/li&gt; 
 &lt;li&gt;The user actions are also written as messages into message queue on &lt;a href="https://aws.amazon.com/sqs/"&gt;Amazon Simple Queue Service&lt;/a&gt; (Amazon SQS). Signeasy used the queue to &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/loosely-coupled-scenarios.html"&gt;loosely couple&lt;/a&gt; their existing microservices on &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon Elastic Kubernetes Service&lt;/a&gt; (Amazon EKS) with the new serverless part of the stack.&lt;/li&gt; 
 &lt;li&gt;This allows Signeasy to &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/microservices-on-aws/asynchronous-messaging-and-event-passing.html"&gt;asynchronously process the messages&lt;/a&gt; in Amazon SQS with minimal changes to the existing microservices on EKS.&lt;/li&gt; 
 &lt;li&gt;The messages are processed by a report writer service (Python script) on &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; and written to the reports database on &lt;a href="https://aws.amazon.com/timestream/"&gt;Amazon Timestream&lt;/a&gt;. The reports database on Timestream stores metadata attributes such as user ID and signature document ID, signature document sent, signature request received, document signed, and signature request cancelled or declined, and timestamp of the data point. To view usage reports, the tenant administrators navigate to the Reports section of the Signeasy portal and select Usage Reports.&lt;/li&gt; 
 &lt;li&gt;The usage reports request from the (tenant) Web Client on the browser is an API call to &lt;a href="https://aws.amazon.com/api-gateway/"&gt;Amazon API Gateway&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;API Gateway works as a front door for the backend reports service running on a separate Lambda function.&lt;/li&gt; 
 &lt;li&gt;The reports service on Lambda uses the user ID from login details to query the Amazon Timestream database to generate the report and send it back to the web client through the API Gateway. The report is immediately available for the administrator to view, which is a huge improvement from having to wait for eight hours before this self-service feature was made available to their SaaS tenants.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Following is a mock-up of the Usage Reports dashboard:&lt;/p&gt; 
&lt;div id="attachment_12370" style="width: 985px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/usage-reports-3.png"&gt;&lt;img aria-describedby="caption-attachment-12370" loading="lazy" class="wp-image-12370 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/usage-reports-3.png" alt="A mockup of the Usage Reports page of the Signeasy portal" width="975" height="556"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12370" class="wp-caption-text"&gt;Figure 3. A mock-up of the Usage Reports page of the Signeasy portal&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;So, how did AWS Serverless help Signeasy?&lt;/h2&gt; 
&lt;p&gt;Amazon SQS persists messages up to 14 days, and enables &lt;a href="https://docs.aws.amazon.com/lambda/latest/operatorguide/sqs-retries.html"&gt;retry&lt;/a&gt; functionality for message processed in Lambda. Lambda is an &lt;a href="https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/"&gt;event-driven&lt;/a&gt; serverless compute service that manages deployment and runs code, with logging and monitoring through &lt;a href="https://aws.amazon.com/cloudwatch/"&gt;Amazon CloudWatch&lt;/a&gt;. The integration of &lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html"&gt;API Gateway with Lambda&lt;/a&gt; helped Signeasy easily deploy and manage the backend processing logic for the reports service. As usage of the reports grew, &lt;a href="https://aws.amazon.com/timestream/features/#Serverless_auto-scaling_architecture"&gt;Timestream continued to scale&lt;/a&gt;, without the need to re-architect their application. Signeasy continued to use &lt;a href="https://docs.aws.amazon.com/timestream/latest/developerguide/supported-sql-constructs.html"&gt;SQL to query data&lt;/a&gt; within the reports database on Timestream in a &lt;a href="https://docs.aws.amazon.com/timestream/latest/developerguide/metering-and-pricing.cost-optimization.html"&gt;cost optimized&lt;/a&gt; manner.&lt;/p&gt; 
&lt;p&gt;Signeasy used AWS Serverless for its functionality without the undifferentiated heavy lifting of infrastructure management tasks such as capacity provisioning and patching. Signeasy’s support team is now more focused on higher-level organizational needs such as customer engagements, quarterly business reviews, and signature and payment related issues instead of managing infrastructure.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Going from eight hours to on-demand self-service (0 hours) response time for usage reports is a huge improvement in their SaaS tenant experience.&lt;/li&gt; 
 &lt;li&gt;The AWS Serverless services scale out and in to meet customer needs. Signeasy pays only for what they use, and they don’t run compute infrastructure 24/7 in anticipation of requests throughout the day.&lt;/li&gt; 
 &lt;li&gt;Signeasy’s support and customer success teams have repurposed their time toward higher value customer engagements vs. capacity, or patch management.&lt;/li&gt; 
 &lt;li&gt;Development time for the Usage Reports dashboard was two weeks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Further reading&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://workshops.aws/card/AWS%20Serverless%20SaaS%20Workshop"&gt;Building a multi-tenant Software-as-a-Service (SaaS) solution using AWS Serverless&lt;/a&gt; (workshop)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/saas-lens/saas-lens.html"&gt;SaaS Lens for the AWS Well-Architected Framework&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Accelerating Well-Architected Framework reviews using integrated AWS Trusted Advisor insights</title>
		<link>https://aws.amazon.com/blogs/architecture/accelerating-well-architected-framework-reviews-using-integrated-aws-trusted-advisor-insights/</link>
					
		
		<dc:creator><![CDATA[Stephen Salim]]></dc:creator>
		<pubDate>Wed, 09 Nov 2022 14:11:01 +0000</pubDate>
				<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Trusted Advisor]]></category>
		<category><![CDATA[AWS Well-Architected Framework]]></category>
		<category><![CDATA[AWS Well-Architected Tool]]></category>
		<guid isPermaLink="false">f0b3e0a8fbb1806b436bbfa9d49a0ac8460d59f4</guid>

					<description>In this blog, we will explain how the new AWS Well-Architected integration with AWS Trusted Advisor can give you insights to accelerate your cloud optimization. Customers that have the most success in their cloud adoption recognize that optimizing their cloud architecture and operations is not a one-time effort. Optimization is a continuous improvement virtuous cycle […]</description>
										<content:encoded>&lt;p&gt;In this blog, we will explain how the new &lt;a href="https://aws.amazon.com/about-aws/whats-new/2022/11/aws-well-architected-tool-workload-discovery-speeds-reviews/"&gt;AWS Well-Architected integration with AWS Trusted Advisor&lt;/a&gt; can give you insights to accelerate your cloud optimization. Customers that have the most success in their cloud adoption recognize that optimizing their cloud architecture and operations is not a one-time effort. Optimization is a continuous improvement virtuous cycle based on learning architectural and operational best practices, measuring workloads against these best practices, and implementing improvements based on opportunities recognized from measurement.&lt;/p&gt; 
&lt;p&gt;Customers can use the &lt;a href="https://aws.amazon.com/architecture/well-architected"&gt;AWS Well-Architected Framework&lt;/a&gt; to build a “learn, measure, and improve” continuous improvement virtuous cycle (Figure 1). With the &lt;a href="https://aws.amazon.com/well-architected-tool/"&gt;AWS Well-Architected Tool&lt;/a&gt;, customers can measure their &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/userguide/workloads.html"&gt;workloads&lt;/a&gt; against these AWS best practices to identify improvement opportunities or risks they should address. After customers complete Well-Architected Framework Reviews (WAFRs) they can generate &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/userguide/tutorial-step3.html"&gt;improvement plans&lt;/a&gt; with prioritized guidance and resources for improvement. They can also track the improvements made over time using the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/userguide/milestones.html"&gt;milestones&lt;/a&gt; feature in the Well-Architected Tool.&lt;/p&gt; 
&lt;div id="attachment_12338" style="width: 310px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-1.-Continuous-optimization-of-workloads-based-on-AWS-best-practices.png"&gt;&lt;img aria-describedby="caption-attachment-12338" loading="lazy" class="wp-image-12338 size-medium" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-1.-Continuous-optimization-of-workloads-based-on-AWS-best-practices-300x300.png" alt="Continuous optimization of workloads based on AWS best practices" width="300" height="300"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12338" class="wp-caption-text"&gt;Figure 1. Continuous optimization of workloads based on AWS best practices&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Customers can add momentum to an AWS Well-Architected “learn, measure, and improve” virtuous cycle using tools that give more insights while measuring workloads. Improved insights result in consistent measurements, that are more efficient and more accurate. This accelerates the optimization cycle by reducing the time required to measure workloads. Collecting information on AWS resources using Trusted Advisor checks allows customers to validate if a workload’s state is aligned with AWS best practices. The new AWS Well-Architected Tool &lt;a href="https://aws.amazon.com/about-aws/whats-new/2022/11/aws-well-architected-tool-workload-discovery-speeds-reviews/"&gt;integration with AWS Trusted Advisor&lt;/a&gt; makes it easier and faster to gain insights during WAFRs. The Trusted Advisor checks that are relevant to a specific set of best practices have been mapped to the corresponding questions in Well-Architected. The new feature now shows the mapped Trusted Advisor checks directly in the Well-Architected Tool. These insights help customers run WAFRs in less time, with more accuracy, accelerating the improvement cycle (Figure 2).&lt;/p&gt; 
&lt;div id="attachment_12340" style="width: 310px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-3.-Insights-from-AWS-Trusted-Advisor-create-acceleration-in-achieving-improved-outcomes.png"&gt;&lt;img aria-describedby="caption-attachment-12340" loading="lazy" class="wp-image-12340 size-medium" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-3.-Insights-from-AWS-Trusted-Advisor-create-acceleration-in-achieving-improved-outcomes-300x203.png" alt="Insights from AWS Trusted Advisor create acceleration in achieving improved outcomes" width="300" height="203"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12340" class="wp-caption-text"&gt;Figure 2. Insights from AWS Trusted Advisor create acceleration in achieving improved outcomes&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;AWS Well-Architected Tool integration with AWS Trusted Advisor: feature example&lt;/h2&gt; 
&lt;p&gt;In the following sections, we detail an example scenario on how to use the integration with Trusted Advisor to gain insights when measuring your workloads.&lt;/p&gt; 
&lt;h4&gt;Enabling the AWS Well-Architected Tool integration with AWS Trusted Advisor&lt;/h4&gt; 
&lt;p&gt;How to enable the new feature in your workload:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a new workload in the AWS Well-Architected Console. Refer to the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/userguide/define-workload.html"&gt;user guide&lt;/a&gt; for detailed instructions.&lt;br&gt; &lt;em&gt;&lt;br&gt; Optional&lt;/em&gt;: When defining a workload, within the &lt;strong&gt;“Application&lt;/strong&gt;” section of workload definition, you can now also specify the AWS Service Catalog AppRegistry AWS Resource Name (ARN). This field is to indicate a relationship between the AWS Well-Architected Tool &lt;strong&gt;workload&lt;/strong&gt; and the AWS resources in an AppRegistry Application when performing a Well-Architected Framework Review (Figure 4).&lt;p&gt;&lt;/p&gt; 
  &lt;div id="attachment_12341" style="width: 928px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-4.-Application-field-to-select-AWS-Service-Catalog-AppRegistry-ARN.png"&gt;&lt;img aria-describedby="caption-attachment-12341" loading="lazy" class="wp-image-12341 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-4.-Application-field-to-select-AWS-Service-Catalog-AppRegistry-ARN.png" alt="Application field to select AWS Service Catalog AppRegistry ARN" width="918" height="226"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12341" class="wp-caption-text"&gt;Figure 4. Application field to select AWS Service Catalog AppRegistry ARN&lt;/p&gt;
  &lt;/div&gt; &lt;p&gt;This is another new AWS Well-Architected Tool feature that launched along with the integration with Trusted Advisor feature. You can find out more details about the integration with AWS Service Catalog AppRegistry in the &lt;a href="https://aws.amazon.com/about-aws/whats-new/2022/11/aws-well-architected-tool-workload-discovery-speeds-reviews/"&gt;What’s New post&lt;/a&gt; and on the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/userguide/intro.html"&gt;feature documentation page&lt;/a&gt;. For details on how to create an AWS Service Catalog AppRegistry Application refer to &lt;a href="https://docs.aws.amazon.com/servicecatalog/latest/arguide/create-apps.html"&gt;Creating applications&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; 
 &lt;li&gt;To enable the integration with Trusted Advisor, after the necessary workload information has been entered, within the &lt;strong&gt;“AWS Trusted Advisor”&lt;/strong&gt; section, tick on &lt;strong&gt;“Activate Trusted Advisor”&lt;/strong&gt; (Figure 5). 
  &lt;div id="attachment_12342" style="width: 854px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-5.-Enabling-the-Trusted-Advisor-feature.png"&gt;&lt;img aria-describedby="caption-attachment-12342" loading="lazy" class="wp-image-12342 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-5.-Enabling-the-Trusted-Advisor-feature.png" alt="Enabling the Trusted Advisor feature" width="844" height="306"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12342" class="wp-caption-text"&gt;Figure 5. Enabling the AWS Trusted Advisor feature&lt;/p&gt;
  &lt;/div&gt; &lt;p&gt;&lt;em&gt;Optional&lt;/em&gt;: Once the workload is created, note the workload ARN. You can find the workload ARN in the Properties section of the workload resource you created (Figure 6). For steps on how to identify your workload, refer to &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/userguide/workloads-page.html"&gt;Well-Architected Tool User Guide on viewing a workload&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12343" style="width: 1253px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-6.-AWS-Well-Architected-Tool-showing-workload-ARN.png"&gt;&lt;img aria-describedby="caption-attachment-12343" loading="lazy" class="wp-image-12343 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-6.-AWS-Well-Architected-Tool-showing-workload-ARN.png" alt="AWS Well-Architected Tool showing workload ARN" width="1243" height="819"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12343" class="wp-caption-text"&gt;Figure 6. AWS Well-Architected Tool showing workload ARN&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;To collect Trusted Advisor checks from accounts other than the account where the workload you are reviewing exists, you must perform two steps. You need to ensure the account IDs are listed in the workload properties for the workload you are reviewing. You must then create an IAM role in the account from which Trusted Advisor checks will be collected with the following &lt;strong&gt;permission and trust&lt;/strong&gt; relationship (Figures 7 and 8). For more information on how to setup this permission, refer to the &lt;a href="https://docs.aws.amazon.com/"&gt;feature documentation&lt;/a&gt;. 
  &lt;div id="attachment_12344" style="width: 1089px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-7.-Permissions-needed-by-AWS-Well-Architected-Tool-to-interrogate-AWS-Trusted-Advisor.png"&gt;&lt;img aria-describedby="caption-attachment-12344" loading="lazy" class="wp-image-12344 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-7.-Permissions-needed-by-AWS-Well-Architected-Tool-to-interrogate-AWS-Trusted-Advisor.png" alt="Permissions needed by AWS Well-Architected Tool to interrogate AWS Trusted Advisor" width="1079" height="779"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12344" class="wp-caption-text"&gt;Figure 7. Permissions needed by AWS Well-Architected Tool to interrogate AWS Trusted Advisor&lt;/p&gt;
  &lt;/div&gt; &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12522" style="width: 1136px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/22/Figure-8.-The-trust-relationship-allowing-AWS-Well-Architected-Tool-to-assume-policy-on-behalf-of-the-workload-1.png"&gt;&lt;img aria-describedby="caption-attachment-12522" loading="lazy" class="wp-image-12522 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/22/Figure-8.-The-trust-relationship-allowing-AWS-Well-Architected-Tool-to-assume-policy-on-behalf-of-the-workload-1.png" alt="The trust relationship allowing AWS Well-Architected Tool to assume policy on behalf of the workload" width="1126" height="652"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12522" class="wp-caption-text"&gt;Figure 8. The trust relationship allowing AWS Well-Architected Tool to assume policy on behalf of the workload&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Using integration with AWS Trusted Advisor for insights during reviews&lt;/h4&gt; 
&lt;p&gt;Once the feature is enabled, additional insights will be noticeable about the resources in your workload using Trusted Advisor checks. Let’s explore an example question. In this case, we will use &lt;a href="https://wa.aws.amazon.com/wat.question.REL_9.en.html"&gt;Question 9 from the Reliability Pillar&lt;/a&gt;, as there are Trusted Advisor checks related to the best practices in it: How do you back up data?&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;AWS Well-Architected Reliability Question 9 includes best practices that are related to how workload backup is performed to support the ability for the workload to recover from failure. Current findings using Trusted Advisor checks indicates the workload may not be configured based on the&lt;strong&gt; “Perform data backup automatically”&lt;/strong&gt; best practice in the Reliability Pillar (Figure 9). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12347" style="width: 684px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-9.-Perform-data-backup-automatically-best-practices.png"&gt;&lt;img aria-describedby="caption-attachment-12347" loading="lazy" class="wp-image-12347 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-9.-Perform-data-backup-automatically-best-practices.png" alt="&amp;quot;Perform data backup automatically&amp;quot; best practices" width="674" height="369"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12347" class="wp-caption-text"&gt;Figure 9. “Perform data backup automatically” best practices&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;To access Trusted Advisor checks as insights, you can select a question in the Well-Architected Tool (Figure 10). If there are related Trusted Advisor checks available for a question, there will be a &lt;strong&gt;“View checks”&lt;/strong&gt; button like the screenshot below. You can also select the &lt;strong&gt;“Trusted Advisor checks”&lt;/strong&gt; tab. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12349" style="width: 854px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-10.-Trusted-Advisor-checks-that-map-to-best-practices.png"&gt;&lt;img aria-describedby="caption-attachment-12349" loading="lazy" class="wp-image-12349 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-10.-Trusted-Advisor-checks-that-map-to-best-practices.png" alt="Trusted Advisor checks that map to best practices" width="844" height="704"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12349" class="wp-caption-text"&gt;Figure 10. AWS Trusted Advisor checks that map to best practices&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Trusted Advisor checks&lt;/strong&gt; are available, which provide insights related to the &lt;strong&gt;best practice&lt;/strong&gt; in the question. You will also notice the state of resources recommendations and the count of resources. Trusted Advisor checks that relate to the best practice “Perform data backup automatically” are displayed. One of the Trusted Advisor checks identified with a &lt;em&gt;x&lt;/em&gt; in a circle (denoting “Action recommended”) status is on the &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html"&gt;Amazon Elastic Block Storage (Amazon EBS) snapshots&lt;/a&gt; availability to &lt;a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/restore.html"&gt;recover your EBS volume&lt;/a&gt; from in the event of disaster (Figure 11). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12352" style="width: 1338px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-11.-AWS-Trusted-Advisor-check-for-Amazon-EBS-snapshots-with-action-required.png"&gt;&lt;img aria-describedby="caption-attachment-12352" loading="lazy" class="wp-image-12352 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-11.-AWS-Trusted-Advisor-check-for-Amazon-EBS-snapshots-with-action-required.png" alt="AWS Trusted Advisor check for Amazon EBS snapshots with &amp;quot;Action recommended&amp;quot;" width="1328" height="553"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12352" class="wp-caption-text"&gt;Figure 11. AWS Trusted Advisor check for Amazon EBS snapshots with “Action recommended”&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Exploring the &lt;a href="https://us-east-1.console.aws.amazon.com/trustedadvisor/home?region=us-east-1"&gt;Trusted Advisor Console&lt;/a&gt;, you can identify the EBS volume ID that has been detected with no snapshot in this us-west-2 region (Figure 12). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12353" style="width: 1735px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-12.-An-EBS-volume-that-does-not-have-snapshots.png"&gt;&lt;img aria-describedby="caption-attachment-12353" loading="lazy" class="wp-image-12353 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-12.-An-EBS-volume-that-does-not-have-snapshots.png" alt="An EBS volume that does not have snapshots" width="1725" height="344"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12353" class="wp-caption-text"&gt;Figure 12. An EBS volume that does not have snapshots&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;With the insights from Trusted Advisor, we can quickly determine that the &lt;strong&gt;“Perform data backup automatically”&lt;/strong&gt; best practice is not in place, as we do not have Amazon EBS snapshots enabled. Through the “helpful resources” section, instructions can be found to help automate the snapshot creation of Amazon EBS volume (Figure 13). One method to achieve this is to use &lt;a href="https://docs.aws.amazon.com/aws-backup/?id=docs_gateway"&gt;AWS Backup&lt;/a&gt;. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12354" style="width: 1335px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-13.-Resources-with-details-about-best-practices-including-links-to-learn-more.png"&gt;&lt;img aria-describedby="caption-attachment-12354" loading="lazy" class="wp-image-12354 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-13.-Resources-with-details-about-best-practices-including-links-to-learn-more.png" alt="Resources with details about best practices, including links to learn more" width="1325" height="644"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12354" class="wp-caption-text"&gt;Figure 13. Resources with details about best practices, including links to learn more&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Using AWS Backup you can define a backup plan to automate snapshots creation of the EBS volume. Using this plan, you adjust the frequency of the backup to help achieve your recovery time objective and recovery point objective (Figure 14). For more information on how to configure EBS volume backup plan, refer to the &lt;a href="https://docs.aws.amazon.com/aws-backup/latest/devguide/creating-a-backup-plan.html"&gt;Developer Guide on creating a backup plan&lt;/a&gt;. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12355" style="width: 1515px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-14.-Setup-automatic-Amazon-EBS-volume-snapshots.png"&gt;&lt;img aria-describedby="caption-attachment-12355" loading="lazy" class="wp-image-12355 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-14.-Setup-automatic-Amazon-EBS-volume-snapshots.png" alt="Setup automatic Amazon EBS volume snapshots" width="1505" height="835"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12355" class="wp-caption-text"&gt;Figure 14. Setup automatic Amazon EBS volume snapshots&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Once this improvement is implemented and the related EBS volume snapshot is taken, Trusted Advisor will reflect the changes to the resource (Figure 15). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12356" style="width: 1594px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-15.-Amazon-EBS-volume-with-a-snapshot.png"&gt;&lt;img aria-describedby="caption-attachment-12356" loading="lazy" class="wp-image-12356 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-15.-Amazon-EBS-volume-with-a-snapshot.png" alt="Amazon EBS volume with a snapshot" width="1584" height="342"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12356" class="wp-caption-text"&gt;Figure 15. Amazon EBS volume with a snapshot&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;The next time we perform a Well-Architected Framework Review on this workload, the related AWS Trusted Advisor Check will show no action required with a &lt;strong&gt;check-mark&lt;/strong&gt; status (Figure 16). 
  &lt;div id="attachment_12357" style="width: 900px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-16.-AWS-Trusted-Advisor-checks-that-represent-improvements-that-have-been-implemented.png"&gt;&lt;img aria-describedby="caption-attachment-12357" loading="lazy" class="wp-image-12357 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-16.-AWS-Trusted-Advisor-checks-that-represent-improvements-that-have-been-implemented.png" alt="AWS Trusted Advisor checks that represent improvements that have been implemented" width="890" height="353"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12357" class="wp-caption-text"&gt;Figure 16. AWS Trusted Advisor checks that represent improvements that have been implemented&lt;/p&gt;
  &lt;/div&gt; &lt;p&gt;&lt;i&gt;Optional&lt;/i&gt;: For access to the list of Trusted Advisor checks in .csv format, you can click on the &lt;strong&gt;“Download check details”&lt;/strong&gt; button on each question to download the resources that were checked in relation to the specified best practices (Figure 17).&lt;/p&gt; &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12358" style="width: 854px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-17.-Download-check-details-button.png"&gt;&lt;img aria-describedby="caption-attachment-12358" loading="lazy" class="wp-image-12358 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-17.-Download-check-details-button.png" alt="&amp;quot;Download check details&amp;quot; button" width="844" height="336"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12358" class="wp-caption-text"&gt;Figure 17. “Download check details” button&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Once implemented, this improvement ensures a means to recover the EBS volume data in the event of disaster. This makes the resources in the workload better aligned to the AWS Reliability Pillar Design principle of “Automatically recover from failure”. To reflect this alignment in the Well-Architected Tool, you can tick on the best practice check items under the related questions (Figure 18). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12359" style="width: 857px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-18.-A-milestone-with-updated-best-practices-based-on-improvements-that-have-been-implemented.png"&gt;&lt;img aria-describedby="caption-attachment-12359" loading="lazy" class="wp-image-12359 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-18.-A-milestone-with-updated-best-practices-based-on-improvements-that-have-been-implemented.png" alt="A milestone with updated best practices based on improvements that have been implemented" width="847" height="436"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12359" class="wp-caption-text"&gt;Figure 18. A milestone with updated best practices based on improvements that have been implemented&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Finally, you can create a &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/userguide/milestones.html"&gt;milestone&lt;/a&gt; to capture a point in time state of your workload WAFR. As you continuously optimize with more WAFRs and improvements, the number of high- and medium-risk items identified within each review will decrease. You will notice the continuous optimization of your workload over time, as in Figure 19. &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12360" style="width: 1307px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-19.-The-history-of-improvements-being-made-over-time.png"&gt;&lt;img aria-describedby="caption-attachment-12360" loading="lazy" class="wp-image-12360 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/07/Figure-19.-The-history-of-improvements-being-made-over-time.png" alt="The history of improvements being made over time" width="1297" height="632"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12360" class="wp-caption-text"&gt;Figure 19. The history of improvements being made over time&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Using the AWS Well-Architected integration with AWS Trusted Advisor, customers have a mechanism to accelerate the “learn, measure, and improve” Well-Architected virtuous cycle. We have demonstrated the value of creating acceleration through the insights from Trusted Advisor checks. You now know how to enable the integration with Trusted Advisor and have seen an example of how the insights can accelerate your review cycle. You will notice the improvements you make over time will reflect in the Trusted Advisor checks as you review the milestones for your workloads. Enable this feature on your next Well-Architected Framework Review (WAFR) to measure the impact that data-driven insights from Trusted Advisor can have on reducing the time-to-value for your reviews. For more information consider these additional resources. You can contact your account team for support in running WAFRs or check out the &lt;a href="https://aws.amazon.com/partners/programs/well-architected/"&gt;AWS Well-Architected Partner Program&lt;/a&gt; to &lt;a href="https://bit.ly/3fC7H0g"&gt;find a partner&lt;/a&gt; that can help you run a review. Additionally, running a WAFR with a partner assisting you in remediating risks may also provide funding credits to offset the costs required to make the improvements.&lt;/p&gt; 
&lt;table border="1"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;“Perform data backup automatically” is part of the Reliability Pillar of the AWS Well-Architected Framework. AWS Well-Architected is a set of guiding design principles developed by AWS to help organizations build secure, high-performing, resilient, and efficient infrastructure for a variety of applications and workloads. Use the AWS Well-Architected Tool to review your workloads periodically to address important design considerations and ensure that they follow the best practices and guidance of the AWS Well-Architected Framework. For follow up questions or comments, join our growing community on AWS re:Post.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Deploying IBM Cloud Pak for integration on Red Hat OpenShift Service on AWS</title>
		<link>https://aws.amazon.com/blogs/architecture/deploying-ibm-cloud-pak-for-integration-on-red-hat-openshift-service-on-aws/</link>
					
		
		<dc:creator><![CDATA[Eduardo Monich Fronza]]></dc:creator>
		<pubDate>Mon, 07 Nov 2022 14:15:40 +0000</pubDate>
				<category><![CDATA[Amazon Elastic Block Store (Amazon EBS)]]></category>
		<category><![CDATA[Amazon Elastic File System (EFS)]]></category>
		<category><![CDATA[Architecture]]></category>
		<guid isPermaLink="false">7ef5880ad76028614173cc1fd6e36a84b96df4d1</guid>

					<description>Customers across many industries use IBM integration software, such as IBM MQ, DataPower, API Connect, and App Connect, as the backbone that integrates and orchestrates their business-critical workloads. These customers often tell Amazon Web Services (AWS), they want to migrate their applications to AWS Cloud, as part of their business strategy: to lower costs, gain […]</description>
										<content:encoded>&lt;p&gt;Customers across many industries use IBM integration software, such as &lt;a href="https://www.ibm.com/products/mq"&gt;IBM MQ&lt;/a&gt;, &lt;a href="https://www.ibm.com/products/datapower-gateway"&gt;DataPower&lt;/a&gt;, &lt;a href="https://www.ibm.com/products/api-connect"&gt;API Connect&lt;/a&gt;, and &lt;a href="https://www.ibm.com/products/app-connect"&gt;App Connect&lt;/a&gt;, as the backbone that integrates and orchestrates their business-critical workloads.&lt;/p&gt; 
&lt;p&gt;These customers often tell Amazon Web Services (AWS), they want to migrate their applications to AWS Cloud, as part of their business strategy: to lower costs, gain agility, and innovate faster.&lt;/p&gt; 
&lt;p&gt;In this blog, we will explore how customers, who are looking at ways to run IBM software on AWS, can use&amp;nbsp;&lt;a href="https://aws.amazon.com/rosa/"&gt;Red Hat OpenShift Service on AWS (ROSA)&lt;/a&gt; to deploy &lt;a href="https://www.ibm.com/products/cloud-pak-for-integration"&gt;IBM Cloud Pak for Integration (CP4I)&lt;/a&gt; with modernized versions of IBM integration products.&lt;/p&gt; 
&lt;p&gt;As ROSA is a fully managed OpenShift service that is jointly supported by AWS and Red Hat, plus managed by Red Hat site reliability engineers, customers benefit from not having to manage the lifecycle of&amp;nbsp;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/container-platform"&gt;Red Hat OpenShift Container Platform (OCP)&lt;/a&gt; clusters.&lt;/p&gt; 
&lt;p&gt;This post explains the steps to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a ROSA cluster&lt;/li&gt; 
 &lt;li&gt;Configure persistent storage&lt;/li&gt; 
 &lt;li&gt;Install CP4I&amp;nbsp;and the &lt;a href="https://www.ibm.com/docs/en/ibm-mq/9.3"&gt;IBM MQ 9.3&lt;/a&gt; operator&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cloud Pak for integration architecture&lt;/h2&gt; 
&lt;p&gt;In this blog, we are implementing a highly available ROSA cluster with three Availability Zones (AZ), three master nodes, three infrastructure nodes, and three worker nodes.&lt;/p&gt; 
&lt;p&gt;Review the AWS documentation for &lt;a href="https://aws.amazon.com/about-aws/global-infrastructure/regions_az/"&gt;Regions and AZs&lt;/a&gt; and the &lt;a href="https://aws.amazon.com/rosa/faqs/"&gt;regions where ROSA is available&lt;/a&gt; to choose the best region for your deployment.&lt;/p&gt; 
&lt;p&gt;Figure 1 demonstrates the solution’s architecture.&lt;/p&gt; 
&lt;div id="attachment_12334" style="width: 1711px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/04/Figure-1.png"&gt;&lt;img aria-describedby="caption-attachment-12334" loading="lazy" class="wp-image-12334 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/04/Figure-1.png" alt="IBM Cloud Pak for Integration on ROSA architecture" width="1701" height="1321"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12334" class="wp-caption-text"&gt;Figure 1. IBM Cloud Pak for Integration on ROSA architecture&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In our scenario, we are building a public ROSA cluster, with an internet-facing Classic Load Balancer providing access to Ports 80 and 443. Consider using a &lt;a href="https://aws.amazon.com/blogs/containers/red-hat-openshift-service-on-aws-private-clusters-with-aws-privatelink/"&gt;ROSA private cluster&lt;/a&gt; when you are deploying CP4I in your AWS account.&lt;/p&gt; 
&lt;p&gt;We are using &lt;a href="https://docs.aws.amazon.com/efs/?id=docs_gateway"&gt;Amazon Elastic File System (Amazon EFS)&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/ebs/?id=docs_gateway"&gt;Amazon Elastic Block Store (Amazon EBS)&lt;/a&gt; for our cluster’s persistent storage. Review the &lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-integration/2022.2?topic=requirements-supported-options-amazon-web-services-aws"&gt;IBM CP4I documentation&lt;/a&gt; for information about supported AWS storage options.&lt;/p&gt; 
&lt;p&gt;Review &lt;a href="https://docs.openshift.com/rosa/rosa_planning/rosa-aws-prereqs.html"&gt;AWS prerequisites for ROSA&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html"&gt;AWS Security best practices in IAM&lt;/a&gt; documentation, before deploying CP4I for production workloads, to protect your AWS account and resources.&lt;/p&gt; 
&lt;h2&gt;Cost&lt;/h2&gt; 
&lt;p&gt;You are responsible for the cost of the AWS services used when deploying CP4I in your AWS account. For cost estimates, see the pricing pages for each AWS service you use.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before getting started, review the following prerequisites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;This blog assumes familiarity with: CP4I, ROSA, &lt;a href="https://docs.aws.amazon.com/ec2/?id=docs_gateway"&gt;Amazon Elastic Compute Cloud (Amazon EC2)&lt;/a&gt;, Amazon EBS, Amazon EFS, &lt;a href="https://docs.aws.amazon.com/vpc/?id=docs_gateway"&gt;Amazon Virtual Private Cloud&lt;/a&gt;, &lt;a href="https://docs.aws.amazon.com/cloud9/?id=docs_gateway"&gt;AWS Cloud9&lt;/a&gt;, and &lt;a href="https://docs.aws.amazon.com/iam/?id=docs_gateway"&gt;AWS Identity and Access Management (IAM)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Access to an AWS account, with permissions to create the resources described in the installation steps section&lt;/li&gt; 
 &lt;li&gt;Verification of the &lt;a href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-sts-required-aws-service-quotas.html"&gt;required AWS service quotas&lt;/a&gt; to deploy ROSA. If needed, you can request service quota increases from the &lt;a href="https://console.aws.amazon.com/servicequotas/home/"&gt;AWS console&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Access to an &lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-integration/2022.2?topic=installing-applying-your-entitlement-key-online-installation"&gt;IBM entitlement API key&lt;/a&gt;: either a &lt;a href="https://www.ibm.com/account/reg/signup?formid=urx-46640"&gt;60-day trial&lt;/a&gt; or an &lt;a href="https://www.ibm.com/software/passportadvantage/index.html"&gt;existing entitlement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Access to a &lt;a href="https://cloud.redhat.com/openshift/token/rosa"&gt;Red Hat ROSA token&lt;/a&gt;; you can register on the &lt;a href="https://cloud.redhat.com/"&gt;Red Hat website&lt;/a&gt; to obtain one&lt;/li&gt; 
 &lt;li&gt;A &lt;em&gt;bastion host&lt;/em&gt; to run the CP4I installation, we have used an AWS Cloud 9 workspace. You can use another device, provided it supports the required software packages: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration.html"&gt;AWS Command Line Interface (&lt;code&gt;aws cli&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.openshift.com/rosa/rosa_cli/rosa-get-started-cli.html"&gt;Red Hat OpenShift Service on AWS command-line interface (&lt;code&gt;rosa&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.10/cli_reference/openshift_cli/getting-started-cli.html"&gt;OpenShift command-line interface (&lt;code&gt;oc&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/"&gt;Kubernetes command-line tool (&lt;code&gt;kubectl&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;The Linux utilities (&lt;code&gt;jq&lt;/code&gt;, &lt;code&gt;wget&lt;/code&gt;, &lt;code&gt;gettext&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation steps&lt;/h2&gt; 
&lt;p&gt;To deploy CP4I on ROSA, complete the following steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;From the &lt;a href="http://console.aws.amazon.com/rosa/home"&gt;AWS ROSA console&lt;/a&gt;, click &lt;strong&gt;Enable ROSA&lt;/strong&gt; to active the service on your AWS account (Figure 2). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12292" style="width: 2010px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-2.-Enable-ROSA-on-your-AWS-account.png"&gt;&lt;img aria-describedby="caption-attachment-12292" loading="lazy" class="wp-image-12292 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-2.-Enable-ROSA-on-your-AWS-account.png" alt="Enable ROSA on your AWS account" width="2000" height="601"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12292" class="wp-caption-text"&gt;Figure 2. Enable ROSA on your AWS account&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Create an &lt;a href="https://console.aws.amazon.com/cloud9/"&gt;AWS Cloud9&lt;/a&gt; environment to run your CP4I installation. We used a &lt;strong&gt;t3.small&lt;/strong&gt; instance type.&lt;/li&gt; 
 &lt;li&gt;When it comes up, close the &lt;strong&gt;Welcome tab&lt;/strong&gt; and open a new &lt;strong&gt;Terminal tab&lt;/strong&gt; to install the required packages: &lt;pre&gt;&lt;code class="lang-bash"&gt;curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
wget https://mirror.openshift.com/pub/openshift-v4/clients/rosa/latest/rosa-linux.tar.gz
sudo tar -xvzf rosa-linux.tar.gz -C /usr/local/bin/

rosa download oc
sudo tar -xvzf openshift-client-linux.tar.gz -C /usr/local/bin/

sudo yum -y install jq gettext&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Ensure the &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/elb-service-linked-roles.html"&gt;ELB service-linked role&lt;/a&gt; exists in your AWS account: &lt;pre&gt;&lt;code class="lang-bash"&gt;aws iam get-role --role-name 
"AWSServiceRoleForElasticLoadBalancing" || aws iam create-service-linked-role --aws-service-name 
"elasticloadbalancing.amazonaws.com"&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://us-east-1.console.aws.amazon.com/iam/home#/policies$new?step=edit"&gt;Create an IAM policy&lt;/a&gt; named cp4i-installer-permissions with the following permissions: &lt;pre&gt;&lt;code class="lang-json"&gt;{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "autoscaling:*",
                "cloudformation:*",
                "cloudwatch:*",
                "ec2:*",
                "elasticfilesystem:*",
                "elasticloadbalancing:*",
                "events:*",
                "iam:*",
                "kms:*",
                "logs:*",
                "route53:*",
                "s3:*",
                "servicequotas:GetRequestedServiceQuotaChange",
                "servicequotas:GetServiceQuota",
                "servicequotas:ListServices",
                "servicequotas:ListServiceQuotas",
                "servicequotas:RequestServiceQuotaIncrease",
                "sts:*",
                "support:*",
                "tag:*"
            ],
            "Resource": "*"
        }
    ]
}&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://us-east-1.console.aws.amazon.com/iamv2/home#/roles/create?commonUseCase=EC2&amp;amp;step=addPermission&amp;amp;trustedEntityType=AWS_SERVICE"&gt;Create an IAM role&lt;/a&gt;: 
  &lt;ol&gt; 
   &lt;li&gt;Select &lt;strong&gt;AWS service&lt;/strong&gt; and &lt;strong&gt;EC2&lt;/strong&gt;, then click &lt;strong&gt;Next: Permissions&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;Select the &lt;strong&gt;cp4i-installer-permissions&lt;/strong&gt; policy, and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;Name it &lt;strong&gt;cp4i-installer&lt;/strong&gt;, and click &lt;strong&gt;Create role&lt;/strong&gt;.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;From your AWS Cloud9 IDE, click the grey circle button on the top right, and select Manage EC2 Instance (Figure 3). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12295" style="width: 1864px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-3.-Manage-the-AWS-Cloud9-EC2-instance.png"&gt;&lt;img aria-describedby="caption-attachment-12295" loading="lazy" class="wp-image-12295 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-3.-Manage-the-AWS-Cloud9-EC2-instance.png" alt="Manage the AWS Cloud9 EC2 instance" width="1854" height="825"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12295" class="wp-caption-text"&gt;Figure 3. Manage the AWS Cloud9 EC2 instance&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;On the &lt;a href="https://console.aws.amazon.com/ec2/home#Instances"&gt;Amazon EC2 console&lt;/a&gt;, select the AWS Cloud9 instance, then choose &lt;strong&gt;Actions / Security / Modify IAM Role&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;cp4i-installer&lt;/strong&gt; from the&lt;strong&gt; IAM Role&lt;/strong&gt; drop down, and click &lt;strong&gt;Update IAM role&lt;/strong&gt; (Figure 4). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12296" style="width: 1472px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-4.-Attach-the-IAM-role-to-your-Workspace.png"&gt;&lt;img aria-describedby="caption-attachment-12296" loading="lazy" class="wp-image-12296 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-4.-Attach-the-IAM-role-to-your-Workspace.png" alt="Attach the IAM role to your workspace" width="1462" height="815"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12296" class="wp-caption-text"&gt;Figure 4. Attach the IAM role to your workspace&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Update the IAM settings for your AWS Cloud9 workspace: &lt;pre&gt;&lt;code class="lang-bash"&gt;aws cloud9 update-environment --environment-id $C9_PID --managed-credentials-action DISABLE
rm -vf ${HOME}/.aws/credentials&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Configure the following environment variables: &lt;pre&gt;&lt;code class="lang-bash"&gt;export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)
export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')
export ROSA_CLUSTER_NAME=cp4iblog01&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Configure the &lt;strong&gt;aws cli&lt;/strong&gt; default region: &lt;pre&gt;&lt;code class="lang-bash"&gt;aws configure set default.region ${AWS_REGION}&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to the &lt;a href="https://console.redhat.com/openshift/token/show"&gt;Red Hat Hybrid Cloud Console&lt;/a&gt;, and copy your &lt;a href="https://cloud.redhat.com/openshift/token/rosa"&gt;OpenShift Cluster Manager API Token&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Use the token and log in to your Red Hat account: &lt;pre&gt;&lt;code class="lang-bash"&gt;rosa login --token=&lt;em&gt;&lt;strong&gt;&amp;lt;your_openshift_api_token&amp;gt;&lt;/strong&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Verify that your AWS account satisfies the quotas to deploy your cluster: &lt;pre&gt;&lt;code class="lang-bash"&gt;rosa verify quota&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;When deploying ROSA for the first time, create the account-wide roles: &lt;pre&gt;&lt;code class="lang-bash"&gt;rosa create account-roles --mode auto --yes&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create your ROSA cluster: &lt;pre&gt;&lt;code class="lang-bash"&gt;rosa create cluster --cluster-name $ROSA_CLUSTER_NAME --sts \
  --multi-az \
  --region $AWS_REGION \
  --version 4.10.35 \
  --compute-machine-type m5.4xlarge \
  --compute-nodes 3 \
  --operator-roles-prefix cp4irosa \
  --mode auto --yes \
  --watch&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Once your cluster is ready, create a &lt;strong&gt;cluster-admin&lt;/strong&gt; user (it takes approximately 5 minutes): &lt;pre&gt;&lt;code class="lang-bash"&gt;rosa create admin --cluster=$ROSA_CLUSTER_NAME&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Log in to your cluster using the &lt;strong&gt;cluster-admin&lt;/strong&gt; credentials. You can copy the command from the output of the previous step. For example: &lt;pre&gt;&lt;code class="lang-bash"&gt;oc login https://&lt;em&gt;&lt;strong&gt;&amp;lt;your_cluster_api_address&amp;gt;&lt;/strong&gt;&lt;/em&gt;:6443 \
  --username cluster-admin \
  --password &lt;em&gt;&lt;strong&gt;&amp;lt;your_cluster-admin_password&amp;gt;&lt;/strong&gt;&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create an IAM policy allowing ROSA to use Amazon EFS: &lt;pre&gt;&lt;code class="lang-bash"&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; $PWD/efs-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
 {
   "Effect": "Allow",
   "Action": [
     "elasticfilesystem:DescribeAccessPoints",
     "elasticfilesystem:DescribeFileSystems"
   ],
   "Resource": "*"
 },
 {
   "Effect": "Allow",
   "Action": [
     "elasticfilesystem:CreateAccessPoint"
   ],
   "Resource": "*",
   "Condition": {
     "StringLike": {
       "aws:RequestTag/efs.csi.aws.com/cluster": "true"
     }
   }
 },
 {
   "Effect": "Allow",
   "Action": "elasticfilesystem:DeleteAccessPoint",
   "Resource": "*",
   "Condition": {
     "StringEquals": {
       "aws:ResourceTag/efs.csi.aws.com/cluster": "true"
     }
   }
 }
  ]
}
EOF
POLICY=$(aws iam create-policy --policy-name "${ROSA_CLUSTER_NAME}-cp4i-efs-csi" --policy-document file://$PWD/efs-policy.json --query 'Policy.Arn' --output text) || POLICY=$(aws iam list-policies --query 'Policies[?PolicyName==`cp4i-efs-csi`].Arn' --output text)&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create an IAM trust policy: &lt;pre&gt;&lt;code class="lang-bash"&gt;export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e "s/^https:\/\///")
cat &amp;lt;&amp;lt;EOF &amp;gt; $PWD/TrustPolicy.json
{
  "Version": "2012-10-17",
  "Statement": [
 {
   "Effect": "Allow",
   "Principal": {
     "Federated": "arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}"
   },
   "Action": "sts:AssumeRoleWithWebIdentity",
   "Condition": {
     "StringEquals": {
       "${OIDC_PROVIDER}:sub": [
         "system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-operator",
         "system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-controller-sa"
       ]
     }
   }
 }
  ]
}
EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create an IAM role with the previously created policies: &lt;pre&gt;&lt;code class="lang-bash"&gt;ROLE=$(aws iam create-role \
  --role-name "${ROSA_CLUSTER_NAME}-aws-efs-csi-operator" \
  --assume-role-policy-document file://$PWD/TrustPolicy.json \
  --query "Role.Arn" --output text)
aws iam attach-role-policy \
  --role-name "${ROSA_CLUSTER_NAME}-aws-efs-csi-operator" \
  --policy-arn $POLICY&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create an OpenShift secret to store the AWS access keys: &lt;pre&gt;&lt;code class="lang-bash"&gt;cat &amp;lt;&amp;lt;EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: aws-efs-cloud-credentials
  namespace: openshift-cluster-csi-drivers
stringData:
  credentials: |-
    [default]
    role_arn = $ROLE
    web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token
EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Install the &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html"&gt;Amazon EFS CSI driver&lt;/a&gt; operator: &lt;pre&gt;&lt;code class="lang-bash"&gt;cat &amp;lt;&amp;lt;EOF | oc create -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  generateName: openshift-cluster-csi-drivers-
  namespace: openshift-cluster-csi-drivers
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  labels:
    operators.coreos.com/aws-efs-csi-driver-operator.openshift-cluster-csi-drivers: ""
  name: aws-efs-csi-driver-operator
  namespace: openshift-cluster-csi-drivers
spec:
  channel: stable
  installPlanApproval: Automatic
  name: aws-efs-csi-driver-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Track the operator installation: &lt;pre&gt;&lt;code class="lang-bash"&gt;watch oc get deployment aws-efs-csi-driver-operator \
 -n openshift-cluster-csi-drivers&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Install the AWS EFS CSI driver: &lt;pre&gt;&lt;code class="lang-bash"&gt;cat &amp;lt;&amp;lt;EOF | oc apply -f -
apiVersion: operator.openshift.io/v1
kind: ClusterCSIDriver
metadata:
  name: efs.csi.aws.com
spec:
  managementState: Managed
EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Wait until the CSI driver is running: &lt;pre&gt;&lt;code class="lang-bash"&gt;watch oc get daemonset aws-efs-csi-driver-node \
 -n openshift-cluster-csi-drivers&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create a rule allowing inbound NFS traffic from your cluster’s VPC Classless Inter-Domain Routing (CIDR): &lt;pre&gt;&lt;code class="lang-bash"&gt;NODE=$(oc get nodes --selector=node-role.kubernetes.io/worker -o jsonpath='{.items[0].metadata.name}')
VPC_ID=$(aws ec2 describe-instances --filters "Name=private-dns-name,Values=$NODE" --query 'Reservations[*].Instances[*].{VpcId:VpcId}' | jq -r '.[0][0].VpcId')
CIDR=$(aws ec2 describe-vpcs --filters "Name=vpc-id,Values=$VPC_ID" --query 'Vpcs[*].CidrBlock' | jq -r '.[0]')
SG=$(aws ec2 describe-instances --filters "Name=private-dns-name,Values=$NODE" --query 'Reservations[*].Instances[*].{SecurityGroups:SecurityGroups}' | jq -r '.[0][0].SecurityGroups[0].GroupId')
aws ec2 authorize-security-group-ingress \
  --group-id $SG \
  --protocol tcp \
  --port 2049 \
  --cidr $CIDR | jq .&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create an Amazon EFS file system: &lt;pre&gt;&lt;code class="lang-bash"&gt;EFS_FS_ID=$(aws efs create-file-system --performance-mode generalPurpose --encrypted --region ${AWS_REGION} --tags Key=Name,Value=ibm_cp4i_fs | jq -r '.FileSystemId')
SUBNETS=($(aws ec2 describe-subnets --filters "Name=vpc-id,Values=${VPC_ID}" "Name=tag:Name,Values=*${ROSA_CLUSTER_NAME}*private*" | jq --raw-output '.Subnets[].SubnetId'))
for subnet in ${SUBNETS[@]}; do
  aws efs create-mount-target \
    --file-system-id $EFS_FS_ID \
    --subnet-id $subnet \
    --security-groups $SG
done&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create an Amazon EFS storage class: &lt;pre&gt;&lt;code class="lang-bash"&gt;cat &amp;lt;&amp;lt;EOF | oc apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: $EFS_FS_ID
  directoryPerms: "750"
  gidRangeStart: "1000"
  gidRangeEnd: "2000"
  basePath: "/ibm_cp4i_rosa_fs"
EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Add the IBM catalog sources to OpenShift: &lt;pre&gt;&lt;code class="lang-bash"&gt;cat &amp;lt;&amp;lt;EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: ibm-operator-catalog
  namespace: openshift-marketplace
spec:
  displayName: IBM Operator Catalog
  image: 'icr.io/cpopen/ibm-operator-catalog:latest'
  publisher: IBM
  sourceType: grpc
  updateStrategy:
    registryPoll:
      interval: 45m
EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Get the console URL of your ROSA cluster: &lt;pre&gt;&lt;code class="lang-bash"&gt;rosa describe cluster --cluster=$ROSA_CLUSTER_NAME | grep Console&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Copy your entitlement key from the &lt;a href="https://myibm.ibm.com/products-services/containerlibrary"&gt;IBM container software library&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Log in to your ROSA web console, navigate to &lt;strong&gt;Workloads &amp;gt; Secrets&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Set the project to &lt;strong&gt;openshift-config&lt;/strong&gt;; locate and click &lt;strong&gt;pull-secret&lt;/strong&gt; (Figure 5). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12301" style="width: 2010px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-5.-Edit-the-pull-secret-entry.png"&gt;&lt;img aria-describedby="caption-attachment-12301" loading="lazy" class="wp-image-12301 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-5.-Edit-the-pull-secret-entry.png" alt="Edit the pull-secret entry" width="2000" height="695"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12301" class="wp-caption-text"&gt;Figure 5. Edit the &lt;em&gt;pull-secret&lt;/em&gt; entry&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Expand &lt;strong&gt;Actions&lt;/strong&gt; and click &lt;strong&gt;Edit Secret&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Scroll to the end of the page, and click &lt;strong&gt;Add credentials&lt;/strong&gt; (Figure 6): 
  &lt;ol&gt; 
   &lt;li&gt;Registry server address: cp.icr.io&lt;/li&gt; 
   &lt;li&gt;Username field: cp&lt;/li&gt; 
   &lt;li&gt;Password: &lt;em&gt;&lt;em&gt;&lt;strong&gt;your_ibm_entitlement_key&lt;br&gt; &lt;/strong&gt;&lt;/em&gt;&lt;/em&gt;&lt;p&gt;&lt;/p&gt; 
    &lt;div id="attachment_12302" style="width: 1915px" class="wp-caption alignnone"&gt;
     &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-6.-Configure-your-IBM-entitlement-key-secret.png"&gt;&lt;img aria-describedby="caption-attachment-12302" loading="lazy" class="wp-image-12302 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-6.-Configure-your-IBM-entitlement-key-secret.png" alt="Configure your IBM entitlement key secret" width="1905" height="965"&gt;&lt;/a&gt;
     &lt;p id="caption-attachment-12302" class="wp-caption-text"&gt;Figure 6. Configure your IBM entitlement key secret&lt;/p&gt;
    &lt;/div&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;Next, navigate to &lt;strong&gt;Operators &amp;gt; OperatorHub&lt;/strong&gt;. On the &lt;em&gt;OperatorHub&lt;/em&gt; page, use the search filter to locate the tile for the operators you plan to install: &lt;strong&gt;IBM Cloud Pak for Integration&lt;/strong&gt; and &lt;strong&gt;IBM MQ&lt;/strong&gt;. Keep all values as default for both installations (Figure 7). For example, IBM Cloud Pak for Integration: &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12303" style="width: 1901px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-7.-Install-CP4I-operators.png"&gt;&lt;img aria-describedby="caption-attachment-12303" loading="lazy" class="wp-image-12303 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-7.-Install-CP4I-operators.png" alt="Figure 7. Install CP4I operators" width="1891" height="920"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12303" class="wp-caption-text"&gt;Figure 7. Install CP4I operators&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;Create a namespace for each CP4I workload that will be deployed. In this blog, we’ve created for the platform UI and IBM MQ: &lt;pre&gt;&lt;code class="lang-bash"&gt;oc new-project integration
oc new-project ibm-mq&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Review the &lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-integration/2022.2?topic=planning-licensing"&gt;IBM documentation&lt;/a&gt; to select the appropriate license for your deployment.&lt;/li&gt; 
 &lt;li&gt;Deploy the platform UI: &lt;pre&gt;&lt;code class="lang-bash"&gt;cat &amp;lt;&amp;lt;EOF | oc apply -f -
apiVersion: integration.ibm.com/v1beta1
kind: PlatformNavigator
metadata:
  name: integration-quickstart
  namespace: integration
spec:
  license:
    accept: true
    license: L-RJON-CD3JKX
  mqDashboard: true
  replicas: 3  # Number of replica pods, 1 by default, 3 for HA
  storage:
    class: efs-sc
  version: 2022.2.1
EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Track the deployment status, which takes approximately 40 minutes: &lt;pre&gt;&lt;code class="lang-bash"&gt;watch oc get platformnavigator -n integration&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create an IBM MQ queue manager instance: &lt;pre&gt;&lt;code class="lang-bash"&gt;cat &amp;lt;&amp;lt;EOF | oc apply -f -
apiVersion: mq.ibm.com/v1beta1
kind: QueueManager
metadata:
  name: qmgr-inst01
  namespace: ibm-mq
spec:
  license:
    accept: true
    license: L-RJON-CD3JKX
    use: NonProduction
  web:
    enabled: true
  template:
    pod:
      containers:
        - env:
            - name: MQSNOAUT
              value: 'yes'
          name: qmgr
  queueManager:
    resources:
      limits:
        cpu: 500m
      requests:
        cpu: 500m
    availability:
      type: SingleInstance
    storage:
      queueManager:
        type: persistent-claim
        class: gp3
        deleteClaim: true
        size: 2Gi
      defaultClass: gp3
    name: CP4IQMGR
  version: 9.3.0.1-r1
EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Check the status of the queue manager: &lt;pre&gt;&lt;code class="lang-bash"&gt;oc describe queuemanager qmgr-inst01 -n ibm-mq&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Validation steps&lt;/h2&gt; 
&lt;p&gt;Let’s verify our installation!&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run the commands to retrieve the CP4I URL and administrator password: &lt;pre&gt;&lt;code class="lang-bash"&gt;oc describe platformnavigator integration-quickstart \
  -n integration | grep "^.*UI Endpoint" | xargs | cut -d ' ' -f3
oc get secret platform-auth-idp-credentials \
  -n ibm-common-services -o jsonpath='{.data.admin_password}' \
  | base64 -d &amp;amp;&amp;amp; echo&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Using the information from the previous step, access your CP4I web console.&lt;/li&gt; 
 &lt;li&gt;Select the option to authenticate with the &lt;strong&gt;IBM provided credentials (admin only)&lt;/strong&gt; to login with your admin password.&lt;/li&gt; 
 &lt;li&gt;From the CP4I console, you can manage users and groups allowed to access the platform, install new operators, and view the components that are installed.&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;qmgr-inst01&lt;/strong&gt; in the &lt;strong&gt;Messaging&lt;/strong&gt; widget to bring up your IBM MQ setup (Figure 8). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12304" style="width: 2010px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-8.-CP4I-console-features.png"&gt;&lt;img aria-describedby="caption-attachment-12304" loading="lazy" class="wp-image-12304 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-8.-CP4I-console-features.png" alt="CP4I console features" width="2000" height="691"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12304" class="wp-caption-text"&gt;Figure 8. CP4I console features&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;In the &lt;strong&gt;Welcome to IBM MQ&lt;/strong&gt; panel, click the &lt;strong&gt;CP4IQMGR&lt;/strong&gt; queue manager. This shows the state, resources, and allows you to configure your instances (Figure 9). &lt;p&gt;&lt;/p&gt;
  &lt;div id="attachment_12307" style="width: 1927px" class="wp-caption alignnone"&gt;
   &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-9.-Queue-manager-details.png"&gt;&lt;img aria-describedby="caption-attachment-12307" loading="lazy" class="wp-image-12307 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/03/Figure-9.-Queue-manager-details.png" alt="Queue manager details" width="1917" height="748"&gt;&lt;/a&gt;
   &lt;p id="caption-attachment-12307" class="wp-caption-text"&gt;Figure 9. Queue manager details&lt;/p&gt;
  &lt;/div&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Congratulations! You have successfully deployed IBM CP4I on Red Hat OpenShift on AWS.&lt;/p&gt; 
&lt;h2&gt;Post installation&lt;/h2&gt; 
&lt;p&gt;Review the following topics, when you are installing CP4I on production environments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.openshift.com/rosa/rosa_getting_started/rosa_getting_started_iam/rosa-config-identity-providers.html"&gt;Configuring identity providers&lt;/a&gt; on ROSA&lt;/li&gt; 
 &lt;li&gt;Configure &lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-integration/2022.2?topic=administering-identity-access-management"&gt;identity and access management&lt;/a&gt; on CP4I&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-integration/2022.2?topic=installing-deploying-instances-capabilities"&gt;Deploying instances of capabilities&lt;/a&gt;, like API Connect, App Connect, and DataPower&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.openshift.com/rosa/rosa_cluster_admin/rosa_nodes/rosa-nodes-about-autoscaling-nodes.html"&gt;Enable auto scaling&lt;/a&gt; for your ROSA cluster&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-integration/2022.2?topic=administering-installing-configuring-cluster-logging"&gt;Configure logging&lt;/a&gt; and &lt;a href="https://www.ibm.com/docs/en/cloud-paks/cp-integration/2022.2?topic=administering-enabling-openshift-container-platform-monitoring"&gt;enable monitoring&lt;/a&gt; for your ROSA cluster&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.ibm.com/support/pages/ibm-mq-considerations-efs-aws"&gt;Considerations for Amazon EFS&lt;/a&gt; to setup IBM MQ using &lt;a href="https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html"&gt;Amazon EFS storage classes&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cleanup&lt;/h2&gt; 
&lt;p&gt;Connect to your Cloud9 workspace, and run the following steps to delete the CP4I installation, including ROSA. This avoids incurring future charges on your AWS account:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;EFS_EF_ID=$(aws efs describe-file-systems \
  --query 'FileSystems[?Name==`ibm_cp4i_fs`].FileSystemId' \
  --output text)
MOUNT_TARGETS=$(aws efs describe-mount-targets --file-system-id $EFS_EF_ID --query 'MountTargets[*].MountTargetId' --output text)
for mt in ${MOUNT_TARGETS[@]}; do
  aws efs delete-mount-target --mount-target-id $mt
done
aws efs delete-file-system --file-system-id $EFS_EF_ID

rosa delete cluster -c $ROSA_CLUSTER_NAME --yes --region $AWS_REGION&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Monitor your cluster uninstallation logs, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;rosa logs uninstall -c $ROSA_CLUSTER_NAME --watch&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the cluster is uninstalled, remove the operator-roles and oidc-provider, as informed in the output of the rosa delete command. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;rosa delete operator-roles -c 1vepskr2ms88ki76k870uflun2tjpvfs --mode auto –yes
rosa delete oidc-provider -c 1vepskr2ms88ki76k870uflun2tjpvfs --mode auto --yes&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;This post explored how to deploy CP4I on AWS ROSA. We also demonstrated how customers can take full advantage of managed OpenShift service, focusing on further modernizing application stacks by using AWS managed services (like ROSA) for their application deployments.&lt;/p&gt; 
&lt;p&gt;If you are interested in learning more about ROSA, take part in the &lt;a href="https://catalog.workshops.aws/aws-openshift-workshop/en-US/0-introduction"&gt;AWS ROSA Immersion Workshop&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Check out the blog on &lt;a href="https://aws.amazon.com/blogs/architecture/running-ibm-mq-on-aws-using-high-performance-amazon-fsx-for-netapp-ontap/"&gt;Running IBM MQ on AWS using High-performance Amazon FSx for NetApp ONTAP&lt;/a&gt; to learn how to use &lt;a href="https://aws.amazon.com/fsx/netapp-ontap/getting-started/"&gt;Amazon FSx for NetApp ONTAP&lt;/a&gt; for distributed storage and high availability with IBM MQ.&lt;/p&gt; 
&lt;p&gt;For more information and getting started with &lt;a href="https://aws.amazon.com/marketplace/search/results?searchTerms=IBM++cloud+pak"&gt;IBM Cloud Pak&lt;/a&gt; deployments, visit the&lt;a href="https://aws.amazon.com/marketplace"&gt; AWS Marketplace&lt;/a&gt; for new offerings.&lt;/p&gt; 
&lt;h2&gt;Further reading&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://partners.amazonaws.com/partners/001E000001IlLnmIAF/IBM"&gt;IBM on AWS Partner Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/containers/red-hat-openshift-service-on-aws-architecture-and-networking/"&gt;Red Hat OpenShift Service on AWS: architecture and networking&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/containers/red-hat-openshift-service-on-aws-private-clusters-with-aws-privatelink/"&gt;Red Hat OpenShift Service on AWS: private clusters with AWS PrivateLink&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>Optimize your modern data architecture for sustainability: Part 1 – data ingestion and data lake</title>
		<link>https://aws.amazon.com/blogs/architecture/optimize-your-modern-data-architecture-for-sustainability-part-1-data-ingestion-and-data-lake/</link>
					
		
		<dc:creator><![CDATA[Sam Mokhtari]]></dc:creator>
		<pubDate>Fri, 04 Nov 2022 21:37:38 +0000</pubDate>
				<category><![CDATA[Amazon Redshift]]></category>
		<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Glue]]></category>
		<category><![CDATA[AWS Well-Architected]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">3ce8e14c4f4b628fdb02e0255d1306dd57ba3007</guid>

					<description>The modern data architecture on AWS focuses on integrating a data lake and purpose-built data services to efficiently build analytics workloads, which provide speed and agility at scale. Using the right service for the right purpose not only provides performance gains, but facilitates the right utilization of resources. Review Modern Data Analytics Reference Architecture on […]</description>
										<content:encoded>&lt;p&gt;The modern data architecture on AWS focuses on integrating a data lake and purpose-built data services to efficiently build analytics workloads, which provide speed and agility at scale. Using the right service for the right purpose not only provides performance gains, but facilitates the right utilization of resources. Review &lt;a href="https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/modern-data-analytics-using-lake-house-ra.pdf?anld_da6"&gt;Modern Data Analytics Reference Architecture on AWS&lt;/a&gt;, see Figure 1.&lt;/p&gt; 
&lt;p&gt;In this series of two blog posts, we will cover guidance from&amp;nbsp;the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html"&gt;Sustainability Pillar&lt;/a&gt;&amp;nbsp;of the&amp;nbsp;&lt;a href="https://aws.amazon.com/architecture/well-architected/"&gt;AWS Well-Architected Framework&lt;/a&gt;&amp;nbsp;on optimizing your modern data architecture for sustainability. Sustainability in the cloud is an ongoing effort focused primarily on energy reduction and efficiency across all components of a workload. This will achieve the maximum benefit from the resources provisioned and minimize the total resources required.&lt;/p&gt; 
&lt;p&gt;Modern data architecture includes five pillars or capabilities: 1) data ingestion, 2) data lake, 3) unified data governance, 4) data movement, and 5) purpose-built analytics. In the first part of this blog series, we will focus on the data ingestion and data lake pillars of modern data architecture. We’ll discuss tips and best practices that can help you minimize resources and improve utilization.&lt;/p&gt; 
&lt;div id="attachment_12272" style="width: 1147px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12272" loading="lazy" class="size-full wp-image-12272" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/02/Fig1-MDARefArch.png" alt="Modern Data Analytics Reference Architecture on AWS" width="1137" height="688"&gt;
 &lt;p id="caption-attachment-12272" class="wp-caption-text"&gt;Figure 1. Modern Data Analytics Reference Architecture on AWS&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;1. Data ingestion&lt;/h2&gt; 
&lt;p&gt;The data ingestion process in modern data architecture can be broadly divided into two main categories: batch, and real-time ingestion modes.&lt;/p&gt; 
&lt;p&gt;To improve the data ingestion process, see the following best practices:&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Avoid unnecessary data ingestion&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Work backwards from your business needs and establish the right datasets you’ll need. Evaluate if you can avoid ingesting data from source systems by using existing publicly available datasets in&amp;nbsp;&lt;a href="https://aws.amazon.com/data-exchange/"&gt;AWS Data Exchange&lt;/a&gt; or &lt;a href="https://registry.opendata.aws/"&gt;Open Data on AWS&lt;/a&gt;. Using these cleaned and curated datasets will help you to avoid duplicating the compute and storage resources needed to ingest this data.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Reduce the size of data before ingestion&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;When you design your data ingestion pipelines, use strategies such as compression, filtering, and aggregation to reduce the size of ingested data. This will permit smaller data sizes to be transferred over network and stored in the data lake.&lt;/p&gt; 
&lt;p&gt;To extract and ingest data from data sources such as databases, use &lt;em&gt;change data capture&lt;/em&gt; (CDC) or date range strategies instead of full-extract ingestion. Use &lt;a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.Transformations.html"&gt;AWS Database Migration Service (DMS) transformation rules&lt;/a&gt; to selectively include and exclude the tables (from schema) and columns (from wide tables, for example) for ingestion.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Consider event-driven serverless data ingestion&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Adopt an event-driven serverless architecture for your data ingestion so it only provisions resources when work needs to be done. For example, when you use&amp;nbsp;&lt;a href="https://aws.amazon.com/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;amp;whats-new-cards.sort-order=desc"&gt;AWS Glue&amp;nbsp;jobs&lt;/a&gt; and&amp;nbsp;&lt;a href="https://aws.amazon.com/step-functions/"&gt;AWS Step Functions&lt;/a&gt;&amp;nbsp;for data ingestion and pre-processing, you pass the responsibility and work of infrastructure optimization to AWS.&lt;/p&gt; 
&lt;h2&gt;2. Data lake&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (S3)&lt;/a&gt; is an object storage service which customers use to store any type of data for different use cases as a foundation for a data lake. To optimize data lakes on Amazon S3, follow these best practices:&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Understand data&amp;nbsp;characteristics&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Understand the characteristics, requirements, and access patterns of your workload data in order to optimally choose the right storage tier. You can classify your data into categories shown in Figure 2, based on their key characteristics.&lt;/p&gt; 
&lt;div id="attachment_12326" style="width: 1560px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12326" loading="lazy" class="size-full wp-image-12326" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/04/Fig2b-dataChar.png" alt="Data Characteristics" width="1550" height="500"&gt;
 &lt;p id="caption-attachment-12326" class="wp-caption-text"&gt;Figure 2. Data Characteristics&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;&lt;em&gt;Adopt sustainable storage options&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Based on your workload data characteristics, use the appropriate storage tier to reduce the environmental impact of your workload, as shown in Figure 3.&lt;/p&gt; 
&lt;div id="attachment_12274" style="width: 1495px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12274" loading="lazy" class="size-full wp-image-12274" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/11/02/Fig3-storageTiering.png" alt="Storage tiering on Amazon S3" width="1485" height="643"&gt;
 &lt;p id="caption-attachment-12274" class="wp-caption-text"&gt;Figure 3. Storage tiering on Amazon S3&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;&lt;em&gt;Implement data lifecycle policies aligned with your sustainability goals&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Based on your data classification information, you can move data to more energy-efficient storage or safely delete it. Manage the lifecycle of all your data automatically using&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"&gt;Amazon S3 Lifecycle policies&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/s3/storage-analytics-insights/"&gt;Amazon S3 Storage Lens&lt;/a&gt; delivers visibility into storage usage, activity trends, and even makes recommendations for improvements. This information can be used to lower the environmental impact of storing information on S3.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Select efficient file formats and compression algorithms&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Use efficient file formats such as&amp;nbsp;&lt;a href="https://parquet.apache.org/"&gt;Parquet&lt;/a&gt;, where a columnar format provides opportunities for flexible compression options and encoding schemes. Parquet also enables more efficient aggregation queries, as you can skip over the non-relevant data. Using an efficient way of storage and accessing data is translated into higher performance with fewer resources.&lt;/p&gt; 
&lt;p&gt;Compress your data to reduce the storage size. Remember, you will need to trade off compression level (storage saved on disk) against the compute effort required to compress and decompress. Choosing the right compression algorithm can be beneficial as well. For instance, &lt;a href="http://facebook.github.io/zstd/"&gt;ZStandard (zstd)&lt;/a&gt; provides a better compression ratio compared with &lt;a href="https://github.com/lz4/lz4"&gt;LZ4&lt;/a&gt; or &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=2ahUKEwiN-ciOtvz6AhXSg_0HHfQyDs8QFnoECA0QAQ&amp;amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FGzip&amp;amp;usg=AOvVaw1120g27hRhasOwHpU4kPRQ"&gt;GZip&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Use data partitioning and bucketing&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html"&gt;Partitioning and bucketing&lt;/a&gt; divides your data and keeps related data together. This can help reduce the amount of data scanned per query, which means less compute resources needed to service the workload.&lt;/p&gt; 
&lt;h2&gt;Track and assess the improvement for environmental sustainability&lt;/h2&gt; 
&lt;p&gt;The best way for customers to evaluate success in optimizing their workloads for sustainability is to use &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/evaluate-specific-improvements.html"&gt;proxy measures and unit of work KPIs&lt;/a&gt;. For storage, this is GB per transaction, and for compute, it would be vCPU minutes per transaction. To use proxy measures to optimize workloads for energy efficiency, read &lt;a href="https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/"&gt;Sustainability Well-Architected Lab&lt;/a&gt; on &lt;em&gt;Turning the Cost and Usage Report into Efficiency Reports&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;In Table 1, we have listed certain metrics to use as a proxy metric to measure specific improvements. These fall under each pillar of modern data architecture covered in this post. This is not an exhaustive list, you could use numerous other metrics to spot inefficiencies. Remember, just tracking one metric may not explain the impact on sustainability. Use an analytical exercise of combining the metric with data, type of attributes, type of workload, and other characteristics.&lt;/p&gt; 
&lt;table style="text-align: top-left;border: 1px;padding: 5px"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th style="background-color: #6699cc;color: black;text-align: left;vertical-align: top;padding: 5px"&gt;Pillar&lt;/th&gt; 
   &lt;th style="background-color: #6699cc;color: black;text-align: left;vertical-align: top;padding: 5px"&gt;Metrics&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="background-color: #6699cc;color: black;text-align: left;vertical-align: top;padding: 5px"&gt;&lt;strong&gt;Data ingestion&lt;br&gt; &lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="background-color: #bdd5ed;text-align: left;vertical-align: top;padding: 5px"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics"&gt;DMS Replication Instance and Task Metrics&lt;/a&gt; – CPUAllocated, CPUUtilization, WriteThroughput, ReadThroughput&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html"&gt;CloudWatch Metrics – Amazon Kinesis&lt;/a&gt; – ReadProvisionedThroughputExceeded, WriteProvisionedThroughputExceeded&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/msk/latest/developerguide/metrics-details.html"&gt;CloudWatch Metrics – Amazon MSK&lt;/a&gt; – BurstBalance, CpuIdle, CpuIoWait, KafkaAppLogsDiskUsed, KafkaDataLogsDiskUsed&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="background-color: #6699cc;color: black;text-align: left;vertical-align: top;padding: 5px"&gt;&lt;strong&gt;Data lake&lt;br&gt; &lt;/strong&gt;&lt;/td&gt; 
   &lt;td style="background-color: #deeaf6;text-align: left;vertical-align: top;padding: 5px"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html"&gt;CloudWatch Metrics for Amazon S3&lt;/a&gt; – BucketSizeBytes&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;span style="color: #000000"&gt;Table 1. Metrics for the Modern data architecture pillars&lt;/span&gt;&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we have provided guidance and best practices to help reduce the environmental impact of the data ingestion and data lake pillars of modern data architecture.&lt;/p&gt; 
&lt;p&gt;In the &lt;a href="https://aws.amazon.com/blogs/architecture/optimize-your-modern-data-architecture-for-sustainability-part-2-unified-data-governance-data-movement-and-purpose-built-analytics/"&gt;Part 2&lt;/a&gt;, we cover best practices for sustainability for the unified governance, data movement, and purpose-built analytics and insights pillars.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To learn more, check out the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html"&gt;Sustainability Pillar of the AWS Well-Architected Framework&lt;/a&gt; and other blog posts on &lt;a href="https://aws.amazon.com/blogs/architecture/tag/sustainability/"&gt;architecting for sustainability&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For more architecture content, refer to &lt;a href="https://aws.amazon.com/architecture/"&gt;AWS Architecture Center&lt;/a&gt; for reference architecture diagrams, vetted architecture solutions, &lt;a href="https://aws.amazon.com/architecture/well-architected/"&gt;Well-Architected&lt;/a&gt; best practices, patterns, icons, and more.&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>How Wego secured developer connectivity to Amazon Relational Database Service instances</title>
		<link>https://aws.amazon.com/blogs/architecture/how-wego-secured-developer-connectivity-to-amazon-relational-database-service-instances/</link>
					
		
		<dc:creator><![CDATA[Adriaan de Jonge]]></dc:creator>
		<pubDate>Tue, 01 Nov 2022 13:34:12 +0000</pubDate>
				<category><![CDATA[Amazon EC2]]></category>
		<category><![CDATA[Amazon ElastiCache]]></category>
		<category><![CDATA[Amazon RDS]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Systems Manager]]></category>
		<guid isPermaLink="false">a9305bbc6b9227d3a1f2025bcd10fe1050c153f8</guid>

					<description>How do you securely access Amazon Relational Database Service (Amazon RDS) instances from a developer’s laptop? Online travel marketplace, Wego, shares their journey from bastion hosts in the public subnet to lightweight VPN tunnels on top of Session Manager, a capability of AWS Systems Manager, using temporary access keys. In this post, we explore how […]</description>
										<content:encoded>&lt;p&gt;How do you securely access &lt;a href="https://docs.aws.amazon.com/rds/?id=docs_gateway"&gt;Amazon Relational Database Service (Amazon RDS)&lt;/a&gt; instances from a developer’s laptop? Online travel marketplace, &lt;a href="https://www.wego.com/"&gt;Wego&lt;/a&gt;, shares their journey from bastion hosts in the public subnet to lightweight VPN tunnels on top of Session Manager, a capability of &lt;a href="https://docs.aws.amazon.com/systems-manager/?id=docs_gateway"&gt;AWS Systems Manager&lt;/a&gt;, using temporary access keys.&lt;/p&gt; 
&lt;p&gt;In this post, we explore how developers get access to allow-listed resources in their virtual private cloud (VPC) directly from their workstation, by tunnelling VPN over secure shell (SSH), which, in turn, is tunneled over Session Manager.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This blog post is not intended as a step-by-step, how-to guide. Commands stated here are for illustrative purposes and may need customization.&lt;/p&gt; 
&lt;h2&gt;Wego’s architecture before starting this journey&lt;/h2&gt; 
&lt;p&gt;In 2021, Wego’s developer connectivity architecture was based on jump hosts in a public subnet, as illustrated in Figure 1.&lt;/p&gt; 
&lt;div id="attachment_12233" style="width: 1647px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-1.-Original-Wego-architecture.png"&gt;&lt;img aria-describedby="caption-attachment-12233" loading="lazy" class="wp-image-12233 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-1.-Original-Wego-architecture.png" alt="Original Wego architecture" width="1637" height="494"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12233" class="wp-caption-text"&gt;Figure 1. Original Wego architecture&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 1 demonstrates a network architecture with both public and private subnets. The public subnet contains an &lt;a href="https://docs.aws.amazon.com/ec2/?id=docs_gateway"&gt;Amazon Elastic Compute Cloud (Amazon EC2)&lt;/a&gt; instance that serves as jump host. The diagram illustrates a VPN tunnel between the developer’s desktop and the VPC.&lt;/p&gt; 
&lt;p&gt;In Wego’s previous architecture, the jump host was connected to the internet for terminal access through the secure shell (SSH) protocol, which accepts traffic at Port 22. Despite restrictions to the allowed source IP addresses, exposing Port 22 to the internet can increase the likeliness of a security breach; it is possible to spoof (mimic) an allowed IP address and attempt a denial of service attack.&lt;/p&gt; 
&lt;h2&gt;Moving the jump host to a private subnet with Session Manager&lt;/h2&gt; 
&lt;p&gt;Session Manager helps minimize the likeliness of a security breach. Figure 2 demonstrates how Wego moved the jump host from a public subnet to a private subnet. In this architecture, Session Manager serves as the main entry point for incoming network traffic.&lt;/p&gt; 
&lt;div id="attachment_12234" style="width: 1668px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-2.-Wegos-new-architecture-using-Session-Manager.png"&gt;&lt;img aria-describedby="caption-attachment-12234" loading="lazy" class="wp-image-12234 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-2.-Wegos-new-architecture-using-Session-Manager.png" alt="Wego's new architecture using Session Manager" width="1658" height="494"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12234" class="wp-caption-text"&gt;Figure 2. Wego’s new architecture using Session Manager&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We will explore how developers connect to Amazon RDS directly from their workstation in this architecture.&lt;/p&gt; 
&lt;h2&gt;Tunnel TCP traffic through Session Manager&lt;/h2&gt; 
&lt;p&gt;Session Manager is best known for its terminal access capability, but it can also tunnel TCP connections. This is helpful if you want to access EC2 instances from your local workstation (Figure 3).&lt;/p&gt; 
&lt;div id="attachment_12235" style="width: 708px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-3.-Tunnelling-TCP-traffic-over-Session-Manager.png"&gt;&lt;img aria-describedby="caption-attachment-12235" loading="lazy" class="wp-image-12235 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-3.-Tunnelling-TCP-traffic-over-Session-Manager.png" alt="Tunneling TCP traffic over Session Manager" width="698" height="249"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12235" class="wp-caption-text"&gt;Figure 3. Tunneling TCP traffic over Session Manager&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Here’s an example command to forward traffic from local host Port 8888 to an EC2 instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ aws ssm start-session --target &amp;lt;instance-id&amp;gt; \
  --document-name AWS-StartPortForwardingSession \
  --parameters '{"portNumber":["8888"], "localPortNumber":["8888"]}'&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This assumes the target EC2 instance is configured with &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-setting-up-ec2.html"&gt;AWS Systems Manager connectivity&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tunnel SSH traffic over Session Manager&lt;/h2&gt; 
&lt;p&gt;SSH is a protocol built on top of TCP; therefore, you can tunnel SSH traffic similarly (Figure 4).&lt;/p&gt; 
&lt;div id="attachment_12236" style="width: 707px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-4.-Tunneling-SSH-traffic-over-Session-Manager.png"&gt;&lt;img aria-describedby="caption-attachment-12236" loading="lazy" class="wp-image-12236 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-4.-Tunneling-SSH-traffic-over-Session-Manager.png" alt="Tunneling SSH traffic over Session Manager" width="697" height="250"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12236" class="wp-caption-text"&gt;Figure 4. Tunneling SSH traffic over Session Manager&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To allow a short-hand notation for SSH over SSM, add the following configuration to the &lt;code&gt;~/.ssh/config&lt;/code&gt; configuration file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;host i-* mi-*&lt;/code&gt; &lt;code&gt;    ProxyCommand sh -c "aws ssm start-session --target %h \&lt;/code&gt; &lt;code&gt;        --document-name AWS-StartSSHSession \&lt;/code&gt; &lt;code&gt;        --parameters 'portNumber=%p'"&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now connect to the EC2 instance over SSH with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;ssh -i &amp;lt;key-file&amp;gt; &amp;lt;username&amp;gt;@&amp;lt;ec2-instance-id&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;ssh -i my_key ec2-user@i-1234567890abcdef0&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ideally, your key-file is a &lt;em&gt;short-lived&lt;/em&gt; credential, as recommended by the &lt;a href="https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/index.en.html"&gt;AWS Well-Architected Framework&lt;/a&gt;, as it narrows the window of opportunity for a security breach. However, it can be tedious to manage short-lived credentials. This is where EC2 Instance Connect comes to the rescue!&lt;/p&gt; 
&lt;h2&gt;Replace SSH keys with EC2 Instance Connect&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-set-up.html"&gt;EC2 Instance Connect&lt;/a&gt; is available both on the AWS console and the command line. It makes it easier to work with short-lived keys. On the command line, it allows us to install our own temporary access credentials into a private EC2 instance for the duration of 60 seconds (Figure 5).&lt;/p&gt; 
&lt;div id="attachment_12238" style="width: 709px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-5.-Connecting-to-SSH-with-temporary-keys.png"&gt;&lt;img aria-describedby="caption-attachment-12238" loading="lazy" class="wp-image-12238 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/27/Figure-5.-Connecting-to-SSH-with-temporary-keys.png" alt="Connecting to SSH with temporary keys" width="699" height="282"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12238" class="wp-caption-text"&gt;Figure 5. Connecting to SSH with temporary keys&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Ensure the&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-set-up.html#ec2-instance-connect-install-eic-CLI"&gt; EC2 instance connect plugin&lt;/a&gt; is installed on your workstation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pip3 install ec2instanceconnectcli&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This blog post assumes you are using &lt;a href="https://aws.amazon.com/amazon-linux/"&gt;Amazon Linux&lt;/a&gt; on the EC2 instance with all pre-requisites installed. Make sure your IAM role or user has the &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-set-up.html#ec2-instance-connect-configure-IAM-role"&gt;required permissions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To generate a temporary SSH key pair, insert:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ ssh-keygen -t rsa -f my_key
$ ssh-add my_key&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install the public key into the EC2 instance, insert:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ aws ec2-instance-connect send-ssh-public-key \
  --instance-id &lt;span style="color: #ff0000"&gt;&amp;lt;instance-id&amp;gt;&lt;/span&gt; \
  --instance-os-user &lt;span style="color: #ff0000"&gt;&amp;lt;username&amp;gt;&lt;/span&gt; \
  --ssh-public-key &lt;span style="color: #ff0000"&gt;&amp;lt;location ssh key public key&amp;gt;&lt;/span&gt; \
  --availability-zone &lt;span style="color: #ff0000"&gt;&amp;lt;availabilityzone&amp;gt;&lt;/span&gt; \
  --region &lt;span style="color: #ff0000"&gt;&amp;lt;region&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ aws ec2-instance-connect send-ssh-public-key \
  --instance-id i-1234567890abcdef0 \
  --instance-os-user ec2-user \
  --ssh-public-key file://my_key.pub \
  --availability-zone ap-southeast-1b \
  --region ap-southeast-1&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Connect to the EC2 instance within 60 seconds and delete the key after use.&lt;/p&gt; 
&lt;h2&gt;Tunneling VPN over SSH, then over Session Manager&lt;/h2&gt; 
&lt;p&gt;In this section, we adopt a third-party, open-source tool that is not supported by AWS, called &lt;a href="https://github.com/sshuttle/sshuttle"&gt;sshuttle&lt;/a&gt;. sshuttle is a transparent proxy server that works as a VPN over SSH. It is based on Python and released under the LGPL 2.1 license. It runs across a wide range of Linux distributions and on macOS (Figure 6).&lt;/p&gt; 
&lt;div id="attachment_12246" style="width: 928px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/28/Figure-6.-Tunneling-VPN-over-SSH-over-Session-Manager.png"&gt;&lt;img aria-describedby="caption-attachment-12246" loading="lazy" class="wp-image-12246 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/28/Figure-6.-Tunneling-VPN-over-SSH-over-Session-Manager.png" alt="Tunneling VPN over SSH over Session Manager" width="918" height="454"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12246" class="wp-caption-text"&gt;Figure 6. Tunneling VPN over SSH over Session Manager&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Why do we need to tunnel VPN over SSH, rather than using the earlier TCP over Session Manager? Keep in mind that the developer’s goal is to connect to Amazon RDS, not Amazon EC2. The SSM tunnel only works for connections to EC2 instances, not Amazon RDS.&lt;/p&gt; 
&lt;p&gt;A lightweight VPN solution, like sshuttle, bridges this gap by allowing you to forward traffic from Amazon EC2 to Amazon RDS. From the developer’s perspective, this works transparently, as if it is regular network traffic.&lt;/p&gt; 
&lt;p&gt;To install sshuttle, use one of the &lt;a href="https://github.com/sshuttle/sshuttle"&gt;documented commands&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ pip3 install sshuttle&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To start sshuttle, use the following command pattern:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ sshuttle -r &lt;span style="color: #ff0000"&gt;&amp;lt;username&amp;gt;&lt;/span&gt;@&lt;span style="color: #ff0000"&gt;&amp;lt;instance-id&amp;gt; &amp;lt;private CIDR range&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ sshuttle -r ec2-user@i-1234567890abcdef0 10.0.0.0/16&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Make sure the security group for the RDS DB instance allows network access from the jump host. You can now connect directly from the developer’s workstation to the RDS DB instance based on its IP address.&lt;/p&gt; 
&lt;h2&gt;Advantages of this architecture&lt;/h2&gt; 
&lt;p&gt;In this blog post, we layered a VPN over SSH that, in turn, is layered over Session Manager, plus we used temporary SSH keys.&lt;/p&gt; 
&lt;p&gt;Wego designed this architecture, and it was practical and stable for day-to-day use. They found that this solution runs at lower cost than &lt;a href="https://docs.aws.amazon.com/vpn/?id=docs_gateway"&gt;AWS Client VPN&lt;/a&gt; and is sufficient for the use case of developers accessing online development environments.&lt;/p&gt; 
&lt;p&gt;Wego’s new architecture has a number of advantages, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;More easily connecting to workloads in private and isolated subnets&lt;/li&gt; 
 &lt;li&gt;Inbound security group rules are not required for the jump host, as Session Manager is an outbound connection&lt;/li&gt; 
 &lt;li&gt;Access attempts are logged in &lt;a href="https://docs.aws.amazon.com/cloudtrail/?id=docs_gateway"&gt;AWS CloudTrail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Access control uses standard IAM policies, including tag-based resource access&lt;/li&gt; 
 &lt;li&gt;Security groups and network access control lists still apply to “allow” or “deny” traffic to specific destinations&lt;/li&gt; 
 &lt;li&gt;SSH keys are installed only temporarily for 60 seconds through EC2 Instance Connect&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post, we explored Wego’s access patterns that can help you reduce your exposure to potential security attacks. Whether you adopt Wego’s full architecture or only adopt intermediary steps (like SSH over Session Manager and EC2 Instance Connect), reducing exposure to the public subnet and shortening the lifetime of access credentials can improve your security posture!&lt;/p&gt; 
&lt;h2&gt;Further reading&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html"&gt;AWS Systems Manager Session Manager&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-enable-ssh-connections.html"&gt;Allow and controlling permissions for SSH connections through Session Manager&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-methods.html"&gt;Connect using EC2 Instance Connect&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sshuttle/sshuttle"&gt;sshuttle Github repository&lt;/a&gt; (third-party, open-source software)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://sshuttle.readthedocs.io/en/stable/"&gt;sshuttle: where transparent proxy meets VPN meets ssh&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>What to consider when modernizing APIs with GraphQL on AWS</title>
		<link>https://aws.amazon.com/blogs/architecture/what-to-consider-when-modernizing-apis-with-graphql-on-aws/</link>
					
		
		<dc:creator><![CDATA[Lewis Tang]]></dc:creator>
		<pubDate>Mon, 31 Oct 2022 14:21:28 +0000</pubDate>
				<category><![CDATA[Amazon Aurora]]></category>
		<category><![CDATA[Amazon DynamoDB]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS AppSync]]></category>
		<category><![CDATA[AWS Lambda]]></category>
		<guid isPermaLink="false">35b7310efc07e9ea98d6c2fa8b84d68a398fbff3</guid>

					<description>In the next few years, companies will build over 500 million new applications, more than has been developed in the previous 40 years combined (see IDC article). API operations enable innovation. They are the “front door” to applications and microservices, and an integral layer in the application stack. In recent years, GraphQL has emerged as […]</description>
										<content:encoded>&lt;p&gt;In the next few years, companies will build over 500 million new applications, more than has been developed in the previous 40 years combined (see &lt;a href="https://www.businesswire.com/news/home/20191029005144/en/IDC-FutureScape-Outlines-the-Impact-Digital-Supremacy-Will-Have-on-Enterprise-Transformation-and-the-IT-Industry"&gt;IDC&lt;/a&gt; article). API operations enable innovation. They are the “front door” to applications and microservices, and an integral layer in the application stack. In recent years, &lt;a href="https://graphql.org/"&gt;GraphQL&lt;/a&gt; has emerged as a modern API approach. With GraphQL, companies can improve the performance of their applications and the speed in which development teams can build applications. In this post, we will discuss how GraphQL works and how integrating it with AWS services can help you build modern applications. We will explore the options for running GraphQL on AWS.&lt;/p&gt; 
&lt;h2&gt;How GraphQL works&lt;/h2&gt; 
&lt;p&gt;Imagine you have an API frontend implemented with GraphQL for your ecommerce application. As shown in Figure 1, there are different services in your ecommerce system backend that are accessible via different technologies. For example, user profile data is stored in a highly scalable NoSQL table. Orders are accessed through a &lt;a href="https://aws.amazon.com/what-is/restful-api/"&gt;REST API&lt;/a&gt;. The current inventory stock is checked through an &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; function. And the pricing information is in an SQL database.&lt;/p&gt; 
&lt;div id="attachment_12208" style="width: 1388px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12208" loading="lazy" class="size-full wp-image-12208" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/21/Fig1-how-graphql-works.png" alt="How GraphQL works" width="1378" height="562"&gt;
 &lt;p id="caption-attachment-12208" class="wp-caption-text"&gt;Figure 1. How GraphQL works&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Without using GraphQL, client applications must make multiple separate calls to each one of these services. Because each service is exposed through different API endpoints, the complexity of accessing data from the client side increases significantly. In order to get the data, you have to make multiple calls. In some cases, you might &lt;em&gt;over fetch&lt;/em&gt; data as the data source would send you an entire payload including data you might not need. In some other circumstances, you might &lt;em&gt;under fetch&lt;/em&gt; data as a single data source would not have all your required data.&lt;/p&gt; 
&lt;p&gt;A GraphQL API combines the data from all these different services into a single payload that the client defines based on its needs. For example, a smartphone has a smaller screen than a desktop application. A smartphone application might require less data. The data is retrieved from multiple data sources automatically. The client just sees a single constructed payload. This payload might be receiving user profile data from &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt;, or order details from &lt;a href="https://aws.amazon.com/api-gateway/"&gt;Amazon API Gateway&lt;/a&gt;. Or it could involve the injection of specific fields with inventory availability and price data from AWS Lambda and &lt;a href="https://aws.amazon.com/rds/aurora/"&gt;Amazon Aurora&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When modernizing frontend APIs with GraphQL, you can build applications faster because your frontend developers don’t need to wait for backend service teams to create new APIs for integration. GraphQL simplifies data access by interacting with data from multiple data sources using a single API. This reduces the number of API requests and network traffic, which results in improved application performance. Furthermore, &lt;a href="https://aws.amazon.com/graphql/graphql-subscriptions-real-time/"&gt;GraphQL subscriptions&lt;/a&gt; enable two-way communication between the backend and client. It supports publishing updates to data in real time to subscribed clients. You can create engaging applications in real time with use cases such as updating sports scores, bidding statuses, and more.&lt;/p&gt; 
&lt;h2&gt;Options for running GraphQL on AWS&lt;/h2&gt; 
&lt;p&gt;There are two main options for running GraphQL implementation on AWS, fully managed on AWS using &lt;a href="https://aws.amazon.com/appsync/"&gt;AWS AppSync&lt;/a&gt;, and self-managed GraphQL.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;I. Fully managed using AWS AppSync&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;The most straightforward way to run GraphQL is by using AWS AppSync, a fully managed service. AWS AppSync handles the heavy lifting of securely connecting to data sources, such as Amazon DynamoDB, and to develop GraphQL APIs. You can write business logic against these data sources by choosing code templates that implement common GraphQL API patterns. Your APIs can also interact with other AWS AppSync functionality such as caching, to improve performance. Use subscriptions to support real-time updates, and client-side data stores to keep offline devices in sync. AWS AppSync will scale automatically to support varied API request loads. You can find more details from the &lt;a href="https://aws.amazon.com/appsync/product-details/"&gt;AWS AppSync features&lt;/a&gt; page.&lt;/p&gt; 
&lt;div id="attachment_12209" style="width: 1391px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12209" loading="lazy" class="size-full wp-image-12209" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/21/Fig2-appsync-ecommerce.png" alt="AWS AppSync in an ecommerce system implementation" width="1381" height="811"&gt;
 &lt;p id="caption-attachment-12209" class="wp-caption-text"&gt;Figure 2. AWS AppSync in an ecommerce system implementation&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Let’s take a closer look at this GraphQL implementation with AWS AppSync in an ecommerce system. In Figure 2, a &lt;a href="https://docs.aws.amazon.com/appsync/latest/devguide/designing-your-schema.html"&gt;schema&lt;/a&gt; is created to define types and capabilities of the desired GraphQL API. You can tie the schema to a &lt;a href="https://docs.aws.amazon.com/appsync/latest/devguide/system-overview-and-architecture.html#resolver"&gt;Resolver&lt;/a&gt; function. The schema can either be created to mirror existing data sources, or AWS AppSync can create tables automatically based the schema definition. You can also use GraphQL features for data discovery without viewing the backend data sources.&lt;/p&gt; 
&lt;p&gt;After a schema definition is established, an &lt;a href="https://docs.aws.amazon.com/appsync/latest/devguide/system-overview-and-architecture.html#appsynclong-client"&gt;AWS AppSync client&lt;/a&gt; can be configured with an operation request, such as a &lt;a href="https://docs.aws.amazon.com/appsync/latest/devguide/quickstart-write-queries.html"&gt;query operation&lt;/a&gt;. The client submits the operation request to &lt;a href="https://docs.aws.amazon.com/appsync/latest/devguide/system-overview-and-architecture.html#graphql-proxy"&gt;GraphQL Proxy&lt;/a&gt; along with an identity context and credentials. The GraphQL Proxy passes this request to the Resolver, which maps and initiates the request payload against pre-configured AWS data services. These can be an Amazon DynamoDB table for user profile, an AWS Lambda function for inventory service, and more. The Resolver initiates calls to one or all of these services within a single API call. This minimizes CPU cycles and network bandwidth needs. The Resolver then returns the response to the client. Additionally, the client application can change data requirements in code on demand. The AWS AppSync GraphQL API will dynamically map requests for data accordingly, enabling faster prototyping and development.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;II. Self-Managed GraphQL&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;If you want the flexibility of selecting a particular open-source project, you may choose to run your own GraphQL API layer. &lt;a href="https://www.apollographql.com/docs/apollo-server/"&gt;Apollo&lt;/a&gt;, &lt;a href="https://github.com/rmosolgo/graphql-ruby"&gt;graphql-ruby&lt;/a&gt;, &lt;a href="https://github.com/graphql-rust/juniper"&gt;Juniper&lt;/a&gt;, &lt;a href="https://github.com/99designs/gqlgen"&gt;gqlgen&lt;/a&gt;, and &lt;a href="https://github.com/walmartlabs/lacinia"&gt;Lacinia&lt;/a&gt; are some popular GraphQL implementations. You can leverage AWS Lambda or container services such as &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html"&gt;Amazon Elastic Container Service (ECS)&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/amazon-elastic-kubernetes-service.html"&gt;Amazon Elastic Kubernetes Services (EKS)&lt;/a&gt; to run GraphQL open-source implementations. This gives you the ability to fine-tune the operational characteristics of your API.&lt;/p&gt; 
&lt;p&gt;When running a GraphQL API layer on AWS Lambda, you can take advantage of the serverless benefits of automatic scaling, paying only for what you use, and not having to manage your servers. You can create a private GraphQL API using Amazon ECS, EKS, or AWS Lambda, which can only be accessed from your &lt;a href="https://aws.amazon.com/vpc/"&gt;Amazon Virtual Private Cloud (VPC)&lt;/a&gt;. With Apollo GraphQL open-source implementation, you can create a Federated GraphQL that allows you to combine GraphQL APIs from multiple microservices into a single API, illustrated in Figure 3. The &lt;a href="https://aws.amazon.com/blogs/mobile/federation-appsync-subgraph/"&gt;Apollo GraphQL Federation with AWS AppSync&lt;/a&gt; post shows a concrete example of how to integrate an AWS AppSync API with an Apollo Federation gateway. It uses specification-compliant queries and directives.&lt;/p&gt; 
&lt;div id="attachment_12210" style="width: 1387px" class="wp-caption alignnone"&gt;
 &lt;img aria-describedby="caption-attachment-12210" loading="lazy" class="size-full wp-image-12210" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/21/Fig3-Apollo-graphql.png" alt="Apollo GraphQL implementation on AWS Lambda" width="1377" height="771"&gt;
 &lt;p id="caption-attachment-12210" class="wp-caption-text"&gt;Figure 3. Apollo GraphQL implementation on AWS Lambda&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;When choosing self-managed GraphQL implementation, you have to spend time writing non-business logic code to connect data sources. You must implement authorization, authentication, and integrate other common functionalities. This can be caches to improve performance, subscriptions to support real-time updates, and client-side data stores to keep offline devices in sync. Because of these responsibilities, you have less time to focus on the business logic of application.&lt;/p&gt; 
&lt;p&gt;Similarly, backend development teams and API operators of an open-source GraphQL implementation must provision and maintain their own GraphQL servers. Remember that even with a serverless model, API developers and operators are still responsible for monitoring, performance tuning, and troubleshooting the API platform service.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Modernizing APIs with GraphQL gives your frontend application the ability to fetch just the data that’s needed from multiple data sources with an API call. You can build modern mobile and web applications faster, because GraphQL simplifies API management. You have flexibility to run an open-source GraphQL implementation most closely aligned with your needs on AWS Lambda, Amazon ECS, and Amazon EKS. With AWS AppSync, you can set up GraphQL quickly and increase your development velocity by reducing the amount of non-business API logic code.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/graphql/serverless-graphql-server/"&gt;How to Create a Serverless GraphQL API on AWS&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</content:encoded>
					
					
			
		
		
			</item>
		<item>
		<title>How USAA built an Amazon S3 malware scanning solution</title>
		<link>https://aws.amazon.com/blogs/architecture/how-usaa-built-an-amazon-s3-malware-scanning-solution/</link>
					
		
		<dc:creator><![CDATA[Jonathan Nguyen]]></dc:creator>
		<pubDate>Fri, 28 Oct 2022 16:45:42 +0000</pubDate>
				<category><![CDATA[Amazon EC2]]></category>
		<category><![CDATA[Amazon Simple Notification Service (SNS)]]></category>
		<category><![CDATA[Amazon Simple Queue Service (SQS)]]></category>
		<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[Architecture]]></category>
		<category><![CDATA[AWS Lambda]]></category>
		<guid isPermaLink="false">7bc560f64191b633743c6ec06938a0ebbba1ad22</guid>

					<description>United Services Automobile Association (USAA) is a San Antonio-based insurance, financial services, banking, and FinTech company supporting millions of military members and their families. USAA has partnered with Amazon Web Services (AWS) to digitally transform and build multiple USAA solutions that help keep members safe and save members money and time. Why build a S3 […]</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://www.usaa.com/"&gt;United Services Automobile Association&lt;/a&gt; (USAA) is a San Antonio-based insurance, financial services, banking, and FinTech company supporting millions of military members and their families. USAA has partnered with Amazon Web Services (AWS) to digitally transform and build multiple USAA solutions that help keep members safe and save members money and time.&lt;/p&gt; 
&lt;h2&gt;Why build a S3 malware scanning solution?&lt;/h2&gt; 
&lt;p&gt;As complex companies’ businesses continue to grow, there may be an increased need for collaboration and interactions with outside vendors.&amp;nbsp;Prior to developing an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Solution&lt;/a&gt; (Amazon S3) scanning solution, a security review and approval process for application teams to ingest data into an &lt;a href="https://aws.amazon.com/organizations/"&gt;AWS Organization&lt;/a&gt; from external vendors’ AWS accounts may be warranted, to ensure additional threats are not being introduced. This could result in a lengthy review and exception process, and subsequently, could hinder the velocity of application teams’ collaboration with external vendors.&lt;/p&gt; 
&lt;p&gt;USAA security standards, like those of most companies, require all data from external vendors to be treated as untrusted, and therefore must be scanned by an antivirus or antimalware solution prior to being ingested by downstream processes within the AWS environment. Companies looking to automate the scanning process may want to consider a solution where all incoming external data flow through a demilitarized drop zone to be scanned, and subsequently released to downstream processes if malware and viruses are not detected.&lt;/p&gt; 
&lt;h2&gt;S3 malware scanning solution overview&lt;/h2&gt; 
&lt;p&gt;Dedicated AWS accounts should be provisioned for specific data classifications and used as a demilitarized zone (DMZ) for an untrusted staging area. The solution discussed in this blog uses a dedicated staging AWS account that controls the release of Amazon S3 objects to other AWS accounts within an AWS Organization. AWS accounts within an AWS Organization should follow &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/welcome.html?secd_intro1"&gt;security best practices&lt;/a&gt; in terms of infrastructure, networking, logging, and security. External vendors should &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html#AccessPolicyLanguage_Interplay"&gt;explicitly&lt;/a&gt; be given limited permissions to appropriate resources in their respective staging S3 bucket.&lt;/p&gt; 
&lt;p&gt;A staging S3 bucket should have specific resource policies restricting which applications and identity and access management (IAM) principals can interact with S3 objects using object attributes, such as object tags, to determine whether an object has been scanned, and what the results of that scan are. Additional guardrails are implemented using Service Control Policies (SCP) to restrict authorized IAM principals to create or modify S3 object attributes (Figure 1).&lt;/p&gt; 
&lt;div id="attachment_12258" style="width: 777px" class="wp-caption alignnone"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/28/Figure-1.-Amazon-S3-antivirus-and-antimalware-scanning-architecture-workflow-2.png"&gt;&lt;img aria-describedby="caption-attachment-12258" loading="lazy" class="wp-image-12258 size-full" src="https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2022/10/28/Figure-1.-Amazon-S3-antivirus-and-antimalware-scanning-architecture-workflow-2.png" alt="Amazon S3 antivirus and antimalware scanning architecture workflow" width="767" height="734"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-12258" class="wp-caption-text"&gt;Figure 1. Amazon S3 antivirus and antimalware scanning architecture workflow&lt;/p&gt;
&lt;/div&gt; 
&lt;ol&gt; 
 &lt;li&gt;The external vendor copies an object to the staging S3 bucket.&lt;/li&gt; 
 &lt;li&gt;The staging S3 bucket has event notifications configured and generates an event.&lt;/li&gt; 
 &lt;li&gt;The S3 PutObject event is sent to an Object Created &lt;a href="https://aws.amazon.com/sqs/"&gt;Amazon Simple Queue Service&lt;/a&gt; (Amazon SQS) queue topic.&lt;/li&gt; 
 &lt;li&gt;An &lt;a href="https://aws.amazon.com/ec2/"&gt;Amazon Elastic Compute Cloud&lt;/a&gt; (Amazon EC2) &lt;a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html"&gt;Auto Scaling&lt;/a&gt; group is configured to scale based on messages in the Object Created SQS queue.&lt;/li&gt; 
 &lt;li&gt;An antivirus and antimalware scanning service application on the Amazon EC2 instances takes the following actions on objects within the Object Created Amazon SQS queue:&lt;br&gt; a. Tag the S3 object with an “In Progress” status.&lt;br&gt; b. Get the object from the Staging S3 bucket and stores it in a local ephemeral file system.&lt;br&gt; c. Scan the copied object using antivirus or antimalware tool.&lt;br&gt; d. Based on the antivirus or antimalware scan results, tag the S3 object with the scan results (for example, No_Malware_Detected vs. Malware_Detected).&lt;br&gt; e. Create and publish a payload to the Object Scanned &lt;a href="https://aws.amazon.com/sns/"&gt;Amazon Simple Notification Service&lt;/a&gt; (Amazon SNS) topic, allowing application team filtering.&lt;br&gt; f. Delete the message from the Object Created SQS queue.&lt;/li&gt; 
 &lt;li&gt;Application teams are subscribed to the Object Scanned SNS topic with a filter for their application.&lt;/li&gt; 
 &lt;li&gt;For any objects where a virus or malware is detected, a company can use its cyber threat response team to conduct a thorough analysis and take appropriate actions.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;USAA built a custom anti-virus and anti-malware scanning application using EC2 instances, using a private, hardened &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"&gt;Amazon Machine Image&lt;/a&gt; (AMI). For cost-efficacy purposes, the EC2 automatic scaling event can be configured based on Object Created SQS queue depth and Service Level Objective (SLO). A serverless version of an anti-virus and anti-malware solution can be used instead of an EC2 application, depending on your specific use-case and other factors. Some important factors include antivirus and antimalware tool serverless support, resource tuning and configuration requirements, and additional AWS services to manage that could possibly result in a bottleneck. If your enterprise is going with a serverless approach, you can use open-source tools such as &lt;a href="https://aws.amazon.com/blogs/developer/virus-scan-s3-buckets-with-a-serverless-clamav-based-cdk-construct/"&gt;ClamAV using Lambda functions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In the event of an infected object, proper guardrails and response mechanisms need to be in place. USAA teams have developed playbooks to monitor the health and performance of S3 scanning solution, as well as responding to detected virus or malware.&lt;/p&gt; 
&lt;p&gt;This cloud native, event-driven solution has benefited multiple USAA application teams who have previously requested the ability to ingest data into AWS workloads from teams outside of USAA’s AWS Organization, and allowed additional capabilities and functionality to better serve their members. To enhance this solution even further, USAA’s security team plans to incorporate additional mechanisms to find specific objects that either failed or required additional processing, without having to scan all objects in the buckets. This can be accomplished by including an additional &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; function and &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt; table to track object metadata as objects get added to the Object Created SQS queue for processing. The metadata could possibly include information such as S3 bucket origin, &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html"&gt;S3 object key&lt;/a&gt;, version ID, scan status, and the original S3 event payload to replay the event into the Object Created SQS queue. The Lambda function primarily ensures the DynamoDB table is kept up to date as objects are processed, as well as handling issues for objects that may need to be reprocessed. The DynamoDB table also has &lt;a href="https://en.wikipedia.org/wiki/Time_to_live"&gt;time-to-live&lt;/a&gt; (TTL) configured to clear records as they expire from the Staging S3 bucket.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we reviewed how USAA’s Public Cloud Security team facilitated collaboration and interactions with external vendors and AWS workloads securely by creating a scalable solution to scan S3 objects for virus and malware prior to releasing objects downstream. The solution uses native AWS services and can be utilized for any use-cases requiring antivirus or antimalware capabilities. Because the S3 object scanning solution uses EC2 instances, you can use your existing antivirus or antimalware enterprise tool.&lt;/p&gt;</content:encoded>
					
					
			
		
		
			</item>
	</channel>
</rss>